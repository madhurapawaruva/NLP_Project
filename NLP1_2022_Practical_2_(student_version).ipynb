{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4JM7xPhkQeE5",
        "FxFoVpvMPB6g"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptMZZMQlfn7c"
      },
      "source": [
        "------\n",
        "**You cannot apply any changes to this file, so please make sure to save it on your Google Colab drive or download it as a .ipynb file.**\n",
        "\n",
        "------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jILqpPLlE9r0"
      },
      "source": [
        "# Practical 2: Representing Sentences with Neural Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JXOZ5uhQ8Qq"
      },
      "source": [
        "In this second practical, we will train neural network models to obtain sentence representations. We can then use these sentence representations for a downstream task such as sentiment classification. \n",
        "\n",
        "In this notebook, we will help you to develop models for your experiments. But this time, next to completing the notebook, **you are expected to write a four-page scientific report with your findings**. Please still submit the notebook together with your scientific report so that we can reproduce your experiments. (Note: if you find it useful, you can split this notebook into multiple notebooks. If you do so, keep it mind that it should be possible for your TAs to reproduce the entire content of the notebooks without having to ask for clarifications or to copy and paste functions from one sub-notebook to another.) Make sure your code corresponds to the description you give in the report; we will deduct points if this is not the case.\n",
        "\n",
        "**Important!** The main purpose of this lab is for you to learn how to answer research questions by experimenting and then writing a scientific report.\n",
        "So you will be *judged by the quality of your report* but will lose points if your experiments are not reproducible.\n",
        "You can find the requirements for the report at the end of this notebook.\n",
        "\n",
        "\n",
        "### Data set\n",
        "We will use the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/) (SST), which provides sentences, their binary tree structure, and fine-grained sentiment scores.\n",
        "This dataset is different from the one we used in the first practical. \n",
        "In Practical 1, a review consisted of several sentences, and we had one sentiment score for the whole review. Now, **a review consists of a single sentence**, and we **have a sentiment score for each node in the binary tree that makes up the sentence**, including the root node (i.e., we still have an overall sentiment score for the entire review). We will look at an example below.\n",
        "\n",
        "In the first part of this practical we will only make use of the sentence tokens whereas in the second part we will also exploit the tree structure that is provided by the SST.\n",
        "\n",
        "We will cover the following approaches:\n",
        "\n",
        "- Bag-of-words (BOW)\n",
        "- Continuous bag-of-words (CBOW)\n",
        "- Deep continuous bag-of-words (Deep CBOW)\n",
        "- LSTM\n",
        "- Tree-LSTM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbNKef3lymaj"
      },
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jxTkpg59FlU"
      },
      "source": [
        "Let's first download the data set and take a look."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch==1.7.0  # installing torch 1.7v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjg9ChRpvN6E",
        "outputId": "9467b3dd-66bc-40e4-f440-2479c9a615ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.0\n",
            "  Downloading torch-1.7.0-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 88 bytes/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.7.0) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.7.0) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from torch==1.7.0) (0.16.0)\n",
            "Installing collected packages: dataclasses, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.7.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed dataclasses-0.6 torch-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZp53HmMP3F2"
      },
      "source": [
        "import re\n",
        "import random\n",
        "import time \n",
        "import math\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('default')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "metadata": {
        "id": "oLfhsHzz146G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(arr, title, y_label):\n",
        "    \"\"\"\n",
        "    Plot either loss or accuracy\n",
        "    \"\"\"\n",
        "    sns.set()\n",
        "    plt.plot([i for i in range(1,len(arr)+1)], arr)\n",
        "    plt.xlabel(\"# of Iterations\")\n",
        "    plt.ylabel(y_label)\n",
        "    plt.ylim(min(arr), max(arr)*1.01)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "ZyJPzKRM2P7S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TovFkDTgE_d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c19ae7-defe-4118-a8ea-7fd63aedff26"
      },
      "source": [
        "!wget http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
        "!unzip trainDevTestTrees_PTB.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-03 14:05:36--  http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip [following]\n",
            "--2022-12-03 14:05:36--  https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 789539 (771K) [application/zip]\n",
            "Saving to: ‘trainDevTestTrees_PTB.zip’\n",
            "\n",
            "trainDevTestTrees_P 100%[===================>] 771.03K   397KB/s    in 1.9s    \n",
            "\n",
            "2022-12-03 14:05:39 (397 KB/s) - ‘trainDevTestTrees_PTB.zip’ saved [789539/789539]\n",
            "\n",
            "Archive:  trainDevTestTrees_PTB.zip\n",
            "   creating: trees/\n",
            "  inflating: trees/dev.txt           \n",
            "  inflating: trees/test.txt          \n",
            "  inflating: trees/train.txt         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IpAphkBO5eW"
      },
      "source": [
        "# this function reads in a textfile and fixes an issue with \"\\\\\"\n",
        "def filereader(path): \n",
        "  with open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "      yield line.strip().replace(\"\\\\\",\"\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP_jpquiprH8"
      },
      "source": [
        "Let's look at a data point. It is a **flattened binary tree**, with sentiment scores at every node, and words as the leaves (or *terminal nodes*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylkIopm0QJML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e5731b-62dc-4212-c541-c1fdbd1c9166"
      },
      "source": [
        "s = next(filereader(\"trees/dev.txt\"))\n",
        "print(s)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_U7HTFwdrWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c03451-0023-4e46-dc21-ef4e25b1cebe"
      },
      "source": [
        "# We can use NLTK to better visualise the tree structure of the sentence\n",
        "from nltk import Tree\n",
        "from nltk.treeprettyprinter import TreePrettyPrinter\n",
        "tree = Tree.fromstring(s)\n",
        "print(TreePrettyPrinter(tree))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              3                                                                     \n",
            "  ____________|____________________                                                  \n",
            " |                                 4                                                \n",
            " |        _________________________|______________________________________________   \n",
            " |       4                                                                        | \n",
            " |    ___|______________                                                          |  \n",
            " |   |                  4                                                         | \n",
            " |   |         _________|__________                                               |  \n",
            " |   |        |                    3                                              | \n",
            " |   |        |               _____|______________________                        |  \n",
            " |   |        |              |                            4                       | \n",
            " |   |        |              |            ________________|_______                |  \n",
            " |   |        |              |           |                        2               | \n",
            " |   |        |              |           |                 _______|___            |  \n",
            " |   |        3              |           |                |           2           | \n",
            " |   |    ____|_____         |           |                |        ___|_____      |  \n",
            " |   |   |          4        |           3                |       2         |     | \n",
            " |   |   |     _____|___     |      _____|_______         |    ___|___      |     |  \n",
            " 2   2   2    3         2    2     3             2        2   2       2     2     2 \n",
            " |   |   |    |         |    |     |             |        |   |       |     |     |  \n",
            " It  's  a  lovely     film with lovely     performances  by Buy     and Accorsi  . \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-56-6ab7e95feba4>:5: DeprecationWarning: \n",
            "    Class TreePrettyPrinter has been deprecated.  Import\n",
            "    `TreePrettyPrinter` using `from nltk.tree import\n",
            "    TreePrettyPrinter` instead.\n",
            "  print(TreePrettyPrinter(tree))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekAWKsji9t93"
      },
      "source": [
        "The sentiment scores range from 0 (very negative) to 5 (very positive). Again, as you can see, every node in the tree is labeled with a sentiment score. For now, we will only use the score at the **root node**, i.e., the sentiment score for the complete sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKynLm0xPKr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b172fa86-8e08-4e56-876b-90849c4ac84c"
      },
      "source": [
        "# Let's first make a function that extracts the tokens (the leaves).\n",
        "\n",
        "def tokens_from_treestring(s):\n",
        "  \"\"\"extract the tokens from a sentiment tree\"\"\"\n",
        "  return re.sub(r\"\\([0-9] |\\)\", \"\", s).split()\n",
        " \n",
        "# let's try it on our example tree\n",
        "tokens = tokens_from_treestring(s)\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
            "13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8vFkeqN-NLP"
      },
      "source": [
        "> *Warning: you could also parse a treestring using NLTK and ask it to return the leaves, but there seems to be an issue with NLTK not always correctly parsing the input, so do not rely on it.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akr9K_Mv4dym"
      },
      "source": [
        "# We will also need the following function, but you can ignore this for now.\n",
        "# It is explained later on.\n",
        "\n",
        "SHIFT = 0\n",
        "REDUCE = 1\n",
        "\n",
        "\n",
        "def transitions_from_treestring(s):\n",
        "  s = re.sub(\"\\([0-5] ([^)]+)\\)\", \"0\", s)\n",
        "  s = re.sub(\"\\)\", \" )\", s)\n",
        "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
        "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
        "  s = re.sub(\"\\)\", \"1\", s)\n",
        "  return list(map(int, s.split()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNtPdlwPgRat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd46052-a4da-48e6-dcf2-f739166a7746"
      },
      "source": [
        "# Now let's first see how large our data sets are.\n",
        "for path in (\"trees/train.txt\", \"trees/dev.txt\", \"trees/test.txt\"):\n",
        "  print(\"{:16s} {:4d}\".format(path, sum(1 for _ in filereader(path))))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trees/train.txt  8544\n",
            "trees/dev.txt    1101\n",
            "trees/test.txt   2210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HexlSqTR_UrY"
      },
      "source": [
        "You can see that the number of sentences is not very large. That's probably because the data set required so much manual annotation. However, it is large enough to train a neural network on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfRjelOcsXuC"
      },
      "source": [
        "It will be useful to store each data example in an `Example` object,\n",
        "containing everything that we may need for each data point.\n",
        "It will contain the tokens, the tree, the top-level sentiment label, and \n",
        "the transitions (explained later)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I07Hb_-q8wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7e7337-cce6-4aca-d057-5628d8b06d27"
      },
      "source": [
        "from collections import namedtuple\n",
        "from nltk import Tree\n",
        "\n",
        "# A simple way to define a class is using namedtuple.\n",
        "Example = namedtuple(\"Example\", [\"tokens\", \"tree\", \"label\", \"transitions\"])\n",
        "\n",
        "   \n",
        "def examplereader(path, lower=False):\n",
        "  \"\"\"Returns all examples in a file one by one.\"\"\"\n",
        "  for line in filereader(path):\n",
        "    line = line.lower() if lower else line\n",
        "    tokens = tokens_from_treestring(line)\n",
        "    tree = Tree.fromstring(line)  # use NLTK's Tree\n",
        "    label = int(line[1])\n",
        "    trans = transitions_from_treestring(line)\n",
        "    yield Example(tokens=tokens, tree=tree, label=label, transitions=trans)\n",
        "  \n",
        "\n",
        "# Let's load the data into memory.\n",
        "LOWER = False  # we will keep the original casing\n",
        "train_data = list(examplereader(\"trees/train.txt\", lower=LOWER))\n",
        "dev_data = list(examplereader(\"trees/dev.txt\", lower=LOWER))\n",
        "test_data = list(examplereader(\"trees/test.txt\", lower=LOWER))\n",
        "\n",
        "print(\"train\", len(train_data))\n",
        "print(\"dev\", len(dev_data))\n",
        "print(\"test\", len(test_data))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 8544\n",
            "dev 1101\n",
            "test 2210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KM0bDyeVZtP"
      },
      "source": [
        "Let's check out an `Example` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8mwcaZwxP1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92db2509-38ca-4482-8f36-e60eb35c1059"
      },
      "source": [
        "example = dev_data[0]\n",
        "print(\"First example:\", example)\n",
        "print(\"First example tokens:\", example.tokens)\n",
        "print(\"First example label:\",  example.label)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First example: Example(tokens=['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.'], tree=Tree('3', [Tree('2', ['It']), Tree('4', [Tree('4', [Tree('2', [\"'s\"]), Tree('4', [Tree('3', [Tree('2', ['a']), Tree('4', [Tree('3', ['lovely']), Tree('2', ['film'])])]), Tree('3', [Tree('2', ['with']), Tree('4', [Tree('3', [Tree('3', ['lovely']), Tree('2', ['performances'])]), Tree('2', [Tree('2', ['by']), Tree('2', [Tree('2', [Tree('2', ['Buy']), Tree('2', ['and'])]), Tree('2', ['Accorsi'])])])])])])]), Tree('2', ['.'])])]), label=3, transitions=[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1])\n",
            "First example tokens: ['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
            "First example label: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WDSprDBVcr-"
      },
      "source": [
        "#### Vocabulary \n",
        "A first step in most NLP tasks is collecting all the word types that appear in the data into a vocabulary, and counting the frequency of their occurrences. On the one hand, this will give us an overview of the word distribution of the data set (what are the most frequent words, how many rare words are there, ...). On the other hand, we will also use the vocabulary to map each word to a unique numeric ID, which is a more handy index than a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvNgKx7usRSt"
      },
      "source": [
        "# Here we first define a class that can map a word to an ID (w2i)\n",
        "# and back (i2w).\n",
        "\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "\n",
        "\n",
        "class OrderedCounter(Counter, OrderedDict):\n",
        "  \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
        "  def __repr__(self):\n",
        "    return '%s(%r)' % (self.__class__.__name__,\n",
        "                      OrderedDict(self))\n",
        "  def __reduce__(self):\n",
        "    return self.__class__, (OrderedDict(self),)\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "  \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.freqs = OrderedCounter()\n",
        "    self.w2i = {}\n",
        "    self.i2w = []\n",
        "\n",
        "  def count_token(self, t):\n",
        "    self.freqs[t] += 1\n",
        "    \n",
        "  def add_token(self, t):\n",
        "    self.w2i[t] = len(self.w2i)\n",
        "    self.i2w.append(t)    \n",
        "    \n",
        "  def build(self, min_freq=0):\n",
        "    '''\n",
        "    min_freq: minimum number of occurrences for a word to be included  \n",
        "              in the vocabulary\n",
        "    '''\n",
        "    self.add_token(\"<unk>\")  # reserve 0 for <unk> (unknown words)\n",
        "    self.add_token(\"<pad>\")  # reserve 1 for <pad> (discussed later)   \n",
        "    \n",
        "    tok_freq = list(self.freqs.items())\n",
        "    tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
        "    for tok, freq in tok_freq:\n",
        "      if freq >= min_freq:\n",
        "        self.add_token(tok)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOvkH_llVsoW"
      },
      "source": [
        "The vocabulary has by default an `<unk>` token and a `<pad>` token. The `<unk>` token is reserved for all words which do not appear in the training data (and for which, therefore, we cannot learn word representations). The function of the `<pad>` token will be explained later.\n",
        "\n",
        "\n",
        "Let's build the vocabulary!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwGQgQQBNUSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e266dd6-d216-4447-d783-52461f323631"
      },
      "source": [
        "# This process should be deterministic and should have the same result \n",
        "# if run multiple times on the same data set.\n",
        "\n",
        "v = Vocabulary()\n",
        "for data_set in (train_data,):\n",
        "  for ex in data_set:\n",
        "    for token in ex.tokens:\n",
        "      v.count_token(token)\n",
        "\n",
        "v.build()\n",
        "print(\"Vocabulary size:\", len(v.w2i))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 18280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UNIedPrPdCw"
      },
      "source": [
        "Let's have a closer look at the properties of our vocabulary. Having a good idea of what it is like can facilitate data analysis and debugging later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJyuogmh0CA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f433848-fc83-4dda-b82e-18de130d36a8"
      },
      "source": [
        "# What is the ID for \"century?\"\n",
        "w = \"century\"\n",
        "print('Id of word {} in the vocabulary is'.format(w),v.w2i[w])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Id of word century in the vocabulary is 1973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8OkPQ8Zv-rI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39cbebfe-8c0f-4ad1-84fa-88e0269ebcf5"
      },
      "source": [
        "# What are the first 10 words in the vocabulary (based on their IDs)?\n",
        "top_n = 10\n",
        "print('------ First {} words in the vocabulary based on their id ------'.format(top_n))\n",
        "for i in range(top_n):\n",
        "  w = v.i2w[i]\n",
        "  print('{} -->'.format(i),w)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ First 10 words in the vocabulary based on their id ------\n",
            "0 --> <unk>\n",
            "1 --> <pad>\n",
            "2 --> .\n",
            "3 --> ,\n",
            "4 --> the\n",
            "5 --> and\n",
            "6 --> a\n",
            "7 --> of\n",
            "8 --> to\n",
            "9 --> 's\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmXwu02lOLWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee8c68c9-598a-462c-fde8-74de17e84177"
      },
      "source": [
        "# What are the 10 most common words?\n",
        "top_n = 10\n",
        "print('------ Most Frequent Words in the Vocabulary ------')\n",
        "# Remember that we add first the word with the greatest frequency but before them\n",
        "# we add a few words that we should skip because they are not the most frequent\n",
        "# ones\n",
        "# Those words are the <unk>  and <pad> \n",
        "skip_words = 2\n",
        "count = 0\n",
        "for i in range(skip_words, skip_words + top_n):\n",
        "  count +=1\n",
        "  w = v.i2w[i]\n",
        "  print('{:3d}:'.format(count),w)\n",
        "\n",
        "###### Different way of doing the same thing as above ######\n",
        "# # Extract the words and their frequencies\n",
        "# tok_freq = list(v.freqs.items())\n",
        "# # Sort the words eys and their frequencies in descending order\n",
        "# tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
        "# for i, (w, freq) in enumerate(tok_freq[:top_n]):\n",
        "#     print('{:3d}:'.format(i+1),w)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ Most Frequent Words in the Vocabulary ------\n",
            "  1: .\n",
            "  2: ,\n",
            "  3: the\n",
            "  4: and\n",
            "  5: a\n",
            "  6: of\n",
            "  7: to\n",
            "  8: 's\n",
            "  9: is\n",
            " 10: that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__NDPaCeOT_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eed94af8-6259-44a1-ffa4-ae95ba33e23a"
      },
      "source": [
        "# And how many words are there with frequency 1?\n",
        "# (A fancy name for these is hapax legomena.)\n",
        "words_with_fr_1 = 0\n",
        "\n",
        "# Extract the words and their frequencies\n",
        "tok_freq = list(v.freqs.items())\n",
        "for w, freq in tok_freq:\n",
        "  if freq == 1:\n",
        "    words_with_fr_1 += 1\n",
        "print('There are {} words with frequency of 1.'.format(words_with_fr_1))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 9543 words with frequency of 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKHocugctZGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2007261d-9387-4d2d-d956-6bfb61dccd02"
      },
      "source": [
        "# Finally 20 random words from the vocabulary.\n",
        "# This is a simple way to get a feeling for the data. \n",
        "# You could use the `choice` function from the already imported `random` package\n",
        "n_rand_ws = 20\n",
        "random.sample(list(v.w2i.keys()), n_rand_ws)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yvan',\n",
              " 'restroom',\n",
              " \"'40s\",\n",
              " 'sets',\n",
              " 'tax',\n",
              " 'Several',\n",
              " 'Eve',\n",
              " 'repellantly',\n",
              " 'Coal',\n",
              " 'denuded',\n",
              " 'played',\n",
              " 'lay',\n",
              " 'controlled',\n",
              " 'laughing',\n",
              " 'telescope',\n",
              " 'elite',\n",
              " 'sweet',\n",
              " 'kidnappings',\n",
              " 'Jesus',\n",
              " 'catalytic']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGWaZahKV_dH"
      },
      "source": [
        "#### Sentiment label vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmTC-rvQelpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5055faec-e3c4-4732-b8a2-7983d13b92a6"
      },
      "source": [
        "# Now let's map the sentiment labels 0-4 to a more readable form\n",
        "i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
        "print(i2t)\n",
        "print(i2t[4])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['very negative', 'negative', 'neutral', 'positive', 'very positive']\n",
            "very positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7UI26DP2dr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b9b10a7-0029-4258-8be1-9e925d246356"
      },
      "source": [
        "# And let's also create the opposite mapping.\n",
        "# We won't use a Vocabulary for this (although we could), since the labels\n",
        "# are already numeric.\n",
        "t2i = OrderedDict({p : i for p, i in zip(i2t, range(len(i2t)))})\n",
        "print(t2i)\n",
        "print(t2i['very positive'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('very negative', 0), ('negative', 1), ('neutral', 2), ('positive', 3), ('very positive', 4)])\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0067ax54-rd"
      },
      "source": [
        "## PyTorch\n",
        "\n",
        "In Colab notebooks, the last available version of PyTorch is already installed.The current stable version is 1.7.\n",
        "\n",
        "*For installing PyTorch in your own computer, follow the instructions on [pytorch.org](pytorch.org) instead. This is for Google Colab only.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKQMGtkR5KWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e963dcc6-3d14-4e64-f342-88503ed59f44"
      },
      "source": [
        "import torch\n",
        "print(\"Using torch\", torch.__version__) # should say 1.7.0+cu101"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnvPcd_E1xH8"
      },
      "source": [
        "# Let's also import torch.nn, a PyTorch package that  \n",
        "# makes building neural networks more convenient.\n",
        "from torch import nn"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYt8uTyGCKc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "347e6c95-f97e-4ce6-aae3-1154d64dd7f2"
      },
      "source": [
        "# PyTorch can run on CPU or on Nvidia GPU (video card) using CUDA\n",
        "# This cell selects the GPU if one is available.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d1VMOOYx1Bw"
      },
      "source": [
        "# Seed manually to make runs reproducible\n",
        "# You need to set this again if you do multiple runs of the same model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# When running on the CuDNN backend two further options must be set for reproducibility\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWBTzkuE3CtZ"
      },
      "source": [
        "# BOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBAjYYySOA5W"
      },
      "source": [
        "Our first model is a rather simple neural **bag-of-words (BOW) model**.\n",
        "Unlike the bag-of-words model that you used in the previous lab, where we would look at the presence / frequency of words in a text, here we associate each word with a multi-dimensional vector which expresses what sentiment is conveyed by the word. In particular, our BOW vectors will be of size 5, exactly our number of sentiment classes. \n",
        "\n",
        "To classify a sentence, we **sum** the vectors of the words in the sentence and a bias vector. Because we sum the vectors, we lose word order: that's why we call this a neural bag-of-words model.\n",
        "\n",
        "```\n",
        "this   [0.0, 0.1, 0.1, 0.1, 0.0]\n",
        "movie  [0.0, 0.1, 0.1, 0.2, 0.1]\n",
        "is     [0.0, 0.1, 0.0, 0.0, 0.0]\n",
        "stupid [0.9, 0.5, 0.1, 0.0, 0.0]\n",
        "\n",
        "bias   [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "--------------------------------\n",
        "sum    [0.9, 0.8, 0.3, 0.3, 0.1]\n",
        "\n",
        "argmax: 0 (very negative)\n",
        "```\n",
        "\n",
        "The **argmax** of this sum is our predicted label.\n",
        "\n",
        "We initialize all vectors *randomly* and train them using cross-entropy loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLtBAIQGynkB"
      },
      "source": [
        "#### Model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZfNklWf3tvs"
      },
      "source": [
        "class BOW(nn.Module):\n",
        "  \"\"\"A simple bag-of-words model\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, vocab):\n",
        "    super(BOW, self).__init__()\n",
        "    self.vocab = vocab\n",
        "    \n",
        "    # this is a trainable look-up table with word embeddings\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    # this is a trainable bias term\n",
        "    self.bias = nn.Parameter(torch.zeros(embedding_dim), requires_grad=True)        \n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # this is the forward pass of the neural network\n",
        "    # it applies a function to the input and returns the output\n",
        "\n",
        "    # this looks up the embeddings for each word ID in inputs\n",
        "    # the result is a sequence of word embeddings\n",
        "    embeds = self.embed(inputs)\n",
        "    \n",
        "    # the output is the sum across the time dimension (1)\n",
        "    # with the bias term added\n",
        "    logits = embeds.sum(1) + self.bias\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKHvBnoBAr6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d7df53e-d832-46a0-bd7c-ddcd60344d04"
      },
      "source": [
        "# Let's create a model.\n",
        "vocab_size = len(v.w2i)\n",
        "n_classes = len(t2i)\n",
        "bow_model = BOW(vocab_size, n_classes, v)\n",
        "print(bow_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOW(\n",
            "  (embed): Embedding(18280, 5)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfCx-HvMH1qQ"
      },
      "source": [
        "> **Hey, wait, where is the bias vector?**\n",
        "> PyTorch does not print Parameters, only Modules!\n",
        "\n",
        "> We can print it ourselves though, to check that it is there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhvk5HenAroT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5f023182-4fb8-41b2-c1f1-d394f7d7feaf"
      },
      "source": [
        "\n",
        "# Here we print each parameter name, shape, and if it is trainable.\n",
        "def print_parameters(model):\n",
        "  total = 0\n",
        "  for name, p in model.named_parameters():\n",
        "    total += np.prod(p.shape)\n",
        "    print(\"{:24s} {:12s} requires_grad={}\".format(name, str(list(p.shape)), p.requires_grad))\n",
        "  print(\"\\nTotal number of parameters: {}\\n\".format(total))\n",
        "    \n",
        "\n",
        "print_parameters(bow_model)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-97426c6ca181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'bow_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSAw292WxuP4"
      },
      "source": [
        "#### Preparing an example for input\n",
        "\n",
        "To feed sentences to our PyTorch model, we need to convert a sequence of tokens to a sequence of IDs. The `prepare_example` function below takes care of this for us. We then use these IDs as indices for the word embedding table. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWeGTC_OGReV"
      },
      "source": [
        "def prepare_example(example, vocab):\n",
        "  \"\"\"\n",
        "  Map tokens to their IDs for a single example\n",
        "  \"\"\"\n",
        "  \n",
        "  # vocab returns 0 if the word is not there (i2w[0] = <unk>)\n",
        "  x = [vocab.w2i.get(t, 0) for t in example.tokens]\n",
        "  \n",
        "  x = torch.LongTensor([x])\n",
        "  x = x.to(device)\n",
        "  \n",
        "  y = torch.LongTensor([example.label])\n",
        "  y = y.to(device)\n",
        "  \n",
        "  return x, y"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfbdv9px3uFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4bb970-7417-4f3c-bffa-972409b8e763"
      },
      "source": [
        "x, y = prepare_example(dev_data[0], v)\n",
        "print('x:', x)\n",
        "print('y:', y)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([[  28,    9,    6,  998,   16,   18,  998,  135,   32, 7688,    5,    0,\n",
            "            2]], device='cuda:0')\n",
            "y: tensor([3], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKNQjEc0yXnJ"
      },
      "source": [
        "#### Evaluation\n",
        "We now need to define an evaluation metric.\n",
        "How many predictions do we get right? The accuracy will tell us.\n",
        "Make sure that you understand this code block.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGmQLcVYKZsh"
      },
      "source": [
        "# TODO: Potentially add an extended evaluate method that would include precision, recall and f1-score\n",
        "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
        "  \"\"\"Accuracy of a model on given data set.\"\"\"\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model.eval()  # disable dropout (explained later)\n",
        "\n",
        "  for example in data:\n",
        "    \n",
        "    # convert the example input and label to PyTorch tensors\n",
        "    x, target = prep_fn(example, model.vocab)\n",
        "\n",
        "    # forward pass without backpropagation (no_grad)\n",
        "    # get the output from the neural network for input x\n",
        "    with torch.no_grad():\n",
        "      logits = model(x)\n",
        "    \n",
        "    # get the prediction\n",
        "    prediction = logits.argmax(dim=-1)\n",
        "    \n",
        "    # add the number of correct predictions to the total correct\n",
        "    correct += (prediction == target).sum().item()\n",
        "    total += 1\n",
        "\n",
        "  return correct, total, correct / float(total)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KlIGFXllWWm"
      },
      "source": [
        "We are using accuracy as a handy evaluation metric. Please consider using [alternative metrics](https://scikit-learn.org/stable/modules/classes.html#classification-metrics) for your experiments if that makes more theoretical sense. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIk6OtSdzGRP"
      },
      "source": [
        "#### Example feed\n",
        "For stochastic gradient descent (SGD) we will need a random training example for every update.\n",
        "We implement this by shuffling the training data and returning examples one by one using `yield`.\n",
        "\n",
        "Shuffling is optional so that we get to use this function to get validation and test examples, too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxDFOZLfCXvJ"
      },
      "source": [
        "def get_examples(data, shuffle=True, **kwargs):\n",
        "  \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
        "  if shuffle:\n",
        "    print(\"Shuffling training data\")\n",
        "    random.shuffle(data)  # shuffle training data each epoch\n",
        "  for example in data:\n",
        "    yield example"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g09SM8yb2cjx"
      },
      "source": [
        "#### Exercise: Training function\n",
        "\n",
        "Your task is now to complete the training loop below.\n",
        "Before you do so, please read the section about optimisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVfUukVdM_1c"
      },
      "source": [
        "**Optimisation**\n",
        "\n",
        "As mentioned in the \"Intro to PyTorch\" notebook, one of the perks of using PyTorch is automatic differentiation. We will use it to train our BOW model. \n",
        "\n",
        "We train our model by feeding it an input, performing a **forward** pass, obtaining an output prediction, and calculating a **loss** with our loss function.\n",
        "After the gradients are computed in the **backward** pass, we can take a step on the surface of the loss function towards more optimal parameter settings (gradient descent). \n",
        "\n",
        "The package we will use to do this optimisation is [torch.optim](https://pytorch.org/docs/stable/optim.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhQigDrQ--YU"
      },
      "source": [
        "from torch import optim"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGIvcTZU_Cez"
      },
      "source": [
        "Besides implementations of stochastic gradient descent (SGD), this package also implements the optimisation algorithm Adam, which we'll be using in this practical. \n",
        "For the purposes of this assignment you do not need to know what Adam does besides that it uses gradient information to update our model parameters by calling: \n",
        "\n",
        "```\n",
        "optimizer.step()\n",
        "```\n",
        "Remember when we updated our parameters in the PyTorch tutorial in a loop?\n",
        "\n",
        "\n",
        "```python\n",
        "# update weights\n",
        "learning_rate = 0.5\n",
        "for f in net.parameters():\n",
        "    # for each parameter, take a small step in the opposite dir of the gradient\n",
        "    p.data = p.data - p.grad.data * learning_rate\n",
        "\n",
        "```\n",
        "The function call optimizer.step() does effectively the same thing.\n",
        "\n",
        "*(If you want to know more about optimisation algorithms using gradient information, [this blog](http://ruder.io/optimizing-gradient-descent/.) gives a nice intuitive overview.)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktFnKBux25lD"
      },
      "source": [
        "def train_model(model, optimizer, num_iterations=10000, \n",
        "                print_every=1000, eval_every=1000,\n",
        "                batch_fn=get_examples, \n",
        "                prep_fn=prepare_example,\n",
        "                eval_fn=simple_evaluate,\n",
        "                batch_size=1, eval_batch_size=None):\n",
        "  \"\"\"Train a model.\"\"\"  \n",
        "  iter_i = 0\n",
        "  train_loss = 0.\n",
        "  print_num = 0\n",
        "  start = time.time()\n",
        "  criterion = nn.CrossEntropyLoss() # loss function\n",
        "  best_eval = 0.\n",
        "  best_iter = 0\n",
        "  \n",
        "  # store train loss and validation accuracy during training\n",
        "  # so we can plot them afterwards\n",
        "  losses = []\n",
        "  accuracies = []  \n",
        "  \n",
        "  if eval_batch_size is None:\n",
        "    eval_batch_size = batch_size\n",
        "  \n",
        "  while True:  # when we run out of examples, shuffle and continue\n",
        "    for batch in batch_fn(train_data, batch_size=batch_size):\n",
        "\n",
        "      # forward pass\n",
        "      model.train()\n",
        "      x, targets = prep_fn(batch, model.vocab)\n",
        "\n",
        "      logits = model(x)\n",
        "\n",
        "      B = targets.size(0)  # later we will use B examples per update\n",
        "      \n",
        "      # compute cross-entropy loss (our criterion)\n",
        "      # note that the cross entropy loss function computes the softmax for us\n",
        "      loss = criterion(logits.view([B, -1]), targets.view(-1))\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # backward pass (tip: check the Introduction to PyTorch notebook)\n",
        "      # erase previous gradients\n",
        "      # Before backward pass make sure there are no previous aggregated\n",
        "      # gradients\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # compute gradients\n",
        "      # i.e., Perform backpropagation\n",
        "      loss.backward()\n",
        "\n",
        "      # update weights - take a small step in the opposite dir of the gradient\n",
        "      optimizer.step()\n",
        "\n",
        "      print_num += 1\n",
        "      iter_i += 1\n",
        "\n",
        "      # print info\n",
        "      if iter_i % print_every == 0:\n",
        "        print(\"Iter %r: loss=%.4f, time=%.2fs\" % \n",
        "              (iter_i, train_loss, time.time()-start))\n",
        "        losses.append(train_loss)\n",
        "        print_num = 0        \n",
        "        train_loss = 0.\n",
        "\n",
        "      # evaluate\n",
        "      if iter_i % eval_every == 0:\n",
        "        _, _, accuracy = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
        "                                 batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "        accuracies.append(accuracy)\n",
        "        print(\"iter %r: dev acc=%.4f\" % (iter_i, accuracy))       \n",
        "        \n",
        "        # save best model parameters\n",
        "        if accuracy > best_eval:\n",
        "          print(\"new highscore\")\n",
        "          best_eval = accuracy\n",
        "          best_iter = iter_i\n",
        "          path = \"{}.pt\".format(model.__class__.__name__)\n",
        "          ckpt = {\n",
        "              \"state_dict\": model.state_dict(),\n",
        "              \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "              \"best_eval\": best_eval,\n",
        "              \"best_iter\": best_iter\n",
        "          }\n",
        "          torch.save(ckpt, path)\n",
        "\n",
        "      # done training\n",
        "      if iter_i == num_iterations:\n",
        "        print(\"Done training\")\n",
        "        \n",
        "        # evaluate on train, dev, and test with best model\n",
        "        print(\"Loading best model\")\n",
        "        path = \"{}.pt\".format(model.__class__.__name__)        \n",
        "        ckpt = torch.load(path)\n",
        "        model.load_state_dict(ckpt[\"state_dict\"])\n",
        "        \n",
        "        _, _, train_acc = eval_fn(\n",
        "            model, train_data, batch_size=eval_batch_size, \n",
        "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "        _, _, dev_acc = eval_fn(\n",
        "            model, dev_data, batch_size=eval_batch_size,\n",
        "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "        _, _, test_acc = eval_fn(\n",
        "            model, test_data, batch_size=eval_batch_size, \n",
        "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "        \n",
        "        print(\"best model iter {:d}: \"\n",
        "              \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
        "                  best_iter, train_acc, dev_acc, test_acc))\n",
        "        \n",
        "        return losses, accuracies"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEPsLvI-3D5b"
      },
      "source": [
        "### Training the BOW model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9luJnNuN_d3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af6b8576-4daf-437e-c701-7d075011457b"
      },
      "source": [
        "# If everything is in place we can now train our first model!\n",
        "# TODO: Potentially run it for more than the default number of iterations (i.e.,  30K to see for convergence)\n",
        "bow_model = BOW(len(v.w2i), len(t2i), vocab=v)\n",
        "print(bow_model)\n",
        "\n",
        "bow_model = bow_model.to(device)\n",
        "eval_every=1000\n",
        "optimizer = optim.Adam(bow_model.parameters(), lr=0.0005)\n",
        "bow_losses, bow_accuracies = train_model(\n",
        "    bow_model, optimizer, num_iterations=30000, \n",
        "    print_every=1000, eval_every=eval_every)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOW(\n",
            "  (embed): Embedding(18280, 5)\n",
            ")\n",
            "Shuffling training data\n",
            "Iter 1000: loss=5249.7221, time=1.23s\n",
            "iter 1000: dev acc=0.2207\n",
            "new highscore\n",
            "Iter 2000: loss=4587.9377, time=2.48s\n",
            "iter 2000: dev acc=0.2189\n",
            "Iter 3000: loss=4725.6607, time=3.63s\n",
            "iter 3000: dev acc=0.2234\n",
            "new highscore\n",
            "Iter 4000: loss=4684.5252, time=4.78s\n",
            "iter 4000: dev acc=0.2398\n",
            "new highscore\n",
            "Iter 5000: loss=4249.6938, time=5.89s\n",
            "iter 5000: dev acc=0.2380\n",
            "Iter 6000: loss=4388.6119, time=6.95s\n",
            "iter 6000: dev acc=0.2434\n",
            "new highscore\n",
            "Iter 7000: loss=4187.1097, time=7.96s\n",
            "iter 7000: dev acc=0.2452\n",
            "new highscore\n",
            "Iter 8000: loss=4314.2058, time=8.91s\n",
            "iter 8000: dev acc=0.2470\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 9000: loss=4202.8355, time=9.89s\n",
            "iter 9000: dev acc=0.2516\n",
            "new highscore\n",
            "Iter 10000: loss=3937.5037, time=10.83s\n",
            "iter 10000: dev acc=0.2534\n",
            "new highscore\n",
            "Iter 11000: loss=4158.6476, time=11.77s\n",
            "iter 11000: dev acc=0.2543\n",
            "new highscore\n",
            "Iter 12000: loss=3781.6740, time=12.69s\n",
            "iter 12000: dev acc=0.2552\n",
            "new highscore\n",
            "Iter 13000: loss=3809.8180, time=13.64s\n",
            "iter 13000: dev acc=0.2579\n",
            "new highscore\n",
            "Iter 14000: loss=3971.0008, time=14.58s\n",
            "iter 14000: dev acc=0.2616\n",
            "new highscore\n",
            "Iter 15000: loss=3784.9992, time=15.54s\n",
            "iter 15000: dev acc=0.2643\n",
            "new highscore\n",
            "Iter 16000: loss=3650.9357, time=16.57s\n",
            "iter 16000: dev acc=0.2670\n",
            "new highscore\n",
            "Iter 17000: loss=3871.6549, time=17.66s\n",
            "iter 17000: dev acc=0.2661\n",
            "Shuffling training data\n",
            "Iter 18000: loss=3359.2563, time=18.70s\n",
            "iter 18000: dev acc=0.2688\n",
            "new highscore\n",
            "Iter 19000: loss=3462.5702, time=19.63s\n",
            "iter 19000: dev acc=0.2716\n",
            "new highscore\n",
            "Iter 20000: loss=3727.1139, time=20.57s\n",
            "iter 20000: dev acc=0.2707\n",
            "Iter 21000: loss=3574.5932, time=21.51s\n",
            "iter 21000: dev acc=0.2734\n",
            "new highscore\n",
            "Iter 22000: loss=3369.8040, time=22.46s\n",
            "iter 22000: dev acc=0.2734\n",
            "Iter 23000: loss=3228.7643, time=23.40s\n",
            "iter 23000: dev acc=0.2743\n",
            "new highscore\n",
            "Iter 24000: loss=3460.0551, time=24.36s\n",
            "iter 24000: dev acc=0.2743\n",
            "Iter 25000: loss=3271.0015, time=25.29s\n",
            "iter 25000: dev acc=0.2743\n",
            "Shuffling training data\n",
            "Iter 26000: loss=3145.1694, time=26.41s\n",
            "iter 26000: dev acc=0.2788\n",
            "new highscore\n",
            "Iter 27000: loss=3008.7834, time=28.52s\n",
            "iter 27000: dev acc=0.2788\n",
            "Iter 28000: loss=3152.5072, time=30.58s\n",
            "iter 28000: dev acc=0.2852\n",
            "new highscore\n",
            "Iter 29000: loss=3028.8963, time=32.25s\n",
            "iter 29000: dev acc=0.2870\n",
            "new highscore\n",
            "Iter 30000: loss=2927.2169, time=33.69s\n",
            "iter 30000: dev acc=0.2870\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 29000: train acc=0.3189, dev acc=0.2870, test acc=0.2733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvYLj8LIAzfS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "df509183-a4a9-4939-c542-ae6526f39c4a"
      },
      "source": [
        "# This will plot the validation accuracies across time.\n",
        "plt.title('Validation Accuracy as a function of epochs for the BOW model')\n",
        "plt.xlabel('Steps of {} epochs each'.format(eval_every))\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.plot(bow_accuracies)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUHm9JfSI4po",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "ac3cebe5-9fbc-433f-aad6-11fc4beea1d2"
      },
      "source": [
        "# This will plot the training loss over time.\n",
        "plt.title('Train Loss as a function of epochs for the BOW model')\n",
        "plt.xlabel('Steps of {} epochs each'.format(eval_every))\n",
        "plt.ylabel('Train Loss')\n",
        "plt.plot(bow_losses)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Psxh-Le1BMDQ"
      },
      "source": [
        "Please note that we set the number of iterations to 30K as an indicative value, after which we simply stop training without checking for convergence. You should choose an appropriate number of iterations and motivate your decision. **This holds for all pre-set numbers of iteration in the following code blocks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9mB1_XhMPNN"
      },
      "source": [
        "# CBOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWk78FvNMw4o"
      },
      "source": [
        "We now continue with a **continuous bag-of-words (CBOW)** model. (*This is not the same as the word2vec CBOW model!*)\n",
        "\n",
        "It is similar to the BOW model above, but now embeddings can have a dimension of *arbitrary size*. \n",
        "This means that we can choose a higher dimensionality and learn more aspects of each word. We will still sum word vectors to get a sentence representation, but now the size of the resulting vector will no longer correspond to the number of sentiment classes. \n",
        "\n",
        "So to turn the size of our summed vector into the number of output classes, we can *learn* a parameter matrix $W$ and multiply it by the sum vector $x$: $$Wx$$\n",
        "If the size of $x$ is `d x 1`, we can set $W$ to be `5 x d`, so that the output of the matrix multiplication will be the of the desired size, `5 x 1`. Then, just like for the BOW model, we can obtain a prediction using the argmax function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIjrCPfCwsXI"
      },
      "source": [
        "## Exercise: implement and train the CBOW model\n",
        "\n",
        "Write a class `CBOW` that:\n",
        "\n",
        "- has word embeddings with size 300\n",
        "- sums the word vectors for the input words (just like in `BOW`)\n",
        "- projects the resulting vector down to 5 units using a linear layer and a bias term (check out `nn.Linear`)\n",
        "\n",
        "Train your CBOW model and plot the validation accuracy and training loss over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEV22aR2MP0Q"
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "  \"\"\"A simple bag-of-words model\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, output_dim, vocab):\n",
        "    super(CBOW, self).__init__()\n",
        "    self.vocab = vocab\n",
        "    \n",
        "    # this is a trainable look-up table with word embeddings\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "    # Linear Layer with bias Term\n",
        "    self.linear = nn.Linear(in_features=embedding_dim, \n",
        "                                out_features=output_dim, \n",
        "                             bias=True)\n",
        "    # # this is a trainable bias term\n",
        "    self.bias = nn.Parameter(torch.zeros(embedding_dim), requires_grad=True)        \n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # this is the forward pass of the neural network\n",
        "    # it applies a function to the input and returns the output\n",
        "\n",
        "    # this looks up the embeddings for each word ID in inputs\n",
        "    # the result is a sequence of word embeddings\n",
        "    embeds = self.embed(inputs)\n",
        "    # TODO: Do we need to have an additional bias term here?\n",
        "    # Sum the embeddings related to each input and the bias \n",
        "    embeds_summed = embeds.sum(1) + self.bias\n",
        "    # Transform the embeddings to sentiment related vectors\n",
        "    logits = self.linear(embeds_summed)\n",
        "    # the output is the sum across the time dimension (1)\n",
        "    # with the bias term added\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If everything is in place we can now train our first model!\n",
        "# TODO: Potentially run it for more than the default number of iterations (i.e.,  30K to see for convergence)\n",
        "cbow_model = CBOW(vocab_size=len(v.w2i), embedding_dim=300, output_dim=len(t2i), vocab=v)\n",
        "print(cbow_model)\n",
        "\n",
        "# Move model to device\n",
        "cbow_model = cbow_model.to(device)\n",
        "eval_every=1000\n",
        "optimizer = optim.Adam(cbow_model.parameters(), lr=0.0005)\n",
        "cbow_losses, cbow_accuracies = train_model(\n",
        "    cbow_model, optimizer, num_iterations=30000, \n",
        "    print_every=1000, eval_every=eval_every)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H2NORYVWKtD1",
        "outputId": "74471e8f-47eb-4e31-bb82-f27293628f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBOW(\n",
            "  (embed): Embedding(18280, 300)\n",
            "  (linear): Linear(in_features=300, out_features=5, bias=True)\n",
            ")\n",
            "Shuffling training data\n",
            "Iter 1000: loss=2637.5324, time=45.45s\n",
            "iter 1000: dev acc=0.2589\n",
            "new highscore\n",
            "Iter 2000: loss=2275.6768, time=87.77s\n",
            "iter 2000: dev acc=0.2461\n",
            "Iter 3000: loss=2048.8999, time=128.04s\n",
            "iter 3000: dev acc=0.2807\n",
            "new highscore\n",
            "Iter 4000: loss=2001.5051, time=167.55s\n",
            "iter 4000: dev acc=0.3034\n",
            "new highscore\n",
            "Iter 5000: loss=1939.6568, time=204.98s\n",
            "iter 5000: dev acc=0.2852\n",
            "Iter 6000: loss=1914.7738, time=240.55s\n",
            "iter 6000: dev acc=0.2852\n",
            "Iter 7000: loss=1862.2437, time=274.65s\n",
            "iter 7000: dev acc=0.2707\n",
            "Iter 8000: loss=1930.8197, time=307.29s\n",
            "iter 8000: dev acc=0.2634\n",
            "Shuffling training data\n",
            "Iter 9000: loss=1744.5082, time=339.19s\n",
            "iter 9000: dev acc=0.3134\n",
            "new highscore\n",
            "Iter 10000: loss=1570.1530, time=370.88s\n",
            "iter 10000: dev acc=0.2861\n",
            "Iter 11000: loss=1632.2951, time=402.23s\n",
            "iter 11000: dev acc=0.3034\n",
            "Iter 12000: loss=1628.9163, time=433.79s\n",
            "iter 12000: dev acc=0.2707\n",
            "Iter 13000: loss=1631.3612, time=465.28s\n",
            "iter 13000: dev acc=0.3433\n",
            "new highscore\n",
            "Iter 14000: loss=1595.5675, time=496.91s\n",
            "iter 14000: dev acc=0.3406\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-70d63307ee3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbow_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m cbow_losses, cbow_accuracies = train_model(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcbow_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     print_every=1000, eval_every=eval_every)\n",
            "\u001b[0;32m<ipython-input-34-ca64733ad0ef>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;31m# update weights - take a small step in the opposite dir of the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0mprint_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This will plot the validation accuracies across time.\n",
        "plt.title('Validation Accuracy as a function of epochs for the CBOW model')\n",
        "plt.xlabel('Steps of {} epochs each'.format(eval_every))\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.plot(cbow_accuracies)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nttgz5sMLNyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will plot the training loss over time.\n",
        "plt.title('Train Loss as a function of epochs for the CBOW model')\n",
        "plt.xlabel('Steps of {} epochs each'.format(eval_every))\n",
        "plt.ylabel('Train Loss')\n",
        "plt.plot(cbow_losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SnhgqsASNPN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpFt_Fo2TdN0"
      },
      "source": [
        "# Deep CBOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZanOMesTfEZ"
      },
      "source": [
        "To see if we can squeeze some more performance out of the CBOW model, we can make it deeper and non-linear by adding more layers and, e.g., tanh-activations.\n",
        "By using more parameters we can learn more aspects of the data, and by using more layers and non-linearities, we can try to learn a more complex function. \n",
        "This is not something that always works. If the input-output mapping of your data is simple, then a complicated function could easily overfit on your training set, thereby leading to poor generalization. \n",
        "\n",
        "#### Exercise: write Deep CBOW class and train it\n",
        "\n",
        "Write a class `DeepCBOW`.\n",
        "\n",
        "In your code, make sure that your `output_layer` consists of the following:\n",
        "- A linear transformation from E units to D units.\n",
        "- A Tanh activation\n",
        "- A linear transformation from D units to D units\n",
        "- A Tanh activation\n",
        "- A linear transformation from D units to 5 units (our output classes).\n",
        "\n",
        "E is the size of the word embeddings (please use E=300) and D for the size of a hidden layer (please use D=100).\n",
        "\n",
        "We recommend using [nn.Sequential](https://pytorch.org/docs/stable/nn.html?highlight=sequential#torch.nn.Sequential) to implement this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8Z1igvpTrZq"
      },
      "source": [
        "class DeepCBOW(nn.Module):\n",
        "  \"\"\"A simple bag-of-words model\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
        "    super(DeepCBOW, self).__init__()\n",
        "    self.vocab = vocab\n",
        "    \n",
        "    # this is a trainable look-up table with word embeddings\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "    list_of_layers = self._get_init_layers(embedding_dim=embedding_dim, \n",
        "                                      hidden_dims=[hidden_dim, hidden_dim], \n",
        "                                      output_dim=output_dim)\n",
        "    # Add the layers\n",
        "    self.seq_model = nn.Sequential(*list_of_layers)\n",
        "    # this is a trainable bias term\n",
        "    self.bias = nn.Parameter(torch.zeros(embedding_dim), requires_grad=True)    \n",
        "\n",
        "  def _get_init_layers(self, embedding_dim=300, hidden_dims=[100, 100], output_dim=5):\n",
        "    list_of_layers = list()\n",
        "    \n",
        "    if hidden_dims == None or len(hidden_dims) == 0:\n",
        "      # In case of no hidden dims\n",
        "      # Single Linear Layer with bias Term\n",
        "      # mapping the embeddings to the outputs\n",
        "      # Same as the CBOW\n",
        "      linear = nn.Linear(in_features=embedding_dim, \n",
        "                                  out_features=output_dim, \n",
        "                                  bias=True)\n",
        "      list_of_layers.append(linear)\n",
        "    else:\n",
        "      # In case of one or more hidden layers\n",
        "      input_dim = embedding_dim\n",
        "      for hid_dim in hidden_dims:\n",
        "        # Linear Layer\n",
        "        linear = nn.Linear(in_features=input_dim, \n",
        "                                    out_features=hid_dim, \n",
        "                                    bias=True)\n",
        "        list_of_layers.append(linear)\n",
        "        # Followed by a tanh activation function\n",
        "        list_of_layers.append(nn.Tanh())\n",
        "        # The input dimension of the iput of next layer is the\n",
        "        # output dimension of this layer\n",
        "        input_dim = hid_dim\n",
        "      # Last linear layer is the output layer which \n",
        "      # will have as input the last hidden dimension\n",
        "      # and as output the number of outputs of the network  \n",
        "      linear = nn.Linear(in_features=input_dim, \n",
        "                                    out_features=output_dim, \n",
        "                                    bias=True)\n",
        "      list_of_layers.append(linear)\n",
        "    return list_of_layers\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # this is the forward pass of the neural network\n",
        "    # it applies a function to the input and returns the output\n",
        "\n",
        "    # this looks up the embeddings for each word ID in inputs\n",
        "    # the result is a sequence of word embeddings\n",
        "    embeds = self.embed(inputs)\n",
        "    # TODO: Do we need to have an additional bias term here?\n",
        "    # Sum the embeddings related to each input and the bias \n",
        "    logits = embeds.sum(1) + self.bias\n",
        "    # Transform the embeddings to sentiment related vectors\n",
        "    transf_logits = self.seq_model(logits)\n",
        "    # the output is the sum across the time dimension (1)\n",
        "    # with the bias term added\n",
        "    return transf_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep_cbow_model = DeepCBOW(vocab_size=len(v.w2i), embedding_dim=300, hidden_dim=100, output_dim=len(t2i), vocab=v)\n",
        "print(deep_cbow_model)\n",
        "\n",
        "# Move model to device\n",
        "deep_cbow_model = deep_cbow_model.to(device)\n",
        "eval_every=1000\n",
        "optimizer = optim.Adam(deep_cbow_model.parameters(), lr=0.0005)\n",
        "deep_cbow_losses, deep_cbow_accuracies = train_model(\n",
        "    deep_cbow_model, optimizer, num_iterations=30000, \n",
        "    print_every=1000, eval_every=eval_every)"
      ],
      "metadata": {
        "id": "Qx65nKq-d4Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will plot the validation accuracies across time.\n",
        "plt.title('Validation Accuracy as a function of epochs for the Deep CBOW model')\n",
        "plt.xlabel('Steps of {} epochs each'.format(eval_every))\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.plot(deep_cbow_accuracies)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YhsopuF_fTtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will plot the training loss over time.\n",
        "plt.title('Train Loss as a function of epochs for the Deep CBOW model')\n",
        "plt.xlabel('Steps of {} epochs each'.format(eval_every))\n",
        "plt.ylabel('Train Loss')\n",
        "plt.plot(deep_cbow_losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D86mrZHffbSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQZ5flHwiiHY"
      },
      "source": [
        "# Pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NX35vecmHy6"
      },
      "source": [
        "The Stanford Sentiment Treebank is a rather small data set, since it required fine-grained manual annotatation. This makes it difficult for the Deep CBOW model to learn good word embeddings, i.e. to learn good word representations for the words in our vocabulary.\n",
        "In fact, the only error signal that the network receives is from predicting the sentiment of entire sentences!\n",
        "\n",
        "To start off with better word representations, we can download **pre-trained word embeddings**. \n",
        "You can choose which pre-trained word embeddings to use:\n",
        "\n",
        "- **GloVe**. The \"original\" Stanford Sentiment classification [paper](http://aclweb.org/anthology/P/P15/P15-1150.pdf) used Glove embeddings, which are just another method (like *word2vec*) to get word embeddings from unannotated text. Glove is described in the following paper which you should cite if you use them:\n",
        "> Jeffrey Pennington, Richard Socher, and Christopher Manning. [\"Glove: Global vectors for word representation.\"](https://nlp.stanford.edu/pubs/glove.pdf) EMNLP 2014. \n",
        "\n",
        "- **Word2Vec**. This is the method that you learned about in class, described in:\n",
        "> Mikolov, Tomas, et al. [\"Distributed representations of words and phrases and their compositionality.\"](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) Advances in neural information processing systems. 2013.\n",
        "\n",
        "Using these pre-trained word embeddings, we can initialize our word embedding lookup table and start form a point where similar words are already close to one another in the distributional semantic space. \n",
        "\n",
        "You can choose to keep the word embeddings **fixed** or to train them further, specialising them to the task at hand.\n",
        "We will keep them fixed for now.\n",
        "\n",
        "For the purposes of this lab, it is enough if you understand how word2vec works (whichever vectors you use), but if you are interested, we encourage you to also check out the GloVe paper.\n",
        "\n",
        "You can either download the word2vec vectors, or the Glove vectors.\n",
        "If you want to compare your results to the Stanford paper later on, then you should use Glove. \n",
        "**At the end of this lab you have the option to compare which vectors give you the best performance. For now, simply choose one of them and continue with that.**\n",
        "\n",
        "[**OPTIONAL in case you don't want to mount Google Drive:** instead of running all the 5 boxes below, you can 1) download the GloVe and word2vec in your local machine, 2) upload them on your Drive folder (\"My Drive\"). Then, uncomment the first 2 lines in box 6 before writing your code!]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGYr02WWO993",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29ccce8-461b-45af-8b8b-f84f6a00e07c"
      },
      "source": [
        "# This downloads the Glove 840B 300d embeddings.\n",
        "# The original file is at http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "# Since that file is 2GB, we provide you with a *filtered version*\n",
        "# which contains all the words you need for this data set.\n",
        "\n",
        "# You only need to do this once.\n",
        "# Please comment this cell out after downloading.\n",
        "\n",
        "!wget https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-03 14:07:34--  https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53933315 (51M) [text/plain]\n",
            "Saving to: ‘glove.840B.300d.sst.txt’\n",
            "\n",
            "glove.840B.300d.sst 100%[===================>]  51.43M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-12-03 14:07:39 (363 MB/s) - ‘glove.840B.300d.sst.txt’ saved [53933315/53933315]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NLsgFGiTjmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80016189-5693-485d-bb24-b727972ccbee"
      },
      "source": [
        "# This downloads the word2vec 300D Google News vectors \n",
        "# The file has been truncated to only contain words that appear in our data set.\n",
        "# You can find the original file here: https://code.google.com/archive/p/word2vec/\n",
        "\n",
        "# You only need to do this once.\n",
        "# Please comment this out after downloading.\n",
        "!wget https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-03 14:07:42--  https://gist.githubusercontent.com/bastings/4d1c346c68969b95f2c34cfbc00ba0a0/raw/76b4fefc9ef635a79d0d8002522543bc53ca2683/googlenews.word2vec.300d.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 66209703 (63M) [text/plain]\n",
            "Saving to: ‘googlenews.word2vec.300d.txt’\n",
            "\n",
            "googlenews.word2vec 100%[===================>]  63.14M   325MB/s    in 0.2s    \n",
            "\n",
            "2022-12-03 14:07:47 (325 MB/s) - ‘googlenews.word2vec.300d.txt’ saved [66209703/66209703]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXBITzPRQUQb",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89811465-a537-4987-8997-d99905eb2dd7"
      },
      "source": [
        "# Mount Google Drive (to save the downloaded files)\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFvzPuiKSCbl"
      },
      "source": [
        "# Copy word vectors *to* Google Drive\n",
        "\n",
        "# You only need to do this once.\n",
        "# Please comment this out after running it. \n",
        "\n",
        "!cp \"glove.840B.300d.sst.txt\" \"/gdrive/My Drive/\"\n",
        "!cp \"googlenews.word2vec.300d.txt\" \"/gdrive/My Drive/\"\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUMH0bM6BuY9"
      },
      "source": [
        "# If you copied the word vectors to your Drive before,\n",
        "# here is where you copy them back to the Colab notebook.\n",
        "\n",
        "# Copy Glove vectors *from* Google Drive\n",
        "!cp \"/gdrive/My Drive/glove.840B.300d.sst.txt\" .\n",
        "!cp \"/gdrive/My Drive/googlenews.word2vec.300d.txt\" ."
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcpkoh6PIjfe"
      },
      "source": [
        "# Uncomment these 2 lines below if went for the OPTIONAL method described above\n",
        "# !cp \"glove.840B.300d.sst.txt\" \"./\"\n",
        "# !cp \"googlenews.word2vec.300d.txt\" \"./\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX2GJVHILM8n"
      },
      "source": [
        "At this point you have the pre-trained word embedding files, but what do they look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChsChH14Ruxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7cfc39-2432-4bf6-ddfc-5493905522ca"
      },
      "source": [
        "# Exercise: Print the first 4 lines of the files that you downloaded.\n",
        "# What do you see?\n",
        "word2vec_path = \"/gdrive/My Drive/googlenews.word2vec.300d.txt\"\n",
        "glove_path = \"/gdrive/My Drive/glove.840B.300d.sst.txt\"\n",
        "\n",
        "count = 0\n",
        "for line in filereader(\"/gdrive/My Drive/googlenews.word2vec.300d.txt\"):\n",
        "  if count > 4:\n",
        "    break\n",
        "  print(line)\n",
        "  count+=1\n",
        "\n",
        "count = 0\n",
        "print()\n",
        "for line in filereader(\"/gdrive/My Drive/glove.840B.300d.sst.txt\"):\n",
        "  if count > 4:\n",
        "    break\n",
        "  print(line)\n",
        "  count+=1\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in 0.0703125 0.08691406 0.087890625 0.0625 0.06933594 -0.10888672 -0.08154297 -0.15429688 0.020751953 0.13183594 -0.11376953 -0.037353516 0.06933594 0.078125 -0.103027344 -0.09765625 0.044189453 0.10253906 -0.060791016 -0.036132812 -0.045410156 0.04736328 -0.12060547 -0.063964844 0.0022583008 0.037109375 -0.0029144287 0.11767578 0.061767578 0.063964844 0.08105469 -0.068847656 -0.021362305 0.05517578 -0.08544922 0.068847656 -0.12792969 -0.033203125 0.09863281 0.17578125 0.110839844 -0.03466797 -0.04711914 -0.008483887 0.035888672 0.103027344 0.026977539 -0.028686523 -0.005126953 0.10644531 0.059814453 0.09423828 0.033691406 -0.02709961 -0.09423828 0.0010299683 -0.048339844 0.034423828 0.08105469 -0.11328125 -0.08886719 0.035888672 -0.14550781 -0.24414062 -0.061523438 0.052978516 0.056884766 0.1796875 0.061035156 0.08691406 0.12402344 -0.040283203 0.022583008 0.17773438 -0.029663086 -0.029663086 0.1171875 0.03112793 -0.096191406 0.06640625 0.004699707 -0.080078125 0.06298828 -0.020629883 -0.0546875 -0.13574219 -0.06347656 0.083496094 -0.063964844 0.021484375 0.07714844 -0.037109375 -0.033691406 -0.18359375 -0.072753906 0.01586914 0.09326172 -0.061523438 -0.014221191 -0.0034484863 0.011108398 -0.15820312 -0.017089844 0.0061950684 -0.008728027 -0.080566406 -0.015258789 -0.087890625 0.003479004 -0.016113281 -0.012329102 0.09765625 -0.13964844 -0.0859375 -0.026855469 0.053955078 0.1328125 0.11279297 0.12109375 0.08544922 -0.0071105957 0.044677734 -0.14550781 -0.0032043457 -0.11767578 -0.06542969 0.07128906 -0.09423828 -0.030273438 0.12011719 0.080078125 -0.09472656 -0.16210938 -0.07763672 0.021240234 -0.08154297 0.0039367676 -0.15722656 -0.09814453 0.039794922 0.03930664 -0.009094238 0.103027344 0.067871094 -0.04272461 0.06347656 -0.049072266 0.020874023 -0.16699219 0.09326172 0.09375 0.006866455 0.053710938 0.052490234 -0.024414062 -0.032470703 -0.061523438 -0.005554199 0.096191406 0.037841797 0.012207031 -0.043945312 -0.0074768066 0.10546875 0.020385742 0.14550781 0.08203125 0.0057678223 0.0045776367 -0.09277344 -0.13867188 -0.057373047 -0.051513672 -0.13085938 -0.13964844 -0.020507812 -0.02709961 0.032714844 0.10498047 -0.0023345947 -0.022583008 0.00050354004 -0.110839844 0.08496094 -0.12988281 -0.017456055 -0.00035858154 0.107910156 0.08886719 0.044677734 0.025146484 0.023803711 0.08105469 0.02368164 -0.10986328 0.0053710938 -0.017700195 -0.033935547 -0.032958984 -0.1640625 0.095703125 -0.018310547 0.0053100586 -0.034423828 -0.044189453 -0.06640625 -0.017944336 -0.029663086 -0.007598877 -0.05126953 -0.05419922 0.08935547 -0.071777344 0.015258789 -0.08251953 -0.03173828 0.03564453 -0.021240234 -0.059326172 -0.013061523 0.046875 0.023071289 0.020996094 -0.07861328 -0.008056641 0.01953125 -0.005554199 0.041503906 0.027832031 0.01361084 0.03466797 -0.18261719 0.12011719 0.07421875 -0.041015625 -0.0099487305 0.04296875 -0.007293701 0.123046875 0.057617188 -0.053466797 -0.032226562 -0.009094238 -0.04663086 0.043945312 -0.05078125 0.068847656 0.0029907227 -0.004180908 -0.044189453 0.07373047 -0.012756348 0.06738281 0.006286621 0.07519531 -0.037841797 0.0048828125 0.044677734 -0.06738281 0.00970459 0.0047302246 0.020507812 0.07128906 0.17089844 0.17382812 0.055664062 0.091308594 -0.037353516 0.049804688 -0.03930664 0.044189453 0.0625 0.048583984 -0.053222656 0.048828125 -0.13085938 -0.028930664 -0.036132812 -0.060791016 -0.057373047 0.123046875 -0.08251953 -0.0119018555 0.125 0.0013580322 0.063964844 -0.10644531 -0.14355469 -0.042236328 0.024047852 -0.16894531 -0.08886719 -0.080566406 0.064941406 0.061279297 -0.04736328 -0.05883789 -0.047607422 0.014465332 -0.0625\n",
            "for -0.011779785 -0.04736328 0.044677734 0.06347656 -0.018188477 -0.063964844 -0.0013122559 -0.072265625 0.064453125 0.08642578 -0.16992188 -0.039794922 0.07128906 -0.025878906 0.018188477 0.13671875 0.14453125 -0.033691406 -0.09765625 -0.12011719 -0.079589844 0.0625 -0.06689453 0.07421875 0.022705078 0.033447266 -0.18066406 0.052001953 0.0138549805 0.09277344 0.0035095215 -0.009094238 -0.09716797 0.067871094 -0.0087890625 0.044189453 -0.13378906 -0.099609375 0.033203125 0.027954102 0.15527344 -0.017700195 0.014282227 -0.10986328 -0.08544922 -0.07324219 -0.024658203 0.17285156 0.061767578 0.08935547 -0.024291992 0.14160156 -0.032958984 0.02746582 -0.15527344 0.007873535 -0.07080078 0.043701172 0.006011963 -0.055908203 -0.14746094 0.028442383 -0.1328125 -0.17675781 -0.091308594 -0.05078125 -0.026000977 -0.1484375 -0.080566406 0.15039062 -0.04345703 0.07910156 0.033203125 0.09033203 0.022705078 -0.0625 0.1640625 0.0859375 -0.012390137 0.19628906 -0.06225586 0.022460938 -0.030151367 0.021240234 0.003326416 -0.055419922 -0.07324219 0.029785156 0.049804688 0.017456055 0.10449219 0.03881836 0.08496094 -0.24804688 0.06933594 -0.14941406 0.05834961 0.095703125 -0.033447266 0.06298828 0.021362305 -0.14550781 0.053710938 -0.09082031 -0.025390625 0.045410156 0.0053100586 -0.115722656 -0.01953125 0.12109375 0.032226562 0.09472656 -0.064453125 0.022705078 0.12060547 0.060302734 0.12060547 0.048828125 0.09326172 0.06689453 0.029296875 -0.034179688 -0.111328125 0.053466797 -0.025634766 0.017822266 0.06225586 -0.025878906 0.14550781 0.0625 0.107910156 -0.16308594 -0.09765625 -0.10595703 -0.08544922 -0.08886719 0.10107422 -0.079589844 0.008422852 0.024047852 0.13085938 0.05126953 0.08154297 0.09375 -0.05859375 -0.09667969 -0.028320312 -0.14550781 -0.14746094 0.14550781 -0.017578125 0.032958984 -0.08544922 -0.010986328 -0.037109375 -0.013671875 0.035888672 -0.008239746 0.05029297 -0.09472656 0.047851562 0.020751953 0.030639648 0.12988281 0.052734375 0.018798828 -0.017578125 0.03491211 0.018310547 -0.009887695 -0.18457031 -0.08984375 -0.029052734 -0.060791016 -0.05126953 -0.0023651123 0.06640625 -0.08251953 -0.040039062 0.096191406 -0.15429688 -0.15332031 0.028320312 0.013122559 0.029907227 -0.012145996 -0.09667969 0.024780273 0.19335938 0.013000488 0.024169922 -0.035888672 0.09863281 -0.09667969 -0.20019531 -0.013793945 0.0859375 -0.080078125 -0.17675781 -0.17480469 0.005126953 -0.03491211 -0.0546875 0.09375 -0.09326172 -0.011962891 -0.0005645752 0.09765625 0.024780273 -0.039794922 0.009765625 0.11816406 0.025756836 0.123046875 0.064453125 0.07080078 0.029296875 -0.049560547 -0.078125 0.028930664 0.045654297 -0.04296875 0.025878906 -0.051757812 0.140625 0.004272461 -0.037841797 0.02746582 0.060058594 0.028320312 0.028076172 -0.036621094 0.13085938 -9.679794e-05 -0.06933594 -0.022094727 0.067871094 -0.02331543 -0.015319824 -0.05834961 0.061035156 0.00064468384 0.0039978027 -0.07128906 0.091796875 0.026245117 0.020019531 0.03540039 -0.057861328 -0.029663086 0.02734375 0.025146484 0.060302734 0.13183594 -0.0043640137 0.0027313232 0.059814453 0.09863281 -0.091796875 -0.045898438 -0.017456055 0.038330078 -0.019165039 0.04638672 0.047851562 0.09814453 -0.040283203 0.09423828 -0.03466797 -0.042236328 0.0703125 -0.013671875 0.10644531 0.016479492 0.13183594 -0.0016937256 -0.008483887 -0.14257812 -0.04663086 -0.10986328 0.08203125 -0.041015625 -0.018920898 0.087890625 -0.0028076172 0.23828125 -0.04711914 -0.022949219 0.040771484 0.029296875 -0.022583008 0.0037231445 -0.08251953 0.08154297 0.00793457 0.00047683716 0.018432617 0.07128906 -0.03491211 0.024169922\n",
            "that -0.01574707 -0.028320312 0.083496094 0.05029297 -0.11035156 0.03173828 -0.014221191 -0.08984375 0.11767578 0.11816406 -0.071777344 -0.07714844 -0.068847656 0.07714844 -0.13867188 0.006500244 0.010986328 -0.015136719 -0.0009613037 -0.030273438 -0.00015830994 0.038330078 -0.024169922 -0.045898438 0.09472656 -0.05517578 -0.064941406 0.0061035156 0.0008544922 0.06201172 -0.05444336 0.014099121 0.022216797 -0.044921875 0.111328125 -0.03857422 0.05126953 0.025146484 0.016967773 0.06298828 0.13769531 0.13574219 0.06542969 -0.064453125 -0.024047852 -0.013366699 -0.037109375 0.0043029785 -0.01574707 0.019042969 0.10839844 0.044677734 -0.044921875 -0.095214844 0.08691406 0.08203125 0.0068359375 -0.13183594 0.0027313232 -0.075683594 0.022460938 0.171875 -0.048583984 0.038330078 -0.088378906 -0.017211914 0.021850586 0.13378906 0.010681152 0.049804688 0.038085938 -0.0052490234 0.061279297 -0.050048828 -0.10595703 0.055664062 0.15429688 0.17089844 0.048095703 0.12695312 0.08154297 -0.1015625 0.043945312 -0.013549805 -0.13378906 -0.15722656 0.024291992 0.20898438 -0.10205078 -0.09375 0.048583984 0.07519531 0.0035247803 -0.123535156 -0.024169922 -0.0013198853 0.038330078 0.007598877 0.017700195 0.043701172 -0.09814453 -0.05810547 0.014099121 0.041015625 -0.03540039 -0.022949219 -0.13085938 -0.14453125 0.028930664 -0.122558594 -0.07128906 -0.0071411133 -0.09667969 0.05859375 0.104003906 0.026367188 0.0075683594 0.036132812 0.040283203 0.052734375 -0.20410156 -0.033447266 -0.029052734 0.03173828 -0.123535156 -0.06738281 0.01586914 -0.08642578 0.012512207 -0.06298828 -0.12060547 0.0234375 -0.13183594 -0.11816406 -0.012145996 -0.10986328 0.095214844 -0.041992188 0.018310547 0.051757812 0.014953613 -0.15917969 -0.021728516 -0.005126953 0.035888672 -0.003036499 -0.078125 -0.05126953 -0.0074157715 0.087402344 -0.023925781 0.044189453 -0.11376953 0.021484375 -0.003829956 -0.04345703 -0.104003906 -0.18554688 -0.044921875 0.044921875 -0.044433594 0.019042969 -0.020996094 0.0134887695 0.015258789 -0.030395508 0.04663086 0.10253906 -0.0138549805 0.008239746 -0.052734375 0.01965332 -0.076171875 -0.08984375 0.043945312 0.00793457 0.012634277 -0.10839844 -0.018554688 -0.10449219 -0.15234375 -0.07910156 -0.028808594 -0.018676758 0.09716797 -0.008361816 -0.03540039 0.007537842 0.05810547 0.08203125 0.028808594 0.05102539 0.0031738281 -0.018554688 0.014953613 -0.072753906 -0.034179688 0.043945312 -0.048828125 -0.16113281 0.025634766 0.036621094 -0.00015354156 -0.0012588501 -0.039794922 -0.032226562 -0.01574707 0.060791016 0.09716797 -0.015014648 -0.033691406 -0.015319824 0.0046691895 0.032958984 -0.08203125 0.10644531 0.15429688 0.0087890625 -0.12011719 0.061279297 0.05859375 0.03881836 -0.015991211 -0.043945312 0.12011719 -0.07519531 0.125 0.03515625 0.072753906 0.087402344 0.021118164 -0.07373047 0.034423828 -0.09423828 0.01159668 -0.057128906 -0.07861328 0.015991211 0.075683594 0.030517578 0.0046081543 -0.14550781 -0.024780273 -0.107910156 0.068847656 0.05810547 0.125 -0.06542969 0.005279541 0.01184082 0.06982422 0.12695312 0.06542969 -0.017333984 0.119140625 -0.032470703 0.14453125 0.076660156 -0.032226562 -0.06591797 0.06298828 -0.0625 -0.096191406 0.10644531 -0.039794922 0.11621094 -0.00970459 -0.03540039 -0.06542969 0.05883789 0.16210938 0.05126953 0.15917969 0.095214844 0.076171875 -0.091796875 0.025146484 -0.07861328 0.08935547 -0.05859375 -0.040039062 0.045898438 0.03100586 0.0390625 0.03564453 -0.10595703 -0.037109375 -0.16113281 0.021362305 0.0012207031 -0.011291504 -0.015625 -0.033447266 -0.020629883 -0.01940918 0.063964844 0.020141602 0.006866455 0.061035156 -0.1484375\n",
            "is 0.0070495605 -0.07324219 0.171875 0.022583008 -0.1328125 0.19824219 0.11279297 -0.107910156 0.071777344 0.020874023 -0.123046875 -0.05908203 0.10107422 0.0107421875 0.14355469 0.25976562 -0.036376953 0.18554688 -0.07861328 -0.022705078 -0.12060547 0.17773438 0.049560547 0.017211914 0.079589844 -0.045654297 -0.18847656 0.18945312 -0.02319336 0.06298828 0.09765625 -0.019042969 -0.07910156 0.15234375 0.17382812 0.1015625 -0.16308594 0.114746094 0.10058594 -0.09277344 0.109375 0.05883789 -0.021606445 0.06347656 0.041992188 -0.008850098 0.032226562 0.10644531 0.064453125 -0.118652344 0.030517578 0.06689453 0.12207031 -0.08300781 0.171875 0.07861328 0.095214844 -0.0077819824 0.02319336 0.0234375 -0.016845703 0.15527344 -0.10986328 -0.17675781 -0.11621094 0.0234375 -0.010620117 0.052734375 -0.13378906 0.079589844 0.07373047 0.043945312 0.115234375 -0.020629883 0.07470703 -0.0115356445 0.080566406 0.041748047 0.080078125 0.3515625 0.09667969 -0.21289062 0.16503906 -0.078125 0.06982422 -0.0013961792 -0.091308594 0.12988281 0.25195312 -0.016113281 0.09326172 -0.14648438 -0.0015106201 -0.15136719 -0.026855469 -0.15722656 0.026367188 0.0859375 0.071777344 0.07714844 -0.0390625 0.05444336 -0.12792969 0.091308594 -0.18457031 -0.037597656 -0.027954102 -0.08984375 -0.11669922 -0.09863281 0.048095703 -0.16210938 -0.10888672 0.08496094 -0.045654297 0.15820312 -0.038085938 -0.08203125 0.203125 0.08642578 0.06933594 0.032226562 -0.16015625 0.09472656 -0.024658203 0.05419922 0.027954102 0.044921875 0.16992188 0.072753906 -0.036376953 -0.010253906 -0.017089844 -0.107421875 -0.0007019043 -0.07373047 0.25390625 0.056640625 0.03515625 -0.008605957 0.18554688 0.021484375 0.26367188 -0.023803711 -0.099121094 -0.041259766 -0.06933594 -0.11376953 0.050048828 -0.05883789 0.046142578 0.087402344 0.10546875 0.10644531 0.027954102 0.09472656 0.11621094 -0.17285156 -0.03491211 -0.20800781 0.059570312 0.104003906 -0.0017929077 0.05859375 -0.029785156 -0.037597656 0.048583984 -0.063964844 0.079589844 0.06933594 -0.10498047 -0.14453125 0.04345703 -0.068847656 -0.03564453 -0.01171875 0.013671875 -0.06591797 0.119140625 0.03125 -0.04638672 -0.0019683838 0.0073547363 -0.056640625 0.027832031 0.08251953 -0.0134887695 0.071777344 0.14453125 0.12792969 0.042236328 0.14160156 -0.018066406 0.021606445 -0.091796875 0.13378906 -0.1953125 -0.05029297 -0.037841797 -0.096191406 0.103027344 -0.106933594 -0.14746094 0.099609375 -0.23046875 0.22753906 -0.07519531 0.064941406 0.091796875 0.046875 0.06298828 0.06982422 0.046142578 0.09716797 -0.20214844 0.19921875 0.18652344 -0.119628906 -0.14257812 0.15039062 -0.033691406 -0.14550781 -0.0006904602 -0.07324219 0.13378906 0.03564453 -0.022949219 0.027709961 -0.07910156 0.20703125 -0.083496094 -0.049560547 0.03149414 0.1484375 0.055664062 -0.044921875 -0.079589844 0.004760742 -0.020751953 0.060058594 0.004760742 0.011169434 0.17285156 -0.13476562 0.030761719 -0.079589844 0.09033203 0.061035156 0.07714844 -0.05029297 -0.092285156 -0.26757812 0.107910156 0.0859375 0.06298828 0.107910156 -0.026733398 0.10205078 -0.12060547 0.052978516 0.09472656 -0.16503906 0.044189453 0.072265625 0.041259766 0.42578125 -0.103027344 -0.16015625 -0.09033203 -0.063964844 -0.048095703 0.14453125 0.06542969 0.049316406 0.05419922 0.13574219 -0.01928711 -0.21582031 -0.07421875 -0.14648438 0.011474609 -0.16503906 -0.10498047 0.0032043457 0.13476562 -0.003967285 -0.103515625 -0.13964844 0.10449219 -0.012573242 -0.23339844 -0.036376953 -0.09375 0.18261719 0.02709961 0.12792969 -0.024780273 0.011230469 0.1640625 0.106933594\n",
            "on 0.026733398 -0.09082031 0.027832031 0.20410156 0.006225586 -0.09033203 0.022583008 -0.16113281 0.1328125 0.061035156 -0.01574707 0.088378906 0.013793945 0.04638672 -0.055908203 -0.06689453 0.012268066 0.13671875 0.15429688 -0.046142578 -0.03930664 -0.15429688 -0.16503906 0.107910156 0.033203125 -0.05102539 0.037109375 0.1015625 0.11035156 0.020507812 0.0067749023 0.0011825562 -0.012512207 -0.125 0.014831543 -0.026855469 -0.021484375 0.015075684 0.13867188 0.048583984 -0.076660156 -0.11669922 0.106933594 0.041748047 0.012817383 -0.009460449 -0.028930664 -0.03857422 0.24316406 0.009521484 0.022094727 0.22265625 0.009155273 -0.045410156 -0.03540039 0.140625 -0.18457031 0.07763672 0.041503906 -0.08496094 -0.099121094 0.05834961 -0.09667969 -0.20214844 -0.014038086 -0.0023651123 0.14746094 0.20019531 0.059570312 0.15429688 0.13476562 0.005279541 0.125 0.08544922 -0.027709961 -0.05810547 0.18359375 0.007873535 -0.15332031 0.12402344 -0.080078125 -0.14355469 0.14941406 0.014587402 0.107910156 -0.20117188 -0.15039062 0.052490234 0.07714844 0.091796875 -0.038085938 0.1484375 0.0546875 -0.15136719 0.014282227 -0.10498047 0.019042969 -0.06347656 0.053466797 0.03491211 0.13964844 -0.13378906 0.21679688 -0.19433594 -0.05834961 -0.13476562 -0.265625 -0.104003906 0.03540039 -0.21582031 0.08251953 0.045166016 -0.06982422 -0.04321289 0.026977539 -0.09033203 0.005493164 0.049804688 -0.03564453 0.059814453 -0.14941406 -0.022094727 -0.033203125 0.17578125 -0.06640625 -0.018310547 0.011291504 -0.042236328 -0.07714844 0.017456055 -0.10498047 -0.10449219 -0.04736328 -0.029541016 -0.061523438 -0.05078125 -0.025634766 -0.095214844 -0.08105469 -0.1015625 0.20214844 0.118652344 -0.002822876 -0.060302734 0.022460938 0.13085938 0.080566406 -0.15429688 -0.08251953 0.16015625 0.057861328 0.09765625 -0.020996094 -0.045166016 -0.07324219 0.0043640137 -0.09082031 0.019165039 -0.016601562 -0.05029297 0.014709473 -0.0041503906 0.03466797 0.057373047 0.080078125 0.006225586 0.063964844 0.024536133 0.03173828 -0.125 -0.078125 -0.024536133 -0.072265625 -0.08642578 -0.07714844 0.04345703 -0.00018787384 -0.011413574 -0.099121094 0.026245117 0.053466797 0.045410156 -0.07128906 0.13867188 0.041015625 0.011169434 -0.015319824 0.032958984 0.18261719 0.017456055 -0.031982422 0.107910156 0.032958984 -0.03515625 -0.21777344 0.10205078 -0.029296875 -0.0009460449 -0.0071411133 -0.026367188 0.061767578 -0.016967773 -0.021728516 -0.119140625 0.009094238 0.103027344 -0.0030059814 0.14941406 0.10595703 -0.040283203 -0.018432617 0.035888672 -0.038085938 0.056884766 0.015319824 0.01977539 0.18066406 0.008178711 -0.15136719 0.032226562 0.15722656 0.05078125 -0.028930664 0.043945312 -0.05859375 0.0030975342 -0.012634277 0.16113281 0.10595703 -0.033935547 0.18164062 -0.044677734 0.034179688 -0.037841797 -0.008850098 -0.036865234 0.07861328 0.02709961 0.046142578 0.068847656 0.05053711 -0.0017471313 -0.13671875 -0.15332031 0.09863281 -0.16113281 0.0066223145 -0.0859375 -0.017578125 0.040771484 0.029907227 0.011413574 -0.020263672 -0.064453125 0.017456055 -0.12890625 -0.00034713745 0.042236328 0.0032958984 0.122558594 -0.095703125 0.092285156 0.10498047 -0.12451172 0.035888672 0.14550781 -0.10546875 0.022949219 -0.008361816 0.004638672 0.21972656 -0.049560547 0.23828125 -0.05834961 0.048339844 0.060546875 -0.037353516 -0.17773438 0.044921875 -0.042236328 0.08251953 0.11035156 -0.109375 0.09423828 -0.072265625 0.049072266 -0.15820312 0.078125 0.029541016 -0.12109375 0.026855469 -0.027954102 0.030883789 0.040527344 -0.13085938 0.08300781 0.01574707 -0.11669922 -0.029418945 -0.07080078\n",
            "\n",
            ", -0.082752 0.67204 -0.14987 -0.064983 0.056491 0.40228 0.0027747 -0.3311 -0.30691 2.0817 0.031819 0.013643 0.30265 0.0071297 -0.5819 -0.2774 -0.062254 1.1451 -0.24232 0.1235 -0.12243 0.33152 -0.006162 -0.30541 -0.13057 -0.054601 0.037083 -0.070552 0.5893 -0.30385 0.2898 -0.14653 -0.27052 0.37161 0.32031 -0.29125 0.0052483 -0.13212 -0.052736 0.087349 -0.26668 -0.16897 0.015162 -0.0083746 -0.14871 0.23413 -0.20719 -0.091386 0.40075 -0.17223 0.18145 0.37586 -0.28682 0.37289 -0.16185 0.18008 0.3032 -0.13216 0.18352 0.095759 0.094916 0.008289 0.11761 0.34046 0.03677 -0.29077 0.058303 -0.027814 0.082941 0.1862 -0.031494 0.27985 -0.074412 -0.13762 -0.21866 0.18138 0.040855 -0.113 0.24107 0.3657 -0.27525 -0.05684 0.34872 0.011884 0.14517 -0.71395 0.48497 0.14807 0.62287 0.20599 0.58379 -0.13438 0.40207 0.18311 0.28021 -0.42349 -0.25626 0.17715 -0.54095 0.16596 -0.036058 0.08499 -0.64989 0.075549 -0.28831 0.40626 -0.2802 0.094062 0.32406 0.28437 -0.26341 0.11553 0.071918 -0.47215 -0.18366 -0.34709 0.29964 -0.66514 0.002516 -0.42333 0.27512 0.36012 0.16311 0.23964 -0.05923 0.3261 0.20559 0.038677 -0.045816 0.089764 0.43151 -0.15954 0.08532 -0.26572 -0.15001 0.084286 -0.16714 -0.43004 0.060807 0.13121 -0.24112 0.66554 0.4453 -0.18019 -0.13919 0.56252 0.21457 -0.46443 -0.012211 0.029988 -0.051094 -0.20135 0.80788 0.47377 -0.057647 0.46216 0.16084 -0.20954 -0.05452 0.15572 -0.13712 0.12972 -0.011936 -0.003378 -0.13595 -0.080711 0.20065 0.054056 0.046816 0.059539 0.046265 0.17754 -0.31094 0.28119 -0.24355 0.085252 -0.21011 -0.19472 0.0027297 -0.46341 0.14789 -0.31517 -0.065939 0.036106 0.42903 -0.33759 0.16432 0.32568 -0.050392 -0.054297 0.24074 0.41923 0.13012 -0.17167 -0.37808 -0.23089 -0.019477 -0.29291 -0.30824 0.30297 -0.22659 0.081574 -0.18516 -0.21408 0.40616 -0.28974 0.074174 -0.17795 0.28595 -0.039626 -0.2339 -0.36054 -0.067503 -0.091065 0.23438 -0.0041331 0.003232 0.0072134 0.008697 0.21614 0.049904 0.35582 0.13748 0.073361 0.14166 0.2412 -0.013322 0.15613 0.083381 0.088146 -0.019357 0.43795 0.083961 0.45309 -0.50489 -0.10865 -0.2527 -0.18251 0.20441 0.13319 0.1294 0.050594 -0.15612 -0.39543 0.12538 0.24881 -0.1927 -0.31847 -0.12719 0.4341 0.31177 -0.0040946 -0.2094 -0.079961 0.1161 -0.050794 0.015266 -0.2803 -0.12486 0.23587 0.2339 -0.14023 0.028462 0.56923 -0.1649 -0.036429 0.010051 -0.17107 -0.042608 0.044965 -0.4393 -0.26137 0.30088 -0.060772 -0.45312 -0.19076 -0.20288 0.27694 -0.060888 0.11944 0.62206 -0.19343 0.47849 -0.30113 0.059389 0.074901 0.061068 -0.4662 0.40054 -0.19099 -0.14331 0.018267 -0.18643 0.20709 -0.35598 0.05338 -0.050821 -0.1918 -0.37846 -0.06589\n",
            ". 0.012001 0.20751 -0.12578 -0.59325 0.12525 0.15975 0.13748 -0.33157 -0.13694 1.7893 -0.47094 0.70434 0.26673 -0.089961 -0.18168 0.067226 0.053347 1.5595 -0.2541 0.038413 -0.01409 0.056774 0.023434 0.024042 0.31703 0.19025 -0.37505 0.035603 0.1181 0.012032 -0.037566 -0.5046 -0.049261 0.092351 0.11031 -0.073062 0.33994 0.28239 0.13413 0.070128 -0.022099 -0.28103 0.49607 -0.48693 -0.090964 -0.1538 -0.38011 -0.014228 -0.19392 -0.11068 -0.014088 -0.17906 0.24509 -0.16878 -0.15351 -0.13808 0.02151 0.13699 0.0068061 -0.14915 -0.38169 0.12727 0.44007 0.32678 -0.46117 0.068687 0.34747 0.18827 -0.31837 0.4447 -0.2095 -0.26987 0.48945 0.15388 0.05295 -0.049831 0.11207 0.14881 -0.37003 0.30777 -0.33865 0.045149 -0.18987 0.26634 -0.26401 -0.47556 0.68381 -0.30653 0.24606 0.31611 -0.071098 0.030417 0.088119 0.045025 0.20125 -0.21618 -0.36371 -0.25948 -0.42398 -0.14305 -0.10208 0.21498 -0.21924 -0.17935 0.21546 0.13801 0.24504 -0.2559 0.054815 0.21307 0.2564 -0.25673 0.17961 -0.47638 -0.25181 -0.0091498 -0.054362 -0.21007 0.12597 -0.40795 -0.021164 0.20585 0.18925 -0.0051896 -0.51394 0.28862 -0.077748 -0.27676 0.46567 -0.14225 -0.17879 -0.4357 -0.32481 0.15034 -0.058367 0.49652 0.20472 0.019866 0.13326 0.12823 -1.0177 0.29007 0.28995 0.029994 -0.10763 0.28665 -0.24387 0.22905 -0.26249 -0.069269 -0.17889 0.21936 0.15146 0.04567 -0.050497 0.071482 -0.1027 -0.080705 0.30296 0.031302 0.26613 -0.0060951 0.10313 -0.39987 -0.043945 -0.057625 0.08702 -0.098152 0.22835 -0.005211 0.038075 0.01591 -0.20622 0.021853 0.0040426 -0.043063 -0.002294 -0.26097 -0.25802 -0.28158 -0.23118 -0.010404 -0.30102 -0.4042 0.014653 -0.10445 0.30377 -0.20957 0.3119 0.068272 0.1008 0.010423 0.54011 0.29865 0.12653 0.013761 0.21738 -0.39521 0.066633 0.50327 0.14913 -0.11554 0.010042 0.095698 0.16607 -0.18808 0.055019 0.026715 -0.3164 -0.046583 -0.051591 0.023475 -0.11007 0.085642 0.28394 0.040497 0.071986 0.14157 -0.021199 0.44718 0.20088 -0.12964 -0.067183 0.47614 0.13394 -0.17287 -0.37324 -0.17285 0.02683 -0.1316 0.09116 -0.46487 0.1274 -0.090159 -0.10552 0.068006 -0.13381 0.17056 0.089509 -0.23133 -0.27572 0.061534 -0.051646 0.28377 0.25286 -0.24139 -0.19905 0.12049 -0.1011 0.27392 0.27843 0.26449 -0.18292 -0.048961 0.19198 0.17192 0.33659 -0.20184 -0.34305 -0.24553 -0.15399 0.3945 0.22839 -0.25753 -0.25675 -0.37332 -0.23884 -0.048816 0.78323 0.18851 -0.26477 0.096566 0.062658 -0.30668 -0.43334 0.10006 0.21136 0.039459 -0.11077 0.24421 0.60942 -0.46646 0.086385 -0.39702 -0.23363 0.021307 -0.10778 -0.2281 0.50803 0.11567 0.16165 -0.066737 -0.29556 0.022612 -0.28135 0.0635 0.14019 0.13871 -0.36049 -0.035\n",
            "the 0.27204 -0.06203 -0.1884 0.023225 -0.018158 0.0067192 -0.13877 0.17708 0.17709 2.5882 -0.35179 -0.17312 0.43285 -0.10708 0.15006 -0.19982 -0.19093 1.1871 -0.16207 -0.23538 0.003664 -0.19156 -0.085662 0.039199 -0.066449 -0.04209 -0.19122 0.011679 -0.37138 0.21886 0.0011423 0.4319 -0.14205 0.38059 0.30654 0.020167 -0.18316 -0.0065186 -0.0080549 -0.12063 0.027507 0.29839 -0.22896 -0.22882 0.14671 -0.076301 -0.1268 -0.0066651 -0.052795 0.14258 0.1561 0.05551 -0.16149 0.09629 -0.076533 -0.049971 -0.010195 -0.047641 -0.16679 -0.2394 0.0050141 -0.049175 0.013338 0.41923 -0.10104 0.015111 -0.077706 -0.13471 0.119 0.10802 0.21061 -0.051904 0.18527 0.17856 0.041293 -0.014385 -0.082567 -0.035483 -0.076173 -0.045367 0.089281 0.33672 -0.22099 -0.0067275 0.23983 -0.23147 -0.88592 0.091297 -0.012123 0.013233 -0.25799 -0.02972 0.016754 0.01369 0.32377 0.039546 0.042114 -0.088243 0.30318 0.087747 0.16346 -0.40485 -0.043845 -0.040697 0.20936 -0.77795 0.2997 0.2334 0.14891 -0.39037 -0.053086 0.062922 0.065663 -0.13906 0.094193 0.10344 -0.2797 0.28905 -0.32161 0.020687 0.063254 -0.23257 -0.4352 -0.017049 -0.32744 -0.047064 -0.075149 -0.18788 -0.015017 0.029342 -0.3527 -0.044278 -0.13507 -0.11644 -0.1043 0.1392 0.0039199 0.37603 0.067217 -0.37992 -1.1241 -0.057357 -0.16826 0.03941 0.2604 -0.023866 0.17963 0.13553 0.2139 0.052633 -0.25033 -0.11307 0.22234 0.066597 -0.11161 0.062438 -0.27972 0.19878 -0.36262 -1.0006e-05 -0.17262 0.29166 -0.15723 0.054295 0.06101 -0.39165 0.2766 0.057816 0.39709 0.025229 0.24672 -0.08905 0.15683 -0.2096 -0.22196 0.052394 -0.01136 0.050417 -0.14023 -0.042825 -0.031931 -0.21336 -0.20402 -0.23272 0.07449 0.088202 -0.11063 -0.33526 -0.014028 -0.29429 -0.086911 -0.1321 -0.43616 0.20513 0.0079362 0.48505 0.064237 0.14261 -0.43711 0.12783 -0.13111 0.24673 -0.27496 0.15896 0.43314 0.090286 0.24662 0.066463 -0.20099 0.1101 0.03644 0.17359 -0.15689 -0.086328 -0.17316 0.36975 -0.40317 -0.064814 -0.034166 -0.013773 0.062854 -0.17183 -0.12366 -0.034663 -0.22793 -0.23172 0.239 0.27473 0.15332 0.10661 -0.060982 -0.024805 -0.13478 0.17932 -0.37374 -0.02893 -0.11142 -0.08389 -0.055932 0.068039 -0.10783 0.1465 0.094617 -0.084554 0.067429 -0.3291 0.034082 -0.16747 -0.25997 -0.22917 0.020159 -0.02758 0.16136 -0.18538 0.037665 0.57603 0.20684 0.27941 0.16477 -0.018769 0.12062 0.069648 0.059022 -0.23154 0.24095 -0.3471 0.04854 -0.056502 0.41566 -0.43194 0.4823 -0.051759 -0.27285 -0.25893 0.16555 -0.1831 -0.06734 0.42457 0.010346 0.14237 0.25939 0.17123 -0.13821 -0.066846 0.015981 -0.30193 0.043579 -0.043102 0.35025 -0.19681 -0.4281 0.16899 0.22511 -0.28557 -0.1028 -0.018168 0.11407 0.13015 -0.18317 0.1323\n",
            "and -0.18567 0.066008 -0.25209 -0.11725 0.26513 0.064908 0.12291 -0.093979 0.024321 2.4926 -0.017916 -0.071218 -0.24782 -0.26237 -0.2246 -0.21961 -0.12927 1.0867 -0.66072 -0.031617 -0.057328 0.056903 -0.27939 -0.39825 0.14251 -0.085146 -0.14779 0.055067 -0.0028687 -0.20917 -0.070735 0.22577 -0.15881 -0.10395 0.09711 -0.56251 -0.32929 -0.20853 0.0098711 0.049777 0.0014883 0.15884 0.042771 -0.0026956 -0.02462 -0.19213 -0.22556 0.10838 0.090086 -0.13291 0.32559 -0.17038 -0.1099 -0.23986 -0.024289 0.014656 -0.237 0.084828 -0.35982 -0.076746 0.048909 0.11431 -0.21013 0.24765 -0.017531 -0.14028 0.046191 0.22972 0.1175 0.12724 0.012992 0.4587 0.41085 0.039106 0.15713 -0.18376 0.26834 0.056662 0.16844 -0.053788 -0.091892 0.11193 -0.08681 -0.13324 0.15062 -0.31733 -0.22078 0.25038 0.34131 0.36419 -0.089514 -0.22193 0.24471 0.040091 0.47798 -0.029996 0.0019212 0.063511 -0.20417 -0.26478 0.20649 0.015573 -0.27722 -0.18861 -0.10289 -0.49773 0.14986 -0.010877 0.25085 -0.28117 0.18966 -0.065879 0.094753 -0.15338 -0.055071 -0.36747 0.24993 0.096527 0.23538 0.18405 0.052859 0.22967 0.12582 0.15536 -0.17275 0.33946 -0.10049 0.074948 -0.093575 -0.04049 -0.016922 -0.0058039 -0.18108 0.19537 0.45178 0.10965 0.2337 -0.09905 -0.078633 0.21678 -0.71231 -0.099759 0.33333 -0.1646 -0.091688 0.21056 0.023669 0.028922 0.1199 -0.12512 -0.026037 -0.062217 0.55816 0.0050273 -0.30888 0.038611 0.17568 -0.11163 -0.10815 -0.19444 0.29433 0.14519 -0.042878 0.18534 0.018891 -0.61883 0.13352 0.036007 0.33995 0.22109 -0.079328 0.071319 0.17678 0.16378 -0.23142 -0.1434 -0.098122 -0.019286 0.2356 -0.34013 -0.061007 -0.23208 -0.31152 0.10063 -0.15957 0.20183 -0.016345 -0.12303 0.022667 -0.20986 -0.20127 -0.087883 0.064731 0.10195 -0.1786 0.33056 0.21407 -0.32165 -0.17106 0.19407 -0.38618 -0.2148 -0.052254 0.023175 0.47389 0.18612 0.12711 0.20855 -0.10256 -0.12016 -0.40488 0.029695 -0.027419 -0.0085227 -0.11415 0.081134 -0.17228 0.19142 0.026514 0.043789 -0.12399 0.13354 0.10112 0.081682 -0.15085 0.0075806 -0.18971 0.24669 0.22491 0.35553 -0.3277 -0.21821 0.1402 0.28604 0.055226 -0.086544 0.02111 -0.19236 0.074245 0.076782 0.00081666 0.034097 -0.57719 0.10657 0.28134 -0.11964 -0.68281 -0.32893 -0.24442 -0.025847 0.0091273 0.2025 -0.050959 -0.11042 0.010962 0.076773 0.40048 -0.40739 -0.44773 0.31954 -0.036326 -0.012789 -0.17282 0.1476 0.2356 0.080642 -0.36528 -0.0083443 0.6239 -0.24379 0.019917 -0.28803 -0.010494 0.038412 -0.11718 -0.072462 0.16381 0.38488 -0.029783 0.23444 0.4532 0.14815 -0.027021 -0.073181 -0.1147 -0.0054545 0.47796 0.090912 0.094489 -0.36882 -0.59396 -0.097729 0.20072 0.17055 -0.0047356 -0.039709 0.32498 -0.023452 0.12302 0.3312\n",
            "to 0.31924 0.06316 -0.27858 0.2612 0.079248 -0.21462 -0.10495 0.15495 -0.03353 2.4834 -0.50904 0.08749 0.21426 0.22151 -0.25234 -0.097544 -0.1927 1.3606 -0.11592 -0.10383 0.21929 0.11997 -0.11063 0.14212 -0.16643 0.21815 0.0042086 -0.070012 -0.23532 -0.26518 0.031248 0.16669 -0.089777 0.20059 0.31614 -0.5583 0.075735 0.27635 0.12741 -0.18185 -0.12722 0.024686 -0.077233 -0.48998 0.020355 0.0039164 0.1215 0.089723 -0.078975 0.081443 -0.099087 -0.055621 0.10737 -0.0044042 0.48496 0.11717 -0.017329 0.109 -0.35558 0.051084 0.15714 0.17961 -0.29711 0.033645 -0.025792 -0.013931 -0.23 -0.040306 0.22282 -0.013544 0.011554 0.3911 0.26533 -0.31012 0.40539 -0.042975 0.020811 -0.33033 0.19573 -0.037958 0.10274 -0.0013581 -0.44505 0.077886 0.08511 -0.20285 -0.19481 0.056933 0.53105 0.034154 -0.56996 -0.18469 0.093403 0.28044 -0.23349 0.10938 -0.014288 -0.274 0.034196 -0.098479 0.13268 0.19437 0.13463 -0.099059 0.040324 -0.66272 0.3571 0.15429 0.18598 0.087542 0.080538 -0.25121 0.24155 0.1783 0.036011 -0.027677 0.21161 -0.29107 -0.0083456 0.11317 0.31064 -0.10693 -0.27367 -0.039785 0.039881 0.034462 -0.16518 0.16115 0.060826 0.3075 -0.22398 0.14619 -0.2661 0.49732 -0.13996 -0.24287 0.039469 -0.084495 -0.24315 0.070701 -1.0136 -0.21733 -0.36878 -0.24973 0.17472 -0.011592 0.068561 -0.090411 0.21878 -0.2639 0.11904 0.14285 -0.18707 -0.13474 -0.13232 -0.26553 0.22947 -0.018215 0.0067383 -0.1019 0.10053 -0.1127 -0.13295 0.15951 0.14906 -0.095578 0.26992 0.011057 0.056568 0.021386 0.20215 0.00048589 0.5336 -0.22947 0.29275 0.17378 0.25423 -0.10976 0.058816 0.014616 -0.04306 0.10732 -0.028149 -0.19181 0.1025 -0.063892 0.012737 -0.12913 0.015037 0.26562 -0.017049 -0.060716 -0.094919 0.017775 0.13221 0.1683 -0.19323 -0.17612 0.075506 0.18939 0.12508 -0.1988 -0.16017 -0.21092 0.46933 0.044747 0.098349 0.011637 0.22281 -0.010837 -0.04833 -0.47335 -0.36811 -0.13592 -0.15086 0.25416 0.069531 0.14211 -0.26703 -0.1259 0.12076 -0.26117 0.033024 -0.034398 -0.13968 0.13446 -0.16709 0.15002 -0.13724 0.091226 -0.27718 0.020098 0.26919 0.43016 0.094019 -0.085496 -0.25192 -0.11645 -0.039734 0.0046738 0.54178 -0.16636 0.34546 0.098501 0.47819 -0.38428 -0.3238 -0.14822 -0.47817 0.16704 -0.064505 0.11834 -0.3448 0.096891 0.32309 0.41471 0.19463 -0.20891 -0.12223 -0.058298 -0.20268 0.2948 0.043397 0.10112 0.27177 -0.52124 -0.073794 0.044808 0.41388 0.088782 0.62255 -0.072391 0.090129 0.15428 0.023163 -0.13028 0.061762 0.33803 -0.091581 0.21039 0.05108 0.19184 0.10444 0.2138 -0.35091 -0.23702 0.038399 -0.10031 0.18359 0.025178 -0.12977 0.3713 0.18888 -0.0042738 -0.10645 -0.2581 -0.044629 0.082745 0.097801 0.25045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIVCkUkE_IjR"
      },
      "source": [
        "#### Exercise: New Vocabulary\n",
        "\n",
        "Since we now use pre-trained word embeddings, we need to create a new vocabulary. \n",
        "This is because of two reasons:\n",
        "\n",
        "1. We do not have pre-trained word embeddings for all words in our SST training set, and we do not want words in our vocabulary for which we have no word embeddings.\n",
        "2. We should be able to look up the pre-trained word embedding for words in the validation and test set, even if these words are unseen in training. \n",
        "\n",
        "Now, create a new vocabulary object `v` based on the word set of pre-trained embeddings, and load the corresponding embeddings into a list `vectors`.\n",
        "\n",
        "The vocabulary `v` should consist of:\n",
        " - a  `<unk>` token at position 0,\n",
        " - a  `<pad>` token at position 1, \n",
        " - and then all words in the pre-trained embedding set.\n",
        " \n",
        "\n",
        "After storing each vector in a list `vectors`, turn the list into a numpy matrix like this:\n",
        "```python\n",
        " vectors = np.stack(vectors, axis=0)\n",
        "```\n",
        "\n",
        "Remember to add new embeddings for the `<unk>` and `<pad>` tokens, as they're not part of the word2vec/GloVe embeddings. These embeddings can be randomly initialized or 0-valued, think about what makes sense and see what the effects are.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITyyCvDnCL4U"
      },
      "source": [
        "def load_embeddings(path, lower=False):\n",
        "  \"\"\"\n",
        "  Reads the word embeddings from a path and stores them as numpy array\n",
        "  of floating point numbers in a dictionary\n",
        "  \"\"\"\n",
        "  word_embeddings = dict()\n",
        "  for line in filereader(path):\n",
        "    line_tokens = line.split()\n",
        "    word = line_tokens[0].lower() if lower else line_tokens[0]\n",
        "    word_embeddings[word] = np.array([float(v) for v in line_tokens[1:]])\n",
        "  return word_embeddings\n",
        "\n",
        "# LOWER\n",
        "\n",
        "def create_vocabulary(embeddings_dict, init_emb_fn='zeros'):\n",
        "  \"\"\"\n",
        "  Create a dictionary based on an embeddings dictionary.\n",
        "  \"\"\"\n",
        "  # Define initialization method for the tokens we\n",
        "  # do not have embeddings of\n",
        "  if init_emb_fn is None or init_emb_fn == 'zeros':\n",
        "    init_emb_fn = lambda x: np.zeros(x)\n",
        "  elif init_emb_fn == 'random':\n",
        "    # We use the torch random method to enforce using the same\n",
        "    # seed set from the torch\n",
        "    init_emb_fn = lambda x: torch.randn(x).cpu().numpy()\n",
        "\n",
        "  v = Vocabulary()\n",
        "  for w in embeddings_dict.keys():\n",
        "    v.count_token(w)\n",
        "  v.build()\n",
        "  # Create an empty list for the Embeddings Vectors\n",
        "  emb_size = len(embeddings_dict[list(embeddings_dict.keys())[0]])\n",
        "  vectors = [init_emb_fn(emb_size) for _ in range(len(v.w2i.keys()))]\n",
        "  for w, emb in embeddings_dict.items():\n",
        "    # Store the Embeddings in the order\n",
        "    # denoted by the vocabulary\n",
        "    vectors[v.w2i[w]] = emb\n",
        "  return v, vectors\n",
        "\n",
        "\n",
        "# embeddings_dict = load_embeddings(path=glove_path, lower=LOWER)\n",
        "embeddings_dict = load_embeddings(path=word2vec_path, lower=LOWER)\n",
        "\n",
        "v, vectors = create_vocabulary(embeddings_dict)\n",
        "vectors = np.stack(vectors, axis=0)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC-7mRyYNG9b"
      },
      "source": [
        "#### Exercise: words not in our pre-trained set\n",
        "\n",
        "How many words in the training, dev, and test set are also in your vector set?\n",
        "How many words are not there?\n",
        "\n",
        "Store the words that are not in the word vector set in the set below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6MA3-wF_X5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f23950d-54c8-4af0-eb68-6293ff3b8572"
      },
      "source": [
        "words_not_found = set()\n",
        "\n",
        "\n",
        "# Add the words from the training set that do not\n",
        "# appear in the vocabulary to the words_not_found\n",
        "for data_set in (train_data,):\n",
        "  for ex in data_set:\n",
        "    for token in ex.tokens:\n",
        "      if token not in v.w2i:\n",
        "        # if the token is not in the vocabulary\n",
        "        # add it to the words_not_found set\n",
        "        words_not_found.add(token)\n",
        "\n",
        "\n",
        "# Add the words from the training set that do not\n",
        "# appear in the vocabulary to the words_not_found\n",
        "for data_set in (dev_data,):\n",
        "  for ex in data_set:\n",
        "    for token in ex.tokens:\n",
        "      if token not in v.w2i:\n",
        "        # if the token is not in the vocabulary\n",
        "        # add it to the words_not_found set\n",
        "        words_not_found.add(token)\n",
        "\n",
        "\n",
        "# Add the words from the test set that do not\n",
        "# appear in the vocabulary to the words_not_found\n",
        "for data_set in (test_data,):\n",
        "  for ex in data_set:\n",
        "    for token in ex.tokens:\n",
        "      if token not in v.w2i:\n",
        "        # if the token is not in the vocabulary\n",
        "        # add it to the words_not_found set\n",
        "        words_not_found.add(token)\n",
        "\n",
        "print('There are {} words from training, validation'.format(len(words_not_found))\n",
        "+' and test data that do not appear in the vocabulary')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2779 words from training, validation and test data that do not appear in the vocabulary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfEd38W0NnAI"
      },
      "source": [
        "#### Exercise: train Deep CBOW with (fixed) pre-trained embeddings\n",
        "\n",
        "Now train Deep CBOW again using the pre-trained word vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_6ooqgEsB20"
      },
      "source": [
        "# We define a new class that inherits from DeepCBOW.\n",
        "class PTDeepCBOW(DeepCBOW):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
        "    super(PTDeepCBOW, self).__init__(\n",
        "        vocab_size, embedding_dim, hidden_dim, output_dim, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfIh4Ni6yuAh"
      },
      "source": [
        "# Create a Deep CBOW model with pre-trained embeddings\n",
        "# YOUR CODE HERE\n",
        "pt_deep_cbow_model = PTDeepCBOW(vocab_size=len(v.w2i), embedding_dim=300, \n",
        "                                hidden_dim=100, output_dim=len(t2i), vocab=v)\n",
        "\n",
        "# copy pre-trained word vectors into embeddings table\n",
        "pt_deep_cbow_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "\n",
        "# disable training the pre-trained embeddings\n",
        "pt_deep_cbow_model.embed.weight.requires_grad = False\n",
        "\n",
        "# move model to specified device\n",
        "pt_deep_cbow_model = pt_deep_cbow_model.to(device)\n",
        "\n",
        "# train the model\n",
        "eval_every=1000\n",
        "optimizer = optim.Adam(pt_deep_cbow_model.parameters(), lr=0.0005)\n",
        "pt_deep_cbow_losses, pt_deep_cbow_accuracies = train_model(\n",
        "    pt_deep_cbow_model, optimizer, num_iterations=30000, \n",
        "    print_every=1000, eval_every=eval_every)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufujv3x31ufD"
      },
      "source": [
        "# This will plot the validation accuracies across time.\n",
        "plt.title('Validation Accuracy as a function of epochs for the Deep CBOW'\n",
        "+' model with pre-trained embeddings')\n",
        "plt.xlabel('Steps of {} epochs each'.format(eval_every))\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.plot(pt_deep_cbow_accuracies)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTJtKBzd7Qjr"
      },
      "source": [
        "# This will plot the training loss over time.\n",
        "plt.title('Train Loss as a function of epochs for the Deep CBOW model ' \n",
        "          + 'with pre-trained embeddings')\n",
        "plt.xlabel('Steps of {} epochs each'.format(eval_every))\n",
        "plt.ylabel('Train Loss')\n",
        "plt.plot(pt_deep_cbow_losses)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFu8xzCy9XDW"
      },
      "source": [
        "**It looks like we've hit what is possible with just using words.**\n",
        "Let's move on by incorporating word order!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g41yW4PL9jG0"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODzXEH0MaGpa"
      },
      "source": [
        "It is time to get more serious. Even with pre-trained word embeddings and multiple layers, we still seem to do pretty badly at sentiment classification. \n",
        "The next step we can take is to introduce word order again, dropping our independence assumptions. In this way, we can get a representation of the sentence as an ordered set of tokens.\n",
        "\n",
        "We will get this representation using a **Long Short-Term Memory** (LSTM). As an exercise, we will code our own LSTM cell, so that we get comfortable with its inner workings.\n",
        "Once we have an LSTM cell, we can call it repeatedly, updating its hidden state one word at a time:\n",
        "\n",
        "```python\n",
        "rnn = MyLSTMCell(input_size, hidden_size)\n",
        "\n",
        "hx = torch.zeros(1, hidden_size)  # initial hidden state\n",
        "cx = torch.zeros(1, hidden_size)  # initial memory cell\n",
        "output = []                       # to save intermediate LSTM states\n",
        "\n",
        "# feed one word at a time\n",
        "for i in range(n_timesteps):\n",
        "  hx, cx = rnn(input[i], (hx, cx))\n",
        "  output.append(hx)\n",
        "```\n",
        "\n",
        "If you need some more help understanding LSTMs, you can check out these resources:\n",
        "- Blog post (highly recommended): http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "- Paper covering LSTM formulas in detail: https://arxiv.org/abs/1503.04069 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9f4b45BXKFC"
      },
      "source": [
        "#### Exercise: Finish the LSTM cell below. \n",
        "You will need to implement the LSTM formulas:\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "        i = \\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\n",
        "        f = \\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\n",
        "        g = \\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\\n",
        "        o = \\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\n",
        "        c' = f * c + i * g \\\\\n",
        "        h' = o \\tanh(c') \\\\\n",
        "\\end{array}\n",
        " $$\n",
        "\n",
        "where $\\sigma$ is the sigmoid function.\n",
        "\n",
        "*Note that the LSTM formulas can differ slightly between different papers. We use the PyTorch LSTM formulation here.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ9m5kLMd7-v"
      },
      "source": [
        "class MyLSTMCell(nn.Module):\n",
        "  \"\"\"Our own LSTM cell\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, bias=True):\n",
        "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
        "    super(MyLSTMCell, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bias = bias\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    self.W_ii = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "    self.b_ii = nn.Parameter(torch.Tensor(hidden_size))\n",
        "    self.W_hi = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.b_hi = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "    self.W_if = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "    self.b_if = nn.Parameter(torch.Tensor(hidden_size))\n",
        "    self.W_hf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.b_hf = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "    self.W_ig = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "    self.b_ig = nn.Parameter(torch.Tensor(hidden_size))\n",
        "    self.W_hg = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.b_hg = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "    self.W_io = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "    self.b_io = nn.Parameter(torch.Tensor(hidden_size))\n",
        "    self.W_ho = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.b_ho = nn.Parameter(torch.Tensor(hidden_size))\n",
        "    \n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    \"\"\"This is PyTorch's default initialization method\"\"\"\n",
        "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "    for weight in self.parameters():\n",
        "      weight.data.uniform_(-stdv, stdv)  \n",
        "\n",
        "  def forward(self, input_, hx, mask=None):\n",
        "    \"\"\"\n",
        "    input is (batch, input_size)\n",
        "    hx is ((batch, hidden_size), (batch, hidden_size))\n",
        "    \"\"\"\n",
        "    prev_h, prev_c = hx\n",
        "\n",
        "    # project input and prev state\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "\n",
        "    # main LSTM computation    \n",
        "\n",
        "    i = torch.sigmoid(input_@self.W_ii + self.b_ii + prev_h @ self.W_hi + self.b_hi)\n",
        "    f = torch.sigmoid(input_@self.W_if + self.b_if + prev_h @ self.W_hf + self.b_hf)\n",
        "    g = torch.tanh(input_@self.W_ig + self.b_ig + prev_h @ self.W_hg + self.b_hg)\n",
        "    o = torch.sigmoid(input_@self.W_io + self.b_io + prev_h @ self.W_ho + self.b_ho)\n",
        "    c = f*prev_c + i*g\n",
        "    h = o * torch.tanh(c)\n",
        "    return h, c\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return \"{}({:d}, {:d})\".format(\n",
        "        self.__class__.__name__, self.input_size, self.hidden_size)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FastMyLSTMCell(nn.Module):\n",
        "  \"\"\"Our own LSTM Fast cell\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, bias=True):\n",
        "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
        "    super(FastMyLSTMCell, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bias = bias\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    self.W_i = nn.Parameter(torch.Tensor(input_size, hidden_size*4))\n",
        "    self.b_i = nn.Parameter(torch.Tensor(hidden_size*4))\n",
        "    self.W_h = nn.Parameter(torch.Tensor(hidden_size, hidden_size*4))\n",
        "    self.b_h = nn.Parameter(torch.Tensor(hidden_size*4))\n",
        "    \n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    \"\"\"This is PyTorch's default initialization method\"\"\"\n",
        "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "    for weight in self.parameters():\n",
        "      weight.data.uniform_(-stdv, stdv)  \n",
        "\n",
        "  def forward(self, input_, hx, mask=None):\n",
        "    \"\"\"\n",
        "    input is (batch, input_size)\n",
        "    hx is ((batch, hidden_size), (batch, hidden_size))\n",
        "    \"\"\"\n",
        "    prev_h, prev_c = hx\n",
        "\n",
        "    # project input and prev state\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "\n",
        "    # main LSTM computation    \n",
        "\n",
        "    gates = input_@self.W_i + self.b_i + prev_h @ self.W_h + self.b_h\n",
        "    pre_i, pre_f, pre_g, pre_o = torch.chunk(gates, 4, dim=-1)\n",
        "    i, f, g, o = torch.sigmoid(pre_i), torch.sigmoid(pre_f), torch.tanh(pre_g),torch.sigmoid(pre_o),\n",
        "    c = f*prev_c + i*g\n",
        "    h = o * torch.tanh(c)\n",
        "    return h, c\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return \"{}({:d}, {:d})\".format(\n",
        "        self.__class__.__name__, self.input_size, self.hidden_size)"
      ],
      "metadata": {
        "id": "NtQKwAcrGVZ3"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JM7xPhkQeE5"
      },
      "source": [
        "#### Optional: Efficient Matrix Multiplication\n",
        "\n",
        "It is more efficient to do a few big matrix multiplications than to do many smaller ones. So we will implement the above cell using just **two** linear layers.\n",
        "\n",
        "This is possible because the eight linear transformations contained in one forward pass through an LSTM cell can be reduced to just two:\n",
        "$$W_h h + b_h$$\n",
        "$$W_i x + b_i $$ \n",
        "\n",
        "with $h = $ `prev_h` and $x = $ `input_`.\n",
        "\n",
        "and where: \n",
        "\n",
        "$W_h =  \\begin{pmatrix}\n",
        "W_{hi}\\\\ \n",
        "W_{hf}\\\\ \n",
        "W_{hg}\\\\ \n",
        "W_{ho}\n",
        "\\end{pmatrix}$, $b_h = \\begin{pmatrix}\n",
        "b_{hi}\\\\ \n",
        "b_{hf}\\\\ \n",
        "b_{hg}\\\\ \n",
        "b_{ho}\n",
        "\\end{pmatrix}$,  $W_i = \\begin{pmatrix}\n",
        "W_{ii}\\\\ \n",
        "W_{if}\\\\ \n",
        "W_{ig}\\\\ \n",
        "W_{io}\n",
        "\\end{pmatrix}$ and $b_i = \\begin{pmatrix}\n",
        "b_{ii}\\\\ \n",
        "b_{if}\\\\ \n",
        "b_{ig}\\\\ \n",
        "b_{io}\n",
        "\\end{pmatrix}$.\n",
        "\n",
        "Convince yourself that, after chunking with [torch.chunk](https://pytorch.org/docs/stable/torch.html?highlight=chunk#torch.chunk), the output of those two linear transformations is equivalent to the output of the eight linear transformations in the LSTM cell calculations above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9gA-UcqSBe0"
      },
      "source": [
        "#### LSTM Classifier\n",
        "\n",
        "Having an LSTM cell is not enough: we still need some code that calls it repeatedly, and then makes a prediction from the final hidden state. \n",
        "You will find that code below. Make sure that you understand it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_fast_lstm = True"
      ],
      "metadata": {
        "id": "RBb2jTCoHmni"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iuYZm5poEn5"
      },
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "  \"\"\"Encodes sentence with an LSTM and projects final hidden state\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
        "    super(LSTMClassifier, self).__init__()\n",
        "    self.vocab = vocab\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
        "    if is_fast_lstm:\n",
        "        print(\"Using Fast LSTM\")\n",
        "        self.rnn = FastMyLSTMCell(embedding_dim, hidden_dim)\n",
        "    else:\n",
        "       print(\"Using Slow LSTM\")\n",
        "       self.rnn = MyLSTMCell(embedding_dim, hidden_dim)\n",
        "    \n",
        "    \n",
        "    self.output_layer = nn.Sequential(     \n",
        "        nn.Dropout(p=0.5),  # explained later\n",
        "        nn.Linear(hidden_dim, output_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    B = x.size(0)  # batch size (this is 1 for now, i.e. 1 single example)\n",
        "    T = x.size(1)  # timesteps (the number of words in the sentence)\n",
        "    \n",
        "    input_ = self.embed(x)\n",
        "\n",
        "    # here we create initial hidden states containing zeros\n",
        "    # we use a trick here so that, if input is on the GPU, then so are hx and cx\n",
        "    hx = input_.new_zeros(B, self.rnn.hidden_size)\n",
        "    cx = input_.new_zeros(B, self.rnn.hidden_size)\n",
        "    \n",
        "    # process input sentences one word/timestep at a time\n",
        "    # input is batch-major (i.e., batch size is the first dimension)\n",
        "    # so the first word(s) is (are) input_[:, 0]\n",
        "    outputs = []   \n",
        "    for i in range(T):\n",
        "      hx, cx = self.rnn(input_[:, i], (hx, cx))\n",
        "      outputs.append(hx)\n",
        "    \n",
        "    # if we have a single example, our final LSTM state is the last hx\n",
        "    if B == 1:\n",
        "      final = hx\n",
        "    else:\n",
        "      #\n",
        "      # This part is explained in next section, ignore this else-block for now.\n",
        "      #\n",
        "      # We processed sentences with different lengths, so some of the sentences\n",
        "      # had already finished and we have been adding padding inputs to hx.\n",
        "      # We select the final state based on the length of each sentence.\n",
        "      \n",
        "      # two lines below not needed if using LSTM from pytorch\n",
        "      outputs = torch.stack(outputs, dim=0)           # [T, B, D]\n",
        "      outputs = outputs.transpose(0, 1).contiguous()  # [B, T, D]\n",
        "      \n",
        "      # to be super-sure we're not accidentally indexing the wrong state\n",
        "      # we zero out positions that are invalid\n",
        "      pad_positions = (x == 1).unsqueeze(-1)\n",
        "      \n",
        "      outputs = outputs.contiguous()      \n",
        "      outputs = outputs.masked_fill_(pad_positions, 0.)\n",
        "        \n",
        "      mask = (x != 1)  # true for valid positions [B, T]\n",
        "      lengths = mask.sum(dim=1)                 # [B, 1]\n",
        "\n",
        "      indexes = (lengths - 1) + torch.arange(B, device=x.device, dtype=x.dtype) * T\n",
        "      final = outputs.view(-1, self.hidden_dim)[indexes]  # [B, D]\n",
        "    \n",
        "    # we use the last hidden state to classify the sentence\n",
        "    logits = self.output_layer(final)\n",
        "    return logits"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxFoVpvMPB6g"
      },
      "source": [
        "#### Dropout\n",
        "\n",
        "Data sparsity and a small data set can cause *overfitting*. This is a phenomenom that is very likely to occur when training strong and expressive models, like LSTMs, on small data. In practice, if your model overfits, this means that it will be very good at predicting (or \"remembering\") the sentiment of the training set, but unable to generalise to new, unseen data in the test set. This is undesirable and one technique to mitigate this issue is *dropout*. \n",
        "\n",
        "A dropout layer is defined by the following formula, which can be applied, for example, to a linear layer:\n",
        "\n",
        "$$\\text{tanh}(W(\\mathbf{h}\\odot \\mathbf{d}) + \\mathbf{b})$$\n",
        "\n",
        "where $\\mathbf{d} \\in \\{0, 1\\}^n$, with $d_j \\sim \\text{Bernoulli}(p)$, \n",
        "\n",
        "These formula simply means that we *drop* certain parameters during training (by setting them to zero). Which parameters we drop is stochastically determined by a Bernoulli distribution and the probability of each parameter being dropped is set to $p = 0.5$ in our experiments (see the previous cell of code where we define our output layer). A dropout layer can be applied at many different places in our models. This technique helps against the undesirable effect that a model relies on single parameters for prediction (e.g. if $h^{\\prime}_j$ is large, always predict positive). If we use dropout, the model needs to learn to rely on different parameters, which is desirable to obtain better generalisation to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQjEjLt9z0XW"
      },
      "source": [
        "**Let's train our LSTM!** Note that is will be a lot slower than previous models because we need to do many more computations per sentence.\n",
        "\n",
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgZoSPD4fsf_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "5129546d-e129-425d-be6e-bcf0f05f6639"
      },
      "source": [
        "lstm_model = LSTMClassifier(len(v.w2i), 300, 168, len(t2i), v)\n",
        "\n",
        "# copy pre-trained word vectors into embeddings table\n",
        "with torch.no_grad():\n",
        "  lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "  lstm_model.embed.weight.requires_grad = False\n",
        "\n",
        "print(lstm_model)\n",
        "print_parameters(lstm_model)\n",
        "\n",
        "lstm_model = lstm_model.to(device)\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
        "\n",
        "start_time = time.time()\n",
        "lstm_losses, lstm_accuracies = train_model(\n",
        "    lstm_model, optimizer, num_iterations=25000, \n",
        "    print_every=250, eval_every=1000)\n",
        "print(\"Total time taken: \", time.time()- start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMClassifier(\n",
            "  (embed): Embedding(18922, 300, padding_idx=1)\n",
            "  (rnn): FastMyLSTMCell(300, 168)\n",
            "  (output_layer): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "embed.weight             [18922, 300] requires_grad=False\n",
            "rnn.W_i                  [300, 672]   requires_grad=True\n",
            "rnn.b_i                  [672]        requires_grad=True\n",
            "rnn.W_h                  [168, 672]   requires_grad=True\n",
            "rnn.b_h                  [672]        requires_grad=True\n",
            "output_layer.1.weight    [5, 168]     requires_grad=True\n",
            "output_layer.1.bias      [5]          requires_grad=True\n",
            "\n",
            "Total number of parameters: 5993285\n",
            "\n",
            "Shuffling training data\n",
            "Iter 250: loss=398.3547, time=2.79s\n",
            "Iter 500: loss=390.0962, time=5.89s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-b4e216055b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m lstm_losses, lstm_accuracies = train_model(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mlstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     print_every=250, eval_every=1000)\n",
            "\u001b[0;32m<ipython-input-32-ca64733ad0ef>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, num_iterations, print_every, eval_every, batch_fn, prep_fn, eval_fn, batch_size, eval_batch_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0;31m# compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;31m# i.e., Perform backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;31m# update weights - take a small step in the opposite dir of the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BKVnyg0Hq5E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "f188edc7-6171-4dcc-dcec-2e7101eff90e"
      },
      "source": [
        "# plot validation accuracy\n",
        "plot_graph(lstm_accuracies, \"Fig 4.2.a LSTM Model No. of Iterations vs Val Accuracies (Fast-Matrix F/W Pass)\", \"Accuracy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"331.389812pt\" version=\"1.1\" viewBox=\"0 0 484.9875 331.389812\" width=\"484.9875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 331.389812 \nL 484.9875 331.389812 \nL 484.9875 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 63.93375 288.430125 \nL 421.05375 288.430125 \nL 421.05375 22.318125 \nL 63.93375 22.318125 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p9d6299a565)\" d=\"M 66.639205 288.430125 \nL 66.639205 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(63.13983 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#p9d6299a565)\" d=\"M 134.275568 288.430125 \nL 134.275568 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(130.776193 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p9d6299a565)\" d=\"M 201.911932 288.430125 \nL 201.911932 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(194.913182 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#p9d6299a565)\" d=\"M 269.548295 288.430125 \nL 269.548295 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g style=\"fill:#262626;\" transform=\"translate(262.549545 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p9d6299a565)\" d=\"M 337.184659 288.430125 \nL 337.184659 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(330.185909 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#p9d6299a565)\" d=\"M 404.821023 288.430125 \nL 404.821023 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g style=\"fill:#262626;\" transform=\"translate(397.822273 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- # of Iterations -->\n     <defs>\n      <path d=\"M 51.125 44 \nL 36.921875 44 \nL 32.8125 27.6875 \nL 47.125 27.6875 \nz\nM 43.796875 71.78125 \nL 38.71875 51.515625 \nL 52.984375 51.515625 \nL 58.109375 71.78125 \nL 65.921875 71.78125 \nL 60.890625 51.515625 \nL 76.125 51.515625 \nL 76.125 44 \nL 58.984375 44 \nL 54.984375 27.6875 \nL 70.515625 27.6875 \nL 70.515625 20.21875 \nL 53.078125 20.21875 \nL 48 0 \nL 40.1875 0 \nL 45.21875 20.21875 \nL 30.90625 20.21875 \nL 25.875 0 \nL 18.015625 0 \nL 23.09375 20.21875 \nL 7.71875 20.21875 \nL 7.71875 27.6875 \nL 24.90625 27.6875 \nL 29 44 \nL 13.28125 44 \nL 13.28125 51.515625 \nL 30.90625 51.515625 \nL 35.890625 71.78125 \nz\n\" id=\"DejaVuSans-35\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-73\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(199.292813 321.694187)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"83.789062\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"115.576172\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"176.757812\" xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"211.962891\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"243.75\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"273.242188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"312.451172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"373.974609\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"415.087891\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"476.367188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"515.576172\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"543.359375\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"604.541016\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"667.919922\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p9d6299a565)\" d=\"M 63.93375 274.521832 \nL 421.05375 274.521832 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.30 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(29.941562 278.700973)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#p9d6299a565)\" d=\"M 63.93375 241.590584 \nL 421.05375 241.590584 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.32 -->\n      <g style=\"fill:#262626;\" transform=\"translate(29.941562 245.769725)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p9d6299a565)\" d=\"M 63.93375 208.659337 \nL 421.05375 208.659337 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.34 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(29.941562 212.838477)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#p9d6299a565)\" d=\"M 63.93375 175.728089 \nL 421.05375 175.728089 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.36 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(29.941562 179.907229)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p9d6299a565)\" d=\"M 63.93375 142.796841 \nL 421.05375 142.796841 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.38 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(29.941562 146.975982)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <path clip-path=\"url(#p9d6299a565)\" d=\"M 63.93375 109.865593 \nL 421.05375 109.865593 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.40 -->\n      <g style=\"fill:#262626;\" transform=\"translate(29.941562 114.044734)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p9d6299a565)\" d=\"M 63.93375 76.934345 \nL 421.05375 76.934345 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.42 -->\n      <g style=\"fill:#262626;\" transform=\"translate(29.941562 81.113486)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <path clip-path=\"url(#p9d6299a565)\" d=\"M 63.93375 44.003097 \nL 421.05375 44.003097 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.44 -->\n      <g style=\"fill:#262626;\" transform=\"translate(29.941562 48.182238)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- Accuracy -->\n     <defs>\n      <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(23.445937 182.767875)rotate(-90)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"66.658203\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"121.638672\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"176.619141\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"239.998047\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"281.111328\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"342.390625\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"397.371094\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p9d6299a565)\" d=\"M 80.166477 288.430125 \nL 93.69375 183.744051 \nL 107.221023 209.167812 \nL 120.748295 101.490707 \nL 134.275568 71.5804 \nL 147.802841 146.356167 \nL 161.330114 111.959315 \nL 174.857386 98.499677 \nL 188.384659 71.5804 \nL 201.911932 83.544523 \nL 215.439205 97.004161 \nL 228.966477 119.436891 \nL 242.49375 56.625247 \nL 256.021023 86.535554 \nL 269.548295 94.013131 \nL 283.075568 89.526584 \nL 296.602841 65.598339 \nL 310.130114 47.652155 \nL 323.657386 47.652155 \nL 337.184659 46.15664 \nL 350.711932 29.705971 \nL 364.239205 47.652155 \nL 377.766477 65.598339 \nL 391.29375 35.688032 \nL 404.821023 85.040038 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 63.93375 288.430125 \nL 63.93375 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 421.05375 288.430125 \nL 421.05375 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 63.93375 288.430125 \nL 421.05375 288.430125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 63.93375 22.318125 \nL 421.05375 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- Fig 4.2.a LSTM Model No. of Iterations vs Val Accuracies (Fast-Matrix F/W Pass) -->\n    <defs>\n     <path d=\"M 9.8125 72.90625 \nL 51.703125 72.90625 \nL 51.703125 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.109375 \nL 48.578125 43.109375 \nL 48.578125 34.8125 \nL 19.671875 34.8125 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-70\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n     <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n     <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     <path d=\"M 28.609375 0 \nL 0.78125 72.90625 \nL 11.078125 72.90625 \nL 34.1875 11.53125 \nL 57.328125 72.90625 \nL 67.578125 72.90625 \nL 39.796875 0 \nz\n\" id=\"DejaVuSans-86\"/>\n     <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n     <path d=\"M 4.890625 31.390625 \nL 31.203125 31.390625 \nL 31.203125 23.390625 \nL 4.890625 23.390625 \nz\n\" id=\"DejaVuSans-45\"/>\n     <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n     <path d=\"M 25.390625 72.90625 \nL 33.6875 72.90625 \nL 8.296875 -9.28125 \nL 0 -9.28125 \nz\n\" id=\"DejaVuSans-47\"/>\n     <path d=\"M 3.328125 72.90625 \nL 13.28125 72.90625 \nL 28.609375 11.28125 \nL 43.890625 72.90625 \nL 54.984375 72.90625 \nL 70.3125 11.28125 \nL 85.59375 72.90625 \nL 95.609375 72.90625 \nL 77.296875 0 \nL 64.890625 0 \nL 49.515625 63.28125 \nL 33.984375 0 \nL 21.578125 0 \nz\n\" id=\"DejaVuSans-87\"/>\n     <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n     <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(7.2 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"50.269531\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"78.052734\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"141.529297\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"173.316406\" xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"236.939453\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"268.726562\" xlink:href=\"#DejaVuSans-50\"/>\n     <use x=\"332.349609\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"364.136719\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"425.416016\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"457.203125\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"512.916016\" xlink:href=\"#DejaVuSans-83\"/>\n     <use x=\"576.392578\" xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"637.476562\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"723.755859\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"755.542969\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"841.822266\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"903.003906\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"966.480469\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1028.003906\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"1055.787109\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1087.574219\" xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"1162.378906\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1221.810547\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"1253.597656\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1285.384766\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1346.566406\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"1381.771484\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1413.558594\" xlink:href=\"#DejaVuSans-73\"/>\n     <use x=\"1443.050781\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1482.259766\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1543.783203\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1584.896484\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1646.175781\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1685.384766\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1713.167969\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1774.349609\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1837.728516\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1889.828125\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1921.615234\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"1980.794922\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2032.894531\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2064.681641\" xlink:href=\"#DejaVuSans-86\"/>\n     <use x=\"2125.339844\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2186.619141\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"2214.402344\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2246.189453\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"2312.847656\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"2367.828125\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"2422.808594\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"2486.1875\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"2527.300781\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2588.580078\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"2643.560547\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"2671.34375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"2732.867188\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2784.966797\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2816.753906\" xlink:href=\"#DejaVuSans-40\"/>\n     <use x=\"2855.767578\" xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"2904.162109\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2965.441406\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"3017.541016\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"3056.75\" xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"3092.833984\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"3179.113281\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"3240.392578\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"3279.601562\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"3320.714844\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"3348.498047\" xlink:href=\"#DejaVuSans-120\"/>\n     <use x=\"3407.677734\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"3439.464844\" xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"3496.984375\" xlink:href=\"#DejaVuSans-47\"/>\n     <use x=\"3530.675781\" xlink:href=\"#DejaVuSans-87\"/>\n     <use x=\"3629.552734\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"3661.339844\" xlink:href=\"#DejaVuSans-80\"/>\n     <use x=\"3717.142578\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"3778.421875\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"3830.521484\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"3882.621094\" xlink:href=\"#DejaVuSans-41\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 408.95375 36.618125 \nL 413.35375 36.618125 \nQ 415.55375 36.618125 415.55375 34.418125 \nL 415.55375 30.018125 \nQ 415.55375 27.818125 413.35375 27.818125 \nL 408.95375 27.818125 \nQ 406.75375 27.818125 406.75375 30.018125 \nL 406.75375 34.418125 \nQ 406.75375 36.618125 408.95375 36.618125 \nz\n\" style=\"fill:#eaeaf2;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p9d6299a565\">\n   <rect height=\"266.112\" width=\"357.12\" x=\"63.93375\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "application/pdf": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZowTV0EBTb3z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "a6dde7cc-b497-4ed9-963f-e50abf5e8bfa"
      },
      "source": [
        "# plot training loss\n",
        "plot_graph(lstm_losses, \"Fig 4.2.b LSTM Model No. of Iterations vs Loss (Fast-Matrix F/W Pass)\", \"Loss\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"331.389812pt\" version=\"1.1\" viewBox=\"0 0 444.439687 331.389812\" width=\"444.439687pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 331.389812 \nL 444.439687 331.389812 \nL 444.439687 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 53.31 288.430125 \nL 410.43 288.430125 \nL 410.43 22.318125 \nL 53.31 22.318125 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p6d531df8ee)\" d=\"M 66.263388 288.430125 \nL 66.263388 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(62.764013 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#p6d531df8ee)\" d=\"M 131.850165 288.430125 \nL 131.850165 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(124.851415 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p6d531df8ee)\" d=\"M 197.436942 288.430125 \nL 197.436942 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(190.438192 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#p6d531df8ee)\" d=\"M 263.023719 288.430125 \nL 263.023719 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(256.024969 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p6d531df8ee)\" d=\"M 328.610496 288.430125 \nL 328.610496 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(321.611746 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#p6d531df8ee)\" d=\"M 394.197273 288.430125 \nL 394.197273 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(383.699148 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- # of Iterations -->\n     <defs>\n      <path d=\"M 51.125 44 \nL 36.921875 44 \nL 32.8125 27.6875 \nL 47.125 27.6875 \nz\nM 43.796875 71.78125 \nL 38.71875 51.515625 \nL 52.984375 51.515625 \nL 58.109375 71.78125 \nL 65.921875 71.78125 \nL 60.890625 51.515625 \nL 76.125 51.515625 \nL 76.125 44 \nL 58.984375 44 \nL 54.984375 27.6875 \nL 70.515625 27.6875 \nL 70.515625 20.21875 \nL 53.078125 20.21875 \nL 48 0 \nL 40.1875 0 \nL 45.21875 20.21875 \nL 30.90625 20.21875 \nL 25.875 0 \nL 18.015625 0 \nL 23.09375 20.21875 \nL 7.71875 20.21875 \nL 7.71875 27.6875 \nL 24.90625 27.6875 \nL 29 44 \nL 13.28125 44 \nL 13.28125 51.515625 \nL 30.90625 51.515625 \nL 35.890625 71.78125 \nz\n\" id=\"DejaVuSans-35\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-73\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(188.669062 321.694187)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"83.789062\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"115.576172\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"176.757812\" xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"211.962891\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"243.75\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"273.242188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"312.451172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"373.974609\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"415.087891\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"476.367188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"515.576172\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"543.359375\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"604.541016\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"667.919922\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p6d531df8ee)\" d=\"M 53.31 252.879389 \nL 410.43 252.879389 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 300 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 257.058529)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#p6d531df8ee)\" d=\"M 53.31 207.69049 \nL 410.43 207.69049 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 320 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 211.869631)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p6d531df8ee)\" d=\"M 53.31 162.501591 \nL 410.43 162.501591 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 340 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 166.680732)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#p6d531df8ee)\" d=\"M 53.31 117.312692 \nL 410.43 117.312692 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 360 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 121.491833)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p6d531df8ee)\" d=\"M 53.31 72.123794 \nL 410.43 72.123794 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 380 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 76.302934)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <path clip-path=\"url(#p6d531df8ee)\" d=\"M 53.31 26.934895 \nL 410.43 26.934895 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 400 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 31.114036)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- Loss -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(16.318125 168.53475)rotate(-90)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p6d531df8ee)\" d=\"M 69.542727 43.739515 \nL 72.822066 31.312132 \nL 76.101405 40.198063 \nL 79.380744 45.676721 \nL 82.660083 54.159542 \nL 85.939421 52.272077 \nL 89.21876 58.171426 \nL 92.498099 90.376686 \nL 95.777438 108.371119 \nL 99.056777 107.910064 \nL 102.336116 155.908677 \nL 105.615455 124.904516 \nL 108.894793 122.244774 \nL 112.174132 172.104904 \nL 115.453471 156.117265 \nL 118.73281 146.109824 \nL 122.012149 124.58422 \nL 125.291488 156.865821 \nL 128.570826 162.807889 \nL 131.850165 150.285933 \nL 135.129504 166.048547 \nL 138.408843 160.688991 \nL 141.688182 152.699682 \nL 144.967521 150.544578 \nL 148.24686 163.604402 \nL 151.526198 176.918013 \nL 154.805537 171.522822 \nL 158.084876 153.279729 \nL 161.364215 182.498362 \nL 164.643554 182.86177 \nL 167.922893 164.369015 \nL 171.202231 174.228127 \nL 174.48157 201.588367 \nL 177.760909 191.144743 \nL 181.040248 222.404934 \nL 184.319587 165.212623 \nL 187.598926 183.550797 \nL 190.878264 212.479041 \nL 194.157603 201.270609 \nL 197.436942 186.839496 \nL 200.716281 168.399981 \nL 203.99562 189.246426 \nL 207.274959 143.098953 \nL 210.554298 223.744019 \nL 213.833636 184.353224 \nL 217.112975 228.314624 \nL 220.392314 200.52381 \nL 223.671653 166.309535 \nL 226.950992 201.083131 \nL 230.230331 199.823118 \nL 233.509669 198.643699 \nL 236.789008 203.349208 \nL 240.068347 205.255751 \nL 243.347686 239.564561 \nL 246.627025 238.897456 \nL 249.906364 191.175522 \nL 253.185702 241.293142 \nL 256.465041 187.030116 \nL 259.74438 199.774221 \nL 263.023719 247.90387 \nL 266.303058 223.84992 \nL 269.582397 205.069316 \nL 272.861736 201.206771 \nL 276.141074 224.625456 \nL 279.420413 190.178469 \nL 282.699752 207.480619 \nL 285.979091 206.714216 \nL 289.25843 203.726838 \nL 292.537769 256.467312 \nL 295.817107 175.66101 \nL 299.096446 176.107049 \nL 302.375785 216.288412 \nL 305.655124 220.017842 \nL 308.934463 270.676419 \nL 312.213802 230.477264 \nL 315.49314 252.51771 \nL 318.772479 251.742077 \nL 322.051818 233.272665 \nL 325.331157 203.830488 \nL 328.610496 233.837481 \nL 331.889835 207.019068 \nL 335.169174 216.510007 \nL 338.448512 246.110251 \nL 341.727851 248.363154 \nL 345.00719 206.810583 \nL 348.286529 255.206523 \nL 351.565868 235.596791 \nL 354.845207 239.578002 \nL 358.124545 286.96487 \nL 361.403884 250.864845 \nL 364.683223 219.227682 \nL 367.962562 230.236929 \nL 371.241901 264.083697 \nL 374.52124 238.159926 \nL 377.800579 221.51154 \nL 381.079917 236.790335 \nL 384.359256 196.679115 \nL 387.638595 288.430125 \nL 390.917934 210.188823 \nL 394.197273 268.815268 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 53.31 288.430125 \nL 53.31 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 410.43 288.430125 \nL 410.43 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 53.31 288.430125 \nL 410.43 288.430125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 53.31 22.318125 \nL 410.43 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"text_15\">\n    <!-- Fig 4.2.b LSTM Model No. of Iterations vs Loss (Fast-Matrix F/W Pass) -->\n    <defs>\n     <path d=\"M 9.8125 72.90625 \nL 51.703125 72.90625 \nL 51.703125 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.109375 \nL 48.578125 43.109375 \nL 48.578125 34.8125 \nL 19.671875 34.8125 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-70\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n     <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n     <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n     <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n     <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n     <path d=\"M 4.890625 31.390625 \nL 31.203125 31.390625 \nL 31.203125 23.390625 \nL 4.890625 23.390625 \nz\n\" id=\"DejaVuSans-45\"/>\n     <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n     <path d=\"M 25.390625 72.90625 \nL 33.6875 72.90625 \nL 8.296875 -9.28125 \nL 0 -9.28125 \nz\n\" id=\"DejaVuSans-47\"/>\n     <path d=\"M 3.328125 72.90625 \nL 13.28125 72.90625 \nL 28.609375 11.28125 \nL 43.890625 72.90625 \nL 54.984375 72.90625 \nL 70.3125 11.28125 \nL 85.59375 72.90625 \nL 95.609375 72.90625 \nL 77.296875 0 \nL 64.890625 0 \nL 49.515625 63.28125 \nL 33.984375 0 \nL 21.578125 0 \nz\n\" id=\"DejaVuSans-87\"/>\n     <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n     <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(26.500313 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"50.269531\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"78.052734\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"141.529297\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"173.316406\" xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"236.939453\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"268.726562\" xlink:href=\"#DejaVuSans-50\"/>\n     <use x=\"332.349609\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"364.136719\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"427.613281\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"459.400391\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"515.113281\" xlink:href=\"#DejaVuSans-83\"/>\n     <use x=\"578.589844\" xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"639.673828\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"725.953125\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"757.740234\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"844.019531\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"905.201172\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"968.677734\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1030.201172\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"1057.984375\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1089.771484\" xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"1164.576172\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1224.007812\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"1255.794922\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1287.582031\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1348.763672\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"1383.96875\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1415.755859\" xlink:href=\"#DejaVuSans-73\"/>\n     <use x=\"1445.248047\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1484.457031\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1545.980469\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1587.09375\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1648.373047\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1687.582031\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1715.365234\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1776.546875\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1839.925781\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1892.025391\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1923.8125\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"1982.992188\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2035.091797\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2066.878906\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"2120.841797\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"2182.023438\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2234.123047\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2286.222656\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2318.009766\" xlink:href=\"#DejaVuSans-40\"/>\n     <use x=\"2357.023438\" xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"2405.417969\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2466.697266\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2518.796875\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"2558.005859\" xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"2594.089844\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"2680.369141\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2741.648438\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"2780.857422\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"2821.970703\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"2849.753906\" xlink:href=\"#DejaVuSans-120\"/>\n     <use x=\"2908.933594\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2940.720703\" xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"2998.240234\" xlink:href=\"#DejaVuSans-47\"/>\n     <use x=\"3031.931641\" xlink:href=\"#DejaVuSans-87\"/>\n     <use x=\"3130.808594\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"3162.595703\" xlink:href=\"#DejaVuSans-80\"/>\n     <use x=\"3218.398438\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"3279.677734\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"3331.777344\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"3383.876953\" xlink:href=\"#DejaVuSans-41\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 398.33 36.618125 \nL 402.73 36.618125 \nQ 404.93 36.618125 404.93 34.418125 \nL 404.93 30.018125 \nQ 404.93 27.818125 402.73 27.818125 \nL 398.33 27.818125 \nQ 396.13 27.818125 396.13 30.018125 \nL 396.13 34.418125 \nQ 396.13 36.618125 398.33 36.618125 \nz\n\" style=\"fill:#eaeaf2;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6d531df8ee\">\n   <rect height=\"266.112\" width=\"357.12\" x=\"53.31\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "application/pdf": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEw6XHQY_AAQ"
      },
      "source": [
        "# Mini-batching\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPf96wGzBTQJ"
      },
      "source": [
        "**Why is the LSTM so slow?** Despite our best efforts, we still need to make a lot of matrix multiplications per example (linear in the length of the example) just to get a single classification, and we can only process the 2nd word once we have computed the hidden state for the 1st word (sequential computation).\n",
        "\n",
        "GPUs are more efficient if we do a few big matrix multiplications, rather than lots of small ones. If we could process multiple examples at the same time, then we could exploit that. That is, we could still process the input sequentially, but doing so for multiple sentences at the same time.\n",
        "\n",
        "Up to now our \"mini-batches\" consisted of a single example. This was for a reason: the sentences in our data sets have **different lengths**, and this makes it difficult to process them at the same time.\n",
        "\n",
        "Consider a batch of 2 sentences:\n",
        "\n",
        "```\n",
        "this movie is bad\n",
        "this movie is super cool !\n",
        "```\n",
        "\n",
        "Let's say the IDs for these sentences are:\n",
        "\n",
        "```\n",
        "2 3 4 5\n",
        "2 3 4 6 7 8\n",
        "```\n",
        "\n",
        "We cannot feed PyTorch an object with rows of variable length! We need to turn this into a matrix.\n",
        "\n",
        "The solution is to add **padding values** to our mini-batch:\n",
        "\n",
        "```\n",
        "2 3 4 5 1 1\n",
        "2 3 4 6 7 8\n",
        "```\n",
        "\n",
        "Whenever a sentence is shorter than the longest sentence in a mini-batch, we just use a padding value (here: 1) to fill the matrix.\n",
        "\n",
        "In our computation, we should **ignore** the padding positions (e.g. mask them out) so that paddings do not contribute to the loss.\n",
        "\n",
        "#### Mini-batch feed\n",
        "We will now implement a `get_minibatch` function which will replace `get_example` and returns a mini-batch of the requested size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoAE2JBiXJ3P"
      },
      "source": [
        "def get_minibatch(data, batch_size=25, shuffle=True):\n",
        "  \"\"\"Return minibatches, optional shuffling\"\"\"\n",
        "  \n",
        "  if shuffle:\n",
        "    print(\"Shuffling training data\")\n",
        "    random.shuffle(data)  # shuffle training data each epoch\n",
        "  \n",
        "  batch = []\n",
        "  \n",
        "  # yield minibatches\n",
        "  for example in data:\n",
        "    batch.append(example)\n",
        "    \n",
        "    if len(batch) == batch_size:\n",
        "      yield batch\n",
        "      batch = []\n",
        "      \n",
        "  # in case there is something left\n",
        "  if len(batch) > 0:\n",
        "    yield batch"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwZM-XYkT8Zx"
      },
      "source": [
        "#### Padding function\n",
        "We will need a function that adds padding 1s to a sequence of IDs so that\n",
        "it becomes as long as the longest sequence in the minibatch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp0sK1ghw4Ft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b53d86-cdc1-4445-9949-75a63aa6957f"
      },
      "source": [
        "def pad(tokens, length, pad_value=1):\n",
        "  \"\"\"add padding 1s to a sequence to that it has the desired length\"\"\"\n",
        "  return tokens + [pad_value] * (length - len(tokens))\n",
        "\n",
        "# example\n",
        "tokens = [2, 3, 4]\n",
        "pad(tokens, 5)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 4, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL2iixMYUgfh"
      },
      "source": [
        "#### New `prepare` function\n",
        "\n",
        "We will also need a new function that turns a mini-batch into PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZID0cqozWks8"
      },
      "source": [
        "def prepare_minibatch(mb, vocab):\n",
        "  \"\"\"\n",
        "  Minibatch is a list of examples.\n",
        "  This function converts words to IDs and returns\n",
        "  torch tensors to be used as input/targets.\n",
        "  \"\"\"\n",
        "  batch_size = len(mb)\n",
        "  maxlen = max([len(ex.tokens) for ex in mb])\n",
        "  \n",
        "  # vocab returns 0 if the word is not there\n",
        "  x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen) for ex in mb]\n",
        "  \n",
        "  x = torch.LongTensor(x)\n",
        "  x = x.to(device)\n",
        "  \n",
        "  y = [ex.label for ex in mb]\n",
        "  y = torch.LongTensor(y)\n",
        "  y = y.to(device)\n",
        "  \n",
        "  return x, y"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwDAtCv1x2hB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1592a182-6ed2-484d-99d6-b886ae8c100d"
      },
      "source": [
        "# Let's test our new function.\n",
        "# This should give us 3 examples.\n",
        "mb = next(get_minibatch(train_data, batch_size=3, shuffle=False))\n",
        "for ex in mb:\n",
        "  print(ex)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example(tokens=['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.'], tree=Tree('3', [Tree('2', [Tree('2', ['The']), Tree('2', ['Rock'])]), Tree('4', [Tree('3', [Tree('2', ['is']), Tree('4', [Tree('2', ['destined']), Tree('2', [Tree('2', [Tree('2', [Tree('2', [Tree('2', ['to']), Tree('2', [Tree('2', ['be']), Tree('2', [Tree('2', ['the']), Tree('2', [Tree('2', ['21st']), Tree('2', [Tree('2', [Tree('2', ['Century']), Tree('2', [\"'s\"])]), Tree('2', [Tree('3', ['new']), Tree('2', [Tree('2', ['``']), Tree('2', ['Conan'])])])])])])])]), Tree('2', [\"''\"])]), Tree('2', ['and'])]), Tree('3', [Tree('2', ['that']), Tree('3', [Tree('2', ['he']), Tree('3', [Tree('2', [\"'s\"]), Tree('3', [Tree('2', ['going']), Tree('3', [Tree('2', ['to']), Tree('4', [Tree('3', [Tree('2', ['make']), Tree('3', [Tree('3', [Tree('2', ['a']), Tree('3', ['splash'])]), Tree('2', [Tree('2', ['even']), Tree('3', ['greater'])])])]), Tree('2', [Tree('2', ['than']), Tree('2', [Tree('2', [Tree('2', [Tree('2', [Tree('1', [Tree('2', ['Arnold']), Tree('2', ['Schwarzenegger'])]), Tree('2', [','])]), Tree('2', [Tree('2', ['Jean-Claud']), Tree('2', [Tree('2', ['Van']), Tree('2', ['Damme'])])])]), Tree('2', ['or'])]), Tree('2', [Tree('2', ['Steven']), Tree('2', ['Segal'])])])])])])])])])])])])]), Tree('2', ['.'])])]), label=3, transitions=[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1])\n",
            "Example(tokens=['The', 'gorgeously', 'elaborate', 'continuation', 'of', '``', 'The', 'Lord', 'of', 'the', 'Rings', \"''\", 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'can', 'not', 'adequately', 'describe', 'co-writer/director', 'Peter', 'Jackson', \"'s\", 'expanded', 'vision', 'of', 'J.R.R.', 'Tolkien', \"'s\", 'Middle-earth', '.'], tree=Tree('4', [Tree('4', [Tree('4', [Tree('2', ['The']), Tree('4', [Tree('3', ['gorgeously']), Tree('3', [Tree('2', ['elaborate']), Tree('2', ['continuation'])])])]), Tree('2', [Tree('2', [Tree('2', ['of']), Tree('2', ['``'])]), Tree('2', [Tree('2', ['The']), Tree('2', [Tree('2', [Tree('2', ['Lord']), Tree('2', [Tree('2', ['of']), Tree('2', [Tree('2', ['the']), Tree('2', ['Rings'])])])]), Tree('2', [Tree('2', [\"''\"]), Tree('2', ['trilogy'])])])])])]), Tree('2', [Tree('3', [Tree('2', [Tree('2', ['is']), Tree('2', [Tree('2', ['so']), Tree('2', ['huge'])])]), Tree('2', [Tree('2', ['that']), Tree('3', [Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', ['column'])]), Tree('2', [Tree('2', ['of']), Tree('2', ['words'])])]), Tree('2', [Tree('2', [Tree('2', [Tree('2', ['can']), Tree('1', ['not'])]), Tree('3', ['adequately'])]), Tree('2', [Tree('2', ['describe']), Tree('2', [Tree('3', [Tree('2', [Tree('2', ['co-writer/director']), Tree('2', [Tree('2', ['Peter']), Tree('3', [Tree('2', ['Jackson']), Tree('2', [\"'s\"])])])]), Tree('3', [Tree('2', ['expanded']), Tree('2', ['vision'])])]), Tree('2', [Tree('2', ['of']), Tree('2', [Tree('2', [Tree('2', ['J.R.R.']), Tree('2', [Tree('2', ['Tolkien']), Tree('2', [\"'s\"])])]), Tree('2', ['Middle-earth'])])])])])])])])]), Tree('2', ['.'])])]), label=4, transitions=[0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1])\n",
            "Example(tokens=['Singer/composer', 'Bryan', 'Adams', 'contributes', 'a', 'slew', 'of', 'songs', '--', 'a', 'few', 'potential', 'hits', ',', 'a', 'few', 'more', 'simply', 'intrusive', 'to', 'the', 'story', '--', 'but', 'the', 'whole', 'package', 'certainly', 'captures', 'the', 'intended', ',', 'er', ',', 'spirit', 'of', 'the', 'piece', '.'], tree=Tree('3', [Tree('3', [Tree('2', [Tree('2', [Tree('2', [Tree('2', [Tree('2', ['Singer/composer']), Tree('2', [Tree('2', ['Bryan']), Tree('2', ['Adams'])])]), Tree('2', [Tree('2', ['contributes']), Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', ['slew'])]), Tree('2', [Tree('2', ['of']), Tree('2', ['songs'])])])])]), Tree('2', [Tree('2', ['--']), Tree('2', [Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', [Tree('2', ['few']), Tree('3', ['potential'])])]), Tree('2', [Tree('2', [Tree('2', ['hits']), Tree('2', [','])]), Tree('2', [Tree('2', [Tree('2', ['a']), Tree('2', ['few'])]), Tree('1', [Tree('1', [Tree('2', ['more']), Tree('1', [Tree('2', ['simply']), Tree('2', ['intrusive'])])]), Tree('2', [Tree('2', ['to']), Tree('2', [Tree('2', ['the']), Tree('2', ['story'])])])])])])]), Tree('2', ['--'])])])]), Tree('2', ['but'])]), Tree('3', [Tree('4', [Tree('2', ['the']), Tree('3', [Tree('2', ['whole']), Tree('2', ['package'])])]), Tree('2', [Tree('3', ['certainly']), Tree('3', [Tree('2', ['captures']), Tree('2', [Tree('1', [Tree('2', ['the']), Tree('2', [Tree('2', [Tree('2', ['intended']), Tree('2', [Tree('2', [',']), Tree('2', [Tree('2', ['er']), Tree('2', [','])])])]), Tree('3', ['spirit'])])]), Tree('2', [Tree('2', ['of']), Tree('2', [Tree('2', ['the']), Tree('2', ['piece'])])])])])])])]), Tree('2', ['.'])]), label=3, transitions=[0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg8zEK8zyUCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec6d3bba-1eb7-4169-bfb5-ff55ee559402"
      },
      "source": [
        "# We should find padding 1s at the end\n",
        "x, y = prepare_minibatch(mb, v)\n",
        "print(\"x\", x)\n",
        "print(\"y\", y)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x tensor([[    7,  2859,     5,  6371,     0,    16,    11,     0,  5692,     0,\n",
            "            63,     0, 10539,     0,     0,     4,    22,     0,   120,     0,\n",
            "           105,     0,  7353,   149,  1293,    58,  5253,  4871,     0,     0,\n",
            "          6320, 18377,    28,  3325, 10866,     0,     1,     1,     1],\n",
            "        [    7, 15627,  4596,  6453,     0,     0,     7,  3404,     0,    11,\n",
            "          8913,     0,  9777,     5,    83,   913,     4,     0,  2974,     0,\n",
            "          1071,    48,    13,  5992,  3454, 18481,  1646,  1081,     0,  2431,\n",
            "          2201,     0,     0, 14081,     0,     0,     0,     1,     1],\n",
            "        [    0,  4202,  2907,  6524,     0,  6895,     0,  2014,     0,     0,\n",
            "           261,   602,   925,     0,     0,   261,    38,  1080,  9346,     0,\n",
            "            11,   511,     0,    32,    11,   806,  1714,  1172,  7027,    11,\n",
            "          1993,     0,  9494,     0,  2688,     0,    11,  1608,     0]],\n",
            "       device='cuda:0')\n",
            "y tensor([3, 4, 3], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYBJEoSNUwI0"
      },
      "source": [
        "#### Evaluate (mini-batch version)\n",
        "\n",
        "We can now update our evaluation function to use mini-batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiZZpEghzqou"
      },
      "source": [
        "def evaluate(model, data, \n",
        "             batch_fn=get_minibatch, prep_fn=prepare_minibatch,\n",
        "             batch_size=16):\n",
        "  \"\"\"Accuracy of a model on given data set (using mini-batches)\"\"\"\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model.eval()  # disable dropout\n",
        "\n",
        "  for mb in batch_fn(data, batch_size=batch_size, shuffle=False):\n",
        "    x, targets = prep_fn(mb, model.vocab)\n",
        "    with torch.no_grad():\n",
        "      logits = model(x)\n",
        "      \n",
        "    predictions = logits.argmax(dim=-1).view(-1)\n",
        "    \n",
        "    # add the number of correct predictions to the total correct\n",
        "    correct += (predictions == targets.view(-1)).sum().item()\n",
        "    total += targets.size(0)\n",
        "\n",
        "  return correct, total, correct / float(total)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23wAZomozh_2"
      },
      "source": [
        "# LSTM (Mini-batched)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-gkPU7jzBe2"
      },
      "source": [
        "With this, let's run the LSTM again but now using mini-batches!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "226Xg9OPzFbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b2e8a4-b76f-4d56-b405-6f816a3c2bd8"
      },
      "source": [
        "lstm_model = LSTMClassifier(\n",
        "    len(v.w2i), 300, 168, len(t2i), v)\n",
        "\n",
        "# copy pre-trained vectors into embeddings table\n",
        "with torch.no_grad():\n",
        "  lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "  lstm_model.embed.weight.requires_grad = False\n",
        "\n",
        "print(lstm_model)\n",
        "print_parameters(lstm_model)  \n",
        "  \n",
        "lstm_model = lstm_model.to(device)\n",
        "\n",
        "batch_size = 25\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=2e-4)\n",
        "\n",
        "start_time = time.time()\n",
        "lstm_losses, lstm_accuracies = train_model(\n",
        "    lstm_model, optimizer, num_iterations=30000, \n",
        "    print_every=250, eval_every=250,\n",
        "    batch_size=batch_size,\n",
        "    batch_fn=get_minibatch, \n",
        "    prep_fn=prepare_minibatch,\n",
        "    eval_fn=evaluate)\n",
        "print(\"Time taken for training LSTM by mini-batch: \", time.time()-start_time)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Fast LSTM\n",
            "LSTMClassifier(\n",
            "  (embed): Embedding(18922, 300, padding_idx=1)\n",
            "  (rnn): FastMyLSTMCell(300, 168)\n",
            "  (output_layer): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "embed.weight             [18922, 300] requires_grad=False\n",
            "rnn.W_i                  [300, 672]   requires_grad=True\n",
            "rnn.b_i                  [672]        requires_grad=True\n",
            "rnn.W_h                  [168, 672]   requires_grad=True\n",
            "rnn.b_h                  [672]        requires_grad=True\n",
            "output_layer.1.weight    [5, 168]     requires_grad=True\n",
            "output_layer.1.bias      [5]          requires_grad=True\n",
            "\n",
            "Total number of parameters: 5993285\n",
            "\n",
            "Shuffling training data\n",
            "Iter 250: loss=390.9841, time=5.84s\n",
            "iter 250: dev acc=0.3533\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 500: loss=353.4318, time=11.96s\n",
            "iter 500: dev acc=0.3978\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 750: loss=335.2589, time=19.14s\n",
            "iter 750: dev acc=0.4133\n",
            "new highscore\n",
            "Iter 1000: loss=327.3634, time=26.49s\n",
            "iter 1000: dev acc=0.4332\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1250: loss=320.5787, time=34.09s\n",
            "iter 1250: dev acc=0.4078\n",
            "Shuffling training data\n",
            "Iter 1500: loss=318.6634, time=40.23s\n",
            "iter 1500: dev acc=0.4233\n",
            "Shuffling training data\n",
            "Iter 1750: loss=316.0932, time=46.45s\n",
            "iter 1750: dev acc=0.4450\n",
            "new highscore\n",
            "Iter 2000: loss=310.1286, time=52.91s\n",
            "iter 2000: dev acc=0.4233\n",
            "Shuffling training data\n",
            "Iter 2250: loss=308.6057, time=59.03s\n",
            "iter 2250: dev acc=0.4460\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 2500: loss=308.5014, time=65.36s\n",
            "iter 2500: dev acc=0.4414\n",
            "Shuffling training data\n",
            "Iter 2750: loss=303.4604, time=71.51s\n",
            "iter 2750: dev acc=0.4432\n",
            "Iter 3000: loss=301.1661, time=77.80s\n",
            "iter 3000: dev acc=0.4578\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 3250: loss=300.3988, time=84.11s\n",
            "iter 3250: dev acc=0.4314\n",
            "Shuffling training data\n",
            "Iter 3500: loss=296.8061, time=90.25s\n",
            "iter 3500: dev acc=0.4559\n",
            "Iter 3750: loss=295.5050, time=96.44s\n",
            "iter 3750: dev acc=0.4496\n",
            "Shuffling training data\n",
            "Iter 4000: loss=292.7315, time=102.59s\n",
            "iter 4000: dev acc=0.4423\n",
            "Shuffling training data\n",
            "Iter 4250: loss=290.5110, time=108.72s\n",
            "iter 4250: dev acc=0.4505\n",
            "Shuffling training data\n",
            "Iter 4500: loss=288.8722, time=115.05s\n",
            "iter 4500: dev acc=0.4550\n",
            "Iter 4750: loss=286.3431, time=121.16s\n",
            "iter 4750: dev acc=0.4460\n",
            "Shuffling training data\n",
            "Iter 5000: loss=282.9046, time=127.31s\n",
            "iter 5000: dev acc=0.4623\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 5250: loss=281.6941, time=133.51s\n",
            "iter 5250: dev acc=0.4460\n",
            "Shuffling training data\n",
            "Iter 5500: loss=283.1632, time=139.70s\n",
            "iter 5500: dev acc=0.4378\n",
            "Iter 5750: loss=275.5873, time=146.06s\n",
            "iter 5750: dev acc=0.4423\n",
            "Shuffling training data\n",
            "Iter 6000: loss=273.0000, time=152.23s\n",
            "iter 6000: dev acc=0.4596\n",
            "Shuffling training data\n",
            "Iter 6250: loss=271.1988, time=158.44s\n",
            "iter 6250: dev acc=0.4487\n",
            "Shuffling training data\n",
            "Iter 6500: loss=271.1175, time=164.70s\n",
            "iter 6500: dev acc=0.4714\n",
            "new highscore\n",
            "Iter 6750: loss=267.7959, time=170.95s\n",
            "iter 6750: dev acc=0.4641\n",
            "Shuffling training data\n",
            "Iter 7000: loss=262.3288, time=177.16s\n",
            "iter 7000: dev acc=0.4487\n",
            "Shuffling training data\n",
            "Iter 7250: loss=262.7535, time=183.32s\n",
            "iter 7250: dev acc=0.4514\n",
            "Iter 7500: loss=260.8217, time=189.52s\n",
            "iter 7500: dev acc=0.4659\n",
            "Shuffling training data\n",
            "Iter 7750: loss=255.6513, time=195.70s\n",
            "iter 7750: dev acc=0.4541\n",
            "Shuffling training data\n",
            "Iter 8000: loss=251.5251, time=201.83s\n",
            "iter 8000: dev acc=0.4469\n",
            "Shuffling training data\n",
            "Iter 8250: loss=253.4047, time=208.07s\n",
            "iter 8250: dev acc=0.4441\n",
            "Iter 8500: loss=246.9329, time=214.24s\n",
            "iter 8500: dev acc=0.4423\n",
            "Shuffling training data\n",
            "Iter 8750: loss=242.0578, time=220.36s\n",
            "iter 8750: dev acc=0.4460\n",
            "Shuffling training data\n",
            "Iter 9000: loss=244.8666, time=226.66s\n",
            "iter 9000: dev acc=0.4478\n",
            "Shuffling training data\n",
            "Iter 9250: loss=238.9686, time=232.92s\n",
            "iter 9250: dev acc=0.4323\n",
            "Iter 9500: loss=232.5706, time=240.48s\n",
            "iter 9500: dev acc=0.4450\n",
            "Shuffling training data\n",
            "Iter 9750: loss=234.2722, time=246.66s\n",
            "iter 9750: dev acc=0.4260\n",
            "Shuffling training data\n",
            "Iter 10000: loss=229.9471, time=252.93s\n",
            "iter 10000: dev acc=0.4351\n",
            "Iter 10250: loss=231.2111, time=259.15s\n",
            "iter 10250: dev acc=0.4314\n",
            "Shuffling training data\n",
            "Iter 10500: loss=224.6704, time=265.40s\n",
            "iter 10500: dev acc=0.4260\n",
            "Shuffling training data\n",
            "Iter 10750: loss=221.4214, time=271.65s\n",
            "iter 10750: dev acc=0.4332\n",
            "Shuffling training data\n",
            "Iter 11000: loss=216.1001, time=277.89s\n",
            "iter 11000: dev acc=0.4205\n",
            "Iter 11250: loss=216.0191, time=284.06s\n",
            "iter 11250: dev acc=0.4151\n",
            "Shuffling training data\n",
            "Iter 11500: loss=214.8565, time=290.22s\n",
            "iter 11500: dev acc=0.4160\n",
            "Shuffling training data\n",
            "Iter 11750: loss=208.8171, time=296.43s\n",
            "iter 11750: dev acc=0.4242\n",
            "Shuffling training data\n",
            "Iter 12000: loss=206.0936, time=302.62s\n",
            "iter 12000: dev acc=0.4214\n",
            "Iter 12250: loss=203.6324, time=308.96s\n",
            "iter 12250: dev acc=0.4151\n",
            "Shuffling training data\n",
            "Iter 12500: loss=200.1626, time=315.20s\n",
            "iter 12500: dev acc=0.4033\n",
            "Shuffling training data\n",
            "Iter 12750: loss=197.8205, time=321.45s\n",
            "iter 12750: dev acc=0.4005\n",
            "Shuffling training data\n",
            "Iter 13000: loss=195.4316, time=327.78s\n",
            "iter 13000: dev acc=0.4223\n",
            "Iter 13250: loss=188.3804, time=334.02s\n",
            "iter 13250: dev acc=0.4196\n",
            "Shuffling training data\n",
            "Iter 13500: loss=186.3494, time=340.06s\n",
            "iter 13500: dev acc=0.4069\n",
            "Shuffling training data\n",
            "Iter 13750: loss=189.1656, time=346.22s\n",
            "iter 13750: dev acc=0.4160\n",
            "Iter 14000: loss=184.5273, time=352.41s\n",
            "iter 14000: dev acc=0.4160\n",
            "Shuffling training data\n",
            "Iter 14250: loss=186.0439, time=358.65s\n",
            "iter 14250: dev acc=0.4196\n",
            "Shuffling training data\n",
            "Iter 14500: loss=174.0128, time=364.67s\n",
            "iter 14500: dev acc=0.3915\n",
            "Shuffling training data\n",
            "Iter 14750: loss=190.3114, time=370.83s\n",
            "iter 14750: dev acc=0.4214\n",
            "Iter 15000: loss=167.6629, time=377.03s\n",
            "iter 15000: dev acc=0.3942\n",
            "Shuffling training data\n",
            "Iter 15250: loss=165.7176, time=383.23s\n",
            "iter 15250: dev acc=0.4223\n",
            "Shuffling training data\n",
            "Iter 15500: loss=163.4401, time=389.42s\n",
            "iter 15500: dev acc=0.4051\n",
            "Shuffling training data\n",
            "Iter 15750: loss=161.7410, time=395.46s\n",
            "iter 15750: dev acc=0.3978\n",
            "Iter 16000: loss=156.3576, time=401.73s\n",
            "iter 16000: dev acc=0.3878\n",
            "Shuffling training data\n",
            "Iter 16250: loss=153.9117, time=408.00s\n",
            "iter 16250: dev acc=0.4114\n",
            "Shuffling training data\n",
            "Iter 16500: loss=151.5101, time=414.25s\n",
            "iter 16500: dev acc=0.3924\n",
            "Iter 16750: loss=148.0976, time=420.51s\n",
            "iter 16750: dev acc=0.3933\n",
            "Shuffling training data\n",
            "Iter 17000: loss=145.2062, time=426.67s\n",
            "iter 17000: dev acc=0.3933\n",
            "Shuffling training data\n",
            "Iter 17250: loss=141.6859, time=432.88s\n",
            "iter 17250: dev acc=0.4051\n",
            "Shuffling training data\n",
            "Iter 17500: loss=137.4775, time=439.04s\n",
            "iter 17500: dev acc=0.3960\n",
            "Iter 17750: loss=141.9198, time=446.23s\n",
            "iter 17750: dev acc=0.4069\n",
            "Shuffling training data\n",
            "Iter 18000: loss=130.4866, time=452.93s\n",
            "iter 18000: dev acc=0.4042\n",
            "Shuffling training data\n",
            "Iter 18250: loss=132.2072, time=459.12s\n",
            "iter 18250: dev acc=0.3860\n",
            "Shuffling training data\n",
            "Iter 18500: loss=124.6004, time=465.34s\n",
            "iter 18500: dev acc=0.4096\n",
            "Iter 18750: loss=126.0829, time=471.64s\n",
            "iter 18750: dev acc=0.3987\n",
            "Shuffling training data\n",
            "Iter 19000: loss=123.8251, time=477.85s\n",
            "iter 19000: dev acc=0.4124\n",
            "Shuffling training data\n",
            "Iter 19250: loss=117.4813, time=484.08s\n",
            "iter 19250: dev acc=0.3806\n",
            "Shuffling training data\n",
            "Iter 19500: loss=117.6929, time=490.26s\n",
            "iter 19500: dev acc=0.3860\n",
            "Iter 19750: loss=121.9611, time=496.53s\n",
            "iter 19750: dev acc=0.3915\n",
            "Shuffling training data\n",
            "Iter 20000: loss=112.6524, time=502.67s\n",
            "iter 20000: dev acc=0.3915\n",
            "Shuffling training data\n",
            "Iter 20250: loss=115.5401, time=508.95s\n",
            "iter 20250: dev acc=0.3915\n",
            "Iter 20500: loss=103.3135, time=515.20s\n",
            "iter 20500: dev acc=0.3697\n",
            "Shuffling training data\n",
            "Iter 20750: loss=113.9833, time=521.41s\n",
            "iter 20750: dev acc=0.3987\n",
            "Shuffling training data\n",
            "Iter 21000: loss=97.8768, time=527.56s\n",
            "iter 21000: dev acc=0.3824\n",
            "Shuffling training data\n",
            "Iter 21250: loss=104.3815, time=533.74s\n",
            "iter 21250: dev acc=0.3906\n",
            "Iter 21500: loss=93.3462, time=540.00s\n",
            "iter 21500: dev acc=0.3751\n",
            "Shuffling training data\n",
            "Iter 21750: loss=107.1772, time=546.21s\n",
            "iter 21750: dev acc=0.3842\n",
            "Shuffling training data\n",
            "Iter 22000: loss=96.8999, time=552.54s\n",
            "iter 22000: dev acc=0.3860\n",
            "Shuffling training data\n",
            "Iter 22250: loss=90.1714, time=558.69s\n",
            "iter 22250: dev acc=0.3751\n",
            "Iter 22500: loss=89.4112, time=564.96s\n",
            "iter 22500: dev acc=0.3906\n",
            "Shuffling training data\n",
            "Iter 22750: loss=86.2074, time=571.41s\n",
            "iter 22750: dev acc=0.4024\n",
            "Shuffling training data\n",
            "Iter 23000: loss=94.8667, time=577.68s\n",
            "iter 23000: dev acc=0.3815\n",
            "Iter 23250: loss=85.8718, time=583.88s\n",
            "iter 23250: dev acc=0.3751\n",
            "Shuffling training data\n",
            "Iter 23500: loss=89.5554, time=590.09s\n",
            "iter 23500: dev acc=0.3951\n",
            "Shuffling training data\n",
            "Iter 23750: loss=85.8056, time=596.34s\n",
            "iter 23750: dev acc=0.3887\n",
            "Shuffling training data\n",
            "Iter 24000: loss=85.7180, time=602.60s\n",
            "iter 24000: dev acc=0.3924\n",
            "Iter 24250: loss=83.3447, time=608.82s\n",
            "iter 24250: dev acc=0.3742\n",
            "Shuffling training data\n",
            "Iter 24500: loss=84.7078, time=614.90s\n",
            "iter 24500: dev acc=0.3760\n",
            "Shuffling training data\n",
            "Iter 24750: loss=82.2332, time=621.07s\n",
            "iter 24750: dev acc=0.3960\n",
            "Shuffling training data\n",
            "Iter 25000: loss=73.2133, time=627.28s\n",
            "iter 25000: dev acc=0.3760\n",
            "Iter 25250: loss=75.6489, time=633.49s\n",
            "iter 25250: dev acc=0.3688\n",
            "Shuffling training data\n",
            "Iter 25500: loss=76.0861, time=639.70s\n",
            "iter 25500: dev acc=0.3733\n",
            "Shuffling training data\n",
            "Iter 25750: loss=76.4472, time=645.98s\n",
            "iter 25750: dev acc=0.3851\n",
            "Shuffling training data\n",
            "Iter 26000: loss=65.5304, time=652.30s\n",
            "iter 26000: dev acc=0.3778\n",
            "Iter 26250: loss=58.8651, time=659.90s\n",
            "iter 26250: dev acc=0.3724\n",
            "Shuffling training data\n",
            "Iter 26500: loss=61.2048, time=666.19s\n",
            "iter 26500: dev acc=0.3760\n",
            "Shuffling training data\n",
            "Iter 26750: loss=66.3843, time=672.41s\n",
            "iter 26750: dev acc=0.3833\n",
            "Iter 27000: loss=65.8570, time=678.51s\n",
            "iter 27000: dev acc=0.3833\n",
            "Shuffling training data\n",
            "Iter 27250: loss=64.7072, time=684.74s\n",
            "iter 27250: dev acc=0.3924\n",
            "Shuffling training data\n",
            "Iter 27500: loss=56.6595, time=691.00s\n",
            "iter 27500: dev acc=0.3706\n",
            "Shuffling training data\n",
            "Iter 27750: loss=69.0185, time=697.31s\n",
            "iter 27750: dev acc=0.3560\n",
            "Iter 28000: loss=69.2386, time=703.52s\n",
            "iter 28000: dev acc=0.3851\n",
            "Shuffling training data\n",
            "Iter 28250: loss=90.6580, time=709.67s\n",
            "iter 28250: dev acc=0.3869\n",
            "Shuffling training data\n",
            "Iter 28500: loss=69.5717, time=715.93s\n",
            "iter 28500: dev acc=0.3787\n",
            "Shuffling training data\n",
            "Iter 28750: loss=58.4872, time=722.15s\n",
            "iter 28750: dev acc=0.3760\n",
            "Iter 29000: loss=50.1418, time=728.35s\n",
            "iter 29000: dev acc=0.3724\n",
            "Shuffling training data\n",
            "Iter 29250: loss=47.0390, time=734.71s\n",
            "iter 29250: dev acc=0.3787\n",
            "Shuffling training data\n",
            "Iter 29500: loss=57.5382, time=740.83s\n",
            "iter 29500: dev acc=0.3887\n",
            "Iter 29750: loss=56.6340, time=747.07s\n",
            "iter 29750: dev acc=0.3751\n",
            "Shuffling training data\n",
            "Iter 30000: loss=45.7931, time=753.25s\n",
            "iter 30000: dev acc=0.3787\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 6500: train acc=0.5549, dev acc=0.4714, test acc=0.4670\n",
            "Time taken for training LSTM by mini-batch:  757.6875174045563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymj1rLDMvyhp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "0c801293-38eb-4658-8578-1475831b481d"
      },
      "source": [
        "# plot validation accuracy\n",
        "plot_graph(lstm_accuracies, \"Fig 4.3.a LSTM Mini Batch Model No. of Iterations vs Val Accuracies\", \"Accuracy\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"331.389812pt\" version=\"1.1\" viewBox=\"0 0 443.037188 331.389812\" width=\"443.037188pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 331.389812 \nL 443.037188 331.389812 \nL 443.037188 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 56.805937 288.430125 \nL 413.925938 288.430125 \nL 413.925938 22.318125 \nL 56.805937 22.318125 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p3209e2a560)\" d=\"M 70.310475 288.430125 \nL 70.310475 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(66.8111 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#p3209e2a560)\" d=\"M 124.874264 288.430125 \nL 124.874264 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(117.875514 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p3209e2a560)\" d=\"M 179.438054 288.430125 \nL 179.438054 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(172.439304 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#p3209e2a560)\" d=\"M 234.001843 288.430125 \nL 234.001843 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(227.003093 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p3209e2a560)\" d=\"M 288.565632 288.430125 \nL 288.565632 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(281.566882 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#p3209e2a560)\" d=\"M 343.129421 288.430125 \nL 343.129421 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(332.631296 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p3209e2a560)\" d=\"M 397.69321 288.430125 \nL 397.69321 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120 -->\n      <g style=\"fill:#262626;\" transform=\"translate(387.195085 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- # of Iterations -->\n     <defs>\n      <path d=\"M 51.125 44 \nL 36.921875 44 \nL 32.8125 27.6875 \nL 47.125 27.6875 \nz\nM 43.796875 71.78125 \nL 38.71875 51.515625 \nL 52.984375 51.515625 \nL 58.109375 71.78125 \nL 65.921875 71.78125 \nL 60.890625 51.515625 \nL 76.125 51.515625 \nL 76.125 44 \nL 58.984375 44 \nL 54.984375 27.6875 \nL 70.515625 27.6875 \nL 70.515625 20.21875 \nL 53.078125 20.21875 \nL 48 0 \nL 40.1875 0 \nL 45.21875 20.21875 \nL 30.90625 20.21875 \nL 25.875 0 \nL 18.015625 0 \nL 23.09375 20.21875 \nL 7.71875 20.21875 \nL 7.71875 27.6875 \nL 24.90625 27.6875 \nL 29 44 \nL 13.28125 44 \nL 13.28125 51.515625 \nL 30.90625 51.515625 \nL 35.890625 71.78125 \nz\n\" id=\"DejaVuSans-35\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-73\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(192.165 321.694187)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"83.789062\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"115.576172\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"176.757812\" xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"211.962891\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"243.75\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"273.242188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"312.451172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"373.974609\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"415.087891\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"476.367188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"515.576172\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"543.359375\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"604.541016\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"667.919922\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#p3209e2a560)\" d=\"M 56.805937 273.942483 \nL 413.925938 273.942483 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.36 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 278.121624)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p3209e2a560)\" d=\"M 56.805937 230.597663 \nL 413.925938 230.597663 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.38 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 234.776803)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#p3209e2a560)\" d=\"M 56.805937 187.252843 \nL 413.925938 187.252843 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.40 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 191.431983)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p3209e2a560)\" d=\"M 56.805937 143.908022 \nL 413.925938 143.908022 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.42 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 148.087163)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <path clip-path=\"url(#p3209e2a560)\" d=\"M 56.805937 100.563202 \nL 413.925938 100.563202 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.44 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 104.742343)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p3209e2a560)\" d=\"M 56.805937 57.218382 \nL 413.925938 57.218382 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.46 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 61.397523)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- Accuracy -->\n     <defs>\n      <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(16.318125 182.767875)rotate(-90)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"66.658203\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"121.638672\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"176.619141\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"239.998047\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"281.111328\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"342.390625\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"397.371094\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p3209e2a560)\" d=\"M 73.038665 288.430125 \nL 75.766854 191.977074 \nL 78.495044 158.51377 \nL 81.223233 115.208319 \nL 83.951423 170.324348 \nL 86.679612 136.861044 \nL 89.407802 89.618734 \nL 92.135991 136.861044 \nL 94.86418 87.650304 \nL 97.59237 97.492452 \nL 100.320559 93.555593 \nL 103.048749 62.060719 \nL 105.776938 119.145178 \nL 108.505128 65.997578 \nL 111.233317 79.776586 \nL 113.961507 95.524022 \nL 116.689696 77.808156 \nL 119.417886 67.966008 \nL 122.146075 87.650304 \nL 124.874264 52.218571 \nL 127.602454 87.650304 \nL 130.330643 105.366171 \nL 133.058833 95.524022 \nL 135.787022 58.12386 \nL 138.515212 81.745015 \nL 141.243401 32.534275 \nL 143.971591 48.281712 \nL 146.69978 81.745015 \nL 149.42797 75.839726 \nL 152.156159 44.344852 \nL 154.884349 69.934437 \nL 157.612538 85.681874 \nL 160.340727 91.587163 \nL 163.068917 95.524022 \nL 165.797106 87.650304 \nL 168.525296 83.713445 \nL 171.253485 117.176748 \nL 173.981675 89.618734 \nL 176.709864 130.955756 \nL 179.438054 111.271459 \nL 182.166243 119.145178 \nL 184.894433 130.955756 \nL 187.622622 115.208319 \nL 190.350811 142.766333 \nL 193.079001 154.576911 \nL 195.80719 152.608481 \nL 198.53538 134.892615 \nL 201.263569 140.797904 \nL 203.991759 154.576911 \nL 206.719948 180.166496 \nL 209.448138 186.071785 \nL 212.176327 138.829474 \nL 214.904517 144.734763 \nL 217.632706 172.292778 \nL 220.360895 152.608481 \nL 223.089085 152.608481 \nL 225.817274 144.734763 \nL 228.545464 205.756081 \nL 231.273653 140.797904 \nL 234.001843 199.850792 \nL 236.730032 138.829474 \nL 239.458222 176.229637 \nL 242.186411 191.977074 \nL 244.914601 213.6298 \nL 247.64279 162.450629 \nL 250.37098 203.787651 \nL 253.099169 201.819222 \nL 255.827358 201.819222 \nL 258.555548 176.229637 \nL 261.283737 195.913933 \nL 264.011927 172.292778 \nL 266.740116 178.198066 \nL 269.468306 217.566659 \nL 272.196495 166.387489 \nL 274.924685 190.008644 \nL 277.652874 160.4822 \nL 280.381064 229.377236 \nL 283.109253 217.566659 \nL 285.837442 205.756081 \nL 288.565632 205.756081 \nL 291.293821 205.756081 \nL 294.022011 252.998392 \nL 296.7502 190.008644 \nL 299.47839 225.440377 \nL 302.206579 207.724511 \nL 304.934769 241.187814 \nL 307.662958 221.503518 \nL 310.391148 217.566659 \nL 313.119337 241.187814 \nL 315.847526 207.724511 \nL 318.575716 182.134926 \nL 321.303905 227.408807 \nL 324.032095 241.187814 \nL 326.760284 197.882363 \nL 329.488474 211.66137 \nL 332.216663 203.787651 \nL 334.944853 243.156244 \nL 337.673042 239.219385 \nL 340.401232 195.913933 \nL 343.129421 239.219385 \nL 345.857611 254.966822 \nL 348.5858 245.124673 \nL 351.313989 219.535088 \nL 354.042179 235.282525 \nL 356.770368 247.093103 \nL 359.498558 239.219385 \nL 362.226747 223.471948 \nL 364.954937 223.471948 \nL 367.683126 203.787651 \nL 370.411316 251.029962 \nL 373.139505 282.524836 \nL 375.867695 219.535088 \nL 378.595884 215.598229 \nL 381.324073 233.314096 \nL 384.052263 239.219385 \nL 386.780452 247.093103 \nL 389.508642 233.314096 \nL 392.236831 211.66137 \nL 394.965021 241.187814 \nL 397.69321 233.314096 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 56.805937 288.430125 \nL 56.805937 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 413.925938 288.430125 \nL 413.925938 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 56.805937 288.430125 \nL 413.925937 288.430125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 56.805937 22.318125 \nL 413.925937 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"text_16\">\n    <!-- Fig 4.3.a LSTM Mini Batch Model No. of Iterations vs Val Accuracies -->\n    <defs>\n     <path d=\"M 9.8125 72.90625 \nL 51.703125 72.90625 \nL 51.703125 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.109375 \nL 48.578125 43.109375 \nL 48.578125 34.8125 \nL 19.671875 34.8125 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-70\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n     <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n     <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n     <path d=\"M 19.671875 34.8125 \nL 19.671875 8.109375 \nL 35.5 8.109375 \nQ 43.453125 8.109375 47.28125 11.40625 \nQ 51.125 14.703125 51.125 21.484375 \nQ 51.125 28.328125 47.28125 31.5625 \nQ 43.453125 34.8125 35.5 34.8125 \nz\nM 19.671875 64.796875 \nL 19.671875 42.828125 \nL 34.28125 42.828125 \nQ 41.5 42.828125 45.03125 45.53125 \nQ 48.578125 48.25 48.578125 53.8125 \nQ 48.578125 59.328125 45.03125 62.0625 \nQ 41.5 64.796875 34.28125 64.796875 \nz\nM 9.8125 72.90625 \nL 35.015625 72.90625 \nQ 46.296875 72.90625 52.390625 68.21875 \nQ 58.5 63.53125 58.5 54.890625 \nQ 58.5 48.1875 55.375 44.234375 \nQ 52.25 40.28125 46.1875 39.3125 \nQ 53.46875 37.75 57.5 32.78125 \nQ 61.53125 27.828125 61.53125 20.40625 \nQ 61.53125 10.640625 54.890625 5.3125 \nQ 48.25 0 35.984375 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-66\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     <path d=\"M 28.609375 0 \nL 0.78125 72.90625 \nL 11.078125 72.90625 \nL 34.1875 11.53125 \nL 57.328125 72.90625 \nL 67.578125 72.90625 \nL 39.796875 0 \nz\n\" id=\"DejaVuSans-86\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(34.894688 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"50.269531\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"78.052734\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"141.529297\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"173.316406\" xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"236.939453\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"268.726562\" xlink:href=\"#DejaVuSans-51\"/>\n     <use x=\"332.349609\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"364.136719\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"425.416016\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"457.203125\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"512.916016\" xlink:href=\"#DejaVuSans-83\"/>\n     <use x=\"576.392578\" xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"637.476562\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"723.755859\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"755.542969\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"841.822266\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"869.605469\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"932.984375\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"960.767578\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"992.554688\" xlink:href=\"#DejaVuSans-66\"/>\n     <use x=\"1061.158203\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1122.4375\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1161.646484\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1216.626953\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"1280.005859\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1311.792969\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"1398.072266\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1459.253906\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"1522.730469\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1584.253906\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"1612.037109\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1643.824219\" xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"1718.628906\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1778.060547\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"1809.847656\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1841.634766\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1902.816406\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"1938.021484\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1969.808594\" xlink:href=\"#DejaVuSans-73\"/>\n     <use x=\"1999.300781\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"2038.509766\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"2100.033203\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"2141.146484\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2202.425781\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"2241.634766\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"2269.417969\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"2330.599609\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"2393.978516\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2446.078125\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2477.865234\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"2537.044922\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2589.144531\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2620.931641\" xlink:href=\"#DejaVuSans-86\"/>\n     <use x=\"2681.589844\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2742.869141\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"2770.652344\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2802.439453\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"2869.097656\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"2924.078125\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"2979.058594\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"3042.4375\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"3083.550781\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"3144.830078\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"3199.810547\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"3227.59375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"3289.117188\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 401.825937 36.618125 \nL 406.225937 36.618125 \nQ 408.425937 36.618125 408.425937 34.418125 \nL 408.425937 30.018125 \nQ 408.425937 27.818125 406.225937 27.818125 \nL 401.825937 27.818125 \nQ 399.625937 27.818125 399.625937 30.018125 \nL 399.625937 34.418125 \nQ 399.625937 36.618125 401.825937 36.618125 \nz\n\" style=\"fill:#eaeaf2;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p3209e2a560\">\n   <rect height=\"266.112\" width=\"357.12\" x=\"56.805937\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "application/pdf": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1je5S1RHVC5R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "32f3253f-2bdf-4271-dd31-6d3a651f7c7d"
      },
      "source": [
        "# plot training loss\n",
        "plot_graph(lstm_losses, \"Fig 4.3.b LSTM Mini Batch Model No. of Iterations vs Loss\", \"Loss\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"331.389812pt\" version=\"1.1\" viewBox=\"0 0 417.63 331.389812\" width=\"417.63pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 331.389812 \nL 417.63 331.389812 \nL 417.63 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 53.31 288.430125 \nL 410.43 288.430125 \nL 410.43 22.318125 \nL 53.31 22.318125 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p3a72f91066)\" d=\"M 66.814538 288.430125 \nL 66.814538 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(63.315163 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#p3a72f91066)\" d=\"M 121.378327 288.430125 \nL 121.378327 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(114.379577 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p3a72f91066)\" d=\"M 175.942116 288.430125 \nL 175.942116 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(168.943366 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#p3a72f91066)\" d=\"M 230.505905 288.430125 \nL 230.505905 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(223.507155 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p3a72f91066)\" d=\"M 285.069694 288.430125 \nL 285.069694 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(278.070944 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#p3a72f91066)\" d=\"M 339.633484 288.430125 \nL 339.633484 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(329.135359 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p3a72f91066)\" d=\"M 394.197273 288.430125 \nL 394.197273 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120 -->\n      <g style=\"fill:#262626;\" transform=\"translate(383.699148 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- # of Iterations -->\n     <defs>\n      <path d=\"M 51.125 44 \nL 36.921875 44 \nL 32.8125 27.6875 \nL 47.125 27.6875 \nz\nM 43.796875 71.78125 \nL 38.71875 51.515625 \nL 52.984375 51.515625 \nL 58.109375 71.78125 \nL 65.921875 71.78125 \nL 60.890625 51.515625 \nL 76.125 51.515625 \nL 76.125 44 \nL 58.984375 44 \nL 54.984375 27.6875 \nL 70.515625 27.6875 \nL 70.515625 20.21875 \nL 53.078125 20.21875 \nL 48 0 \nL 40.1875 0 \nL 45.21875 20.21875 \nL 30.90625 20.21875 \nL 25.875 0 \nL 18.015625 0 \nL 23.09375 20.21875 \nL 7.71875 20.21875 \nL 7.71875 27.6875 \nL 24.90625 27.6875 \nL 29 44 \nL 13.28125 44 \nL 13.28125 51.515625 \nL 30.90625 51.515625 \nL 35.890625 71.78125 \nz\n\" id=\"DejaVuSans-35\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-73\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(188.669062 321.694187)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"83.789062\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"115.576172\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"176.757812\" xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"211.962891\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"243.75\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"273.242188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"312.451172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"373.974609\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"415.087891\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"476.367188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"515.576172\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"543.359375\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"604.541016\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"667.919922\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#p3a72f91066)\" d=\"M 53.31 285.223272 \nL 410.43 285.223272 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(29.8125 289.402413)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p3a72f91066)\" d=\"M 53.31 247.109361 \nL 410.43 247.109361 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 100 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 251.288502)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#p3a72f91066)\" d=\"M 53.31 208.99545 \nL 410.43 208.99545 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 150 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 213.174591)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p3a72f91066)\" d=\"M 53.31 170.881539 \nL 410.43 170.881539 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 200 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 175.06068)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <path clip-path=\"url(#p3a72f91066)\" d=\"M 53.31 132.767629 \nL 410.43 132.767629 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 250 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 136.946769)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p3a72f91066)\" d=\"M 53.31 94.653718 \nL 410.43 94.653718 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_14\">\n      <!-- 300 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 98.832858)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <path clip-path=\"url(#p3a72f91066)\" d=\"M 53.31 56.539807 \nL 410.43 56.539807 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_15\">\n      <!-- 350 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 60.718947)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- Loss -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(16.318125 168.53475)rotate(-90)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p3a72f91066)\" d=\"M 69.542727 25.298512 \nL 72.270917 53.923836 \nL 74.999106 67.776615 \nL 77.727296 73.7952 \nL 80.455485 78.967005 \nL 83.183675 80.426983 \nL 85.911864 82.386192 \nL 88.640053 86.932885 \nL 91.368243 88.093775 \nL 94.096432 88.173297 \nL 96.824622 92.015958 \nL 99.552811 93.764835 \nL 102.281001 94.349758 \nL 105.00919 97.088352 \nL 107.73738 98.080187 \nL 110.465569 100.194304 \nL 113.193759 101.886944 \nL 115.921948 103.136212 \nL 118.650138 105.064094 \nL 121.378327 107.685189 \nL 124.106516 108.60791 \nL 126.834706 107.488012 \nL 129.562895 113.263013 \nL 132.291085 115.235225 \nL 135.019274 116.608223 \nL 137.747464 116.670183 \nL 140.475653 119.202205 \nL 143.203843 123.369664 \nL 145.932032 123.045878 \nL 148.660222 124.518451 \nL 151.388411 128.459802 \nL 154.1166 131.605052 \nL 156.84479 130.172299 \nL 159.572979 135.105594 \nL 162.301169 138.821778 \nL 165.029358 136.680696 \nL 167.757548 141.176646 \nL 170.485737 146.053716 \nL 173.213927 144.75659 \nL 175.942116 148.053489 \nL 178.670306 147.090001 \nL 181.398495 152.075798 \nL 184.126684 154.552465 \nL 186.854874 158.608751 \nL 189.583063 158.670491 \nL 192.311253 159.556775 \nL 195.039442 164.160444 \nL 197.767632 166.236483 \nL 200.495821 168.112627 \nL 203.224011 170.757591 \nL 205.9522 172.542951 \nL 208.68039 174.363911 \nL 211.408579 179.738882 \nL 214.136769 181.287088 \nL 216.864958 179.140376 \nL 219.593147 182.676078 \nL 222.321337 181.519978 \nL 225.049526 190.691042 \nL 227.777716 178.266959 \nL 230.505905 195.531399 \nL 233.234095 197.014304 \nL 235.962284 198.750364 \nL 238.690474 200.045536 \nL 241.418663 204.149187 \nL 244.146853 206.013613 \nL 246.875042 207.844367 \nL 249.603231 210.445611 \nL 252.331421 212.649691 \nL 255.05961 215.333108 \nL 257.7878 218.541093 \nL 260.515989 215.154809 \nL 263.244179 223.870052 \nL 265.972368 222.55849 \nL 268.700558 228.356996 \nL 271.428747 227.226918 \nL 274.156937 228.948031 \nL 276.885126 233.783762 \nL 279.613316 233.622446 \nL 282.341505 230.368925 \nL 285.069694 237.464703 \nL 287.797884 235.263447 \nL 290.526073 244.583567 \nL 293.254263 236.450191 \nL 295.982452 248.727833 \nL 298.710642 243.769434 \nL 301.438831 252.181378 \nL 304.167021 241.638371 \nL 306.89521 249.47252 \nL 309.6234 254.601452 \nL 312.351589 255.180981 \nL 315.079778 257.62314 \nL 317.807968 251.022391 \nL 320.536157 257.878987 \nL 323.264347 255.07102 \nL 325.992536 257.929446 \nL 328.720726 257.996211 \nL 331.448915 259.805331 \nL 334.177105 258.766245 \nL 336.905294 260.652606 \nL 339.633484 267.528251 \nL 342.361673 265.671658 \nL 345.089862 265.338439 \nL 347.818052 265.063131 \nL 350.546241 273.384812 \nL 353.274431 278.465626 \nL 356.00262 276.682096 \nL 358.73081 272.733885 \nL 361.458999 273.135849 \nL 364.187189 274.012267 \nL 366.915378 280.146911 \nL 369.643568 270.725885 \nL 372.371757 270.558122 \nL 375.099947 254.23053 \nL 377.828136 270.304161 \nL 380.556325 278.753692 \nL 383.284515 285.115171 \nL 386.012704 287.480353 \nL 388.740894 279.477032 \nL 391.469083 280.16633 \nL 394.197273 288.430125 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 53.31 288.430125 \nL 53.31 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 410.43 288.430125 \nL 410.43 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 53.31 288.430125 \nL 410.43 288.430125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 53.31 22.318125 \nL 410.43 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- Fig 4.3.b LSTM Mini Batch Model No. of Iterations vs Loss -->\n    <defs>\n     <path d=\"M 9.8125 72.90625 \nL 51.703125 72.90625 \nL 51.703125 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.109375 \nL 48.578125 43.109375 \nL 48.578125 34.8125 \nL 19.671875 34.8125 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-70\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n     <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n     <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n     <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n     <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n     <path d=\"M 19.671875 34.8125 \nL 19.671875 8.109375 \nL 35.5 8.109375 \nQ 43.453125 8.109375 47.28125 11.40625 \nQ 51.125 14.703125 51.125 21.484375 \nQ 51.125 28.328125 47.28125 31.5625 \nQ 43.453125 34.8125 35.5 34.8125 \nz\nM 19.671875 64.796875 \nL 19.671875 42.828125 \nL 34.28125 42.828125 \nQ 41.5 42.828125 45.03125 45.53125 \nQ 48.578125 48.25 48.578125 53.8125 \nQ 48.578125 59.328125 45.03125 62.0625 \nQ 41.5 64.796875 34.28125 64.796875 \nz\nM 9.8125 72.90625 \nL 35.015625 72.90625 \nQ 46.296875 72.90625 52.390625 68.21875 \nQ 58.5 63.53125 58.5 54.890625 \nQ 58.5 48.1875 55.375 44.234375 \nQ 52.25 40.28125 46.1875 39.3125 \nQ 53.46875 37.75 57.5 32.78125 \nQ 61.53125 27.828125 61.53125 20.40625 \nQ 61.53125 10.640625 54.890625 5.3125 \nQ 48.25 0 35.984375 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-66\"/>\n     <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(61.322812 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"50.269531\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"78.052734\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"141.529297\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"173.316406\" xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"236.939453\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"268.726562\" xlink:href=\"#DejaVuSans-51\"/>\n     <use x=\"332.349609\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"364.136719\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"427.613281\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"459.400391\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"515.113281\" xlink:href=\"#DejaVuSans-83\"/>\n     <use x=\"578.589844\" xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"639.673828\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"725.953125\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"757.740234\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"844.019531\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"871.802734\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"935.181641\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"962.964844\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"994.751953\" xlink:href=\"#DejaVuSans-66\"/>\n     <use x=\"1063.355469\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1124.634766\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1163.84375\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1218.824219\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"1282.203125\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1313.990234\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"1400.269531\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1461.451172\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"1524.927734\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1586.451172\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"1614.234375\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1646.021484\" xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"1720.826172\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1780.257812\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"1812.044922\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1843.832031\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1905.013672\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"1940.21875\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1972.005859\" xlink:href=\"#DejaVuSans-73\"/>\n     <use x=\"2001.498047\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"2040.707031\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"2102.230469\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"2143.34375\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2204.623047\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"2243.832031\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"2271.615234\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"2332.796875\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"2396.175781\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2448.275391\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2480.0625\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"2539.242188\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2591.341797\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2623.128906\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"2677.091797\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"2738.273438\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2790.373047\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 398.33 36.618125 \nL 402.73 36.618125 \nQ 404.93 36.618125 404.93 34.418125 \nL 404.93 30.018125 \nQ 404.93 27.818125 402.73 27.818125 \nL 398.33 27.818125 \nQ 396.13 27.818125 396.13 30.018125 \nL 396.13 34.418125 \nQ 396.13 36.618125 398.33 36.618125 \nz\n\" style=\"fill:#eaeaf2;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p3a72f91066\">\n   <rect height=\"266.112\" width=\"357.12\" x=\"53.31\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "application/pdf": "JVBERi0xLjQKJazcIKu6CjEgMCBvYmoKPDwgL1BhZ2VzIDIgMCBSIC9UeXBlIC9DYXRhbG9nID4+CmVuZG9iago4IDAgb2JqCjw8IC9FeHRHU3RhdGUgNCAwIFIgL0ZvbnQgMyAwIFIgL1BhdHRlcm4gNSAwIFIKL1Byb2NTZXQgWyAvUERGIC9UZXh0IC9JbWFnZUIgL0ltYWdlQyAvSW1hZ2VJIF0gL1NoYWRpbmcgNiAwIFIKL1hPYmplY3QgNyAwIFIgPj4KZW5kb2JqCjEwIDAgb2JqCjw8IC9Bbm5vdHMgWyBdIC9Db250ZW50cyA5IDAgUgovR3JvdXAgPDwgL0NTIC9EZXZpY2VSR0IgL1MgL1RyYW5zcGFyZW5jeSAvVHlwZSAvR3JvdXAgPj4KL01lZGlhQm94IFsgMCAwIDQxNy42NDUgMzMxLjQwMjYyNSBdIC9QYXJlbnQgMiAwIFIgL1Jlc291cmNlcyA4IDAgUgovVHlwZSAvUGFnZSA+PgplbmRvYmoKOSAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDExIDAgUiA+PgpzdHJlYW0KeJy9mUtvXMcRhff3VzSQTbJIs6urn0sJiYkEVgDbBLwwsnAoSiZBibAo2H/fX9Udztyh6MAGhgQxIOewp7pPPU5V35Fws0h4H1K44fVr+CH8l99vg4RzXu+XxLsPS5EeW6n8fbv/W1ViSbnlCpiO3/60LO+Ws1cYuA8pTumt9FTH+OJNmUlmS32ET3aG86MFy/9bvSxVo7JbyXG2avvaOVMsZYvd7jFNM6beV3D32SPMD/1z+MKs1h4lh9xaFH5/ugrfh4/h7FU2dhL+zeuG15HvlrN/XP1yfXn17fnrcHm/8NGRZ9VxdNoDenSO5bvlm/DzwwYpSiU+j/dw+Hz33+X1RTj7SoJIuHi3NCOA2T6kSsg1SjLDF2+Xv6a/hYub8M8L3+A5iEqWqFM19yOmG/ikVEXKarfN1vMx1/z8ZHuNs3aRdkz2AJ+WbBs7uyLzEdny7GSzplhzmum44jbwScnmrDu7ucsjsu35yY4a0yhtlmOyB/i0ZPvY2S1ZH5Edz05WdSLog58jshv4pGTRpihml6PXR2QlPT/bWWKWnLsesz3Ap2U7NHazaz/6iO1Boh4+k73Qx4itF183Y/e1fwl378K/Pl99+vHz9d3H++f20oNNLPRcaLib3rrHTpD4kw6ISYlz5j5TGZg1uvXZ82BncxD20XTMDcMDdgodc4YjWdx7Eq2pv1Sq72ySfUxmSTcM99DJCIqMmHNqWBbdMXyxIEqjCUnJnGjD8QCejmVtUbVUul5vbTdmvBjLiSzkwSy1ZXkAT8cSLSwoNB1A547li8Uya4u95EQfPLDcgCdjSaeNnF1hmnaCrC8Wy9xLHLXOsq3KDXg6lj3RSzh8ba3uWNan207C9N8Fu9J2VYWJgbEPfgu0j359d3+K1hPrgQOXvDxH0pml9mCO8OteVm6bsT1c+Er49jwce3JpM9bau98pWkyzZ7U+1TPqXbk4wr3H0mVKwZ8M5UnwdONwGhuoWn9nCYmVSfPM0VtK1VDkmmOUgf8qSplHLw5rpN5aN5jraBu9dYOZ93MbrYRcGB3SrNWao3XxWlOl6xcKiiptZmRyFRqai8G4D4Ld4YLHpNj4B5xzLtXSYHJHpAgzMPOY9tmn1cCEe+OCx/1OOTYji5gRSZDHYWnFU6nTl0uCfS4ygbkxpa5zOAz7am7LylGEJLE9xfJxpNpsucSM30pznIOl0Ss4nbumMdKK4wBlesVOpg1Uv9MajgdaFbOfTTiFJDH8cBu0Mb9zuiaO4wPSgIsT/GMfFneD8UGZ3ULH8slstpq3M/Q+JuFgJ8GeWjj4A4otWfRIZfpA9VGOgMekhSoDL2aeu4XjeKHl0na41YrjBTfM1CyAkom8Vs8Csfgwo1kEE4N7ZiunVXADaWMhBNcyR3fnFxvnarYYkqiDlJ02WkCUJqzFgkjcMG8eNBw34Ct0mOmPA6yxovWQ1h3fJ2JCHKv7rOKEYWnB8kbwk6w+pnixIc1whI6Ir4dpOKEUJUNN5jtNurqTG06AejGckqQ+kuPoR03JPCvkuZoIr7gl6WBqA29k4BTf9nD3FaqF5XkNLTecNiphAbcMrL24DwY+EFyF3HTyG8rVj8kiqrWRANIt/EpNO84bHNINtxRpc/XlwA3ICPnrOGVbHYe72kMpw4VSGmu1ycQNtXDfCsYctdE1lSduGCiE4czqlcK2XKDLM0ykQZsV5CMPlayO80Zxuvi8wdAovm+m4PyRhNRB2JhtVxQvUBzEBLFJioscFpzAESyG5ARFMtpwHCdU+BtuA+RoLTtuWYSniBXyRL5qL45zY0tDCjEh51C+5jG3YlJtajFEoMj7vsNxQmOaISjkOoLXk++bcQKRtSCi8IL8TQvi5q5vtYTgaV1xnEClWBBNXsZOL6AUJ1tZEKlJdtKx4hwO71sQ0Rf6UXaJRqAiQ0ezIHIEKnLVI5NPUnlYEDmyDh8xDccPMyULoj0uMKVZ1+MHoYVYZ0OnK2O3+xlnKWVuQUQwekEZVhw/8M5gGknLLHG4R0YDnEz1j8ppfFMbK9Fbbge2Opc0izvB0sLaDjiySv6tdZtJI/JViSgBJAN7dh80C1C1TiHkUFKi4j4gSynPbjFMRnsMH3psNiCDp8UQsaBL8sbxhiImNDKQuvZkaPiuFFOzLHUYwVSv5jzwQG3VIoiKIF3DpXHzYGOisMrWznXgAcJPANEWYqh+ZUBkYqWVcp+1ckSyV7fz0dwoGA0oyyyQ8xNSanPSQHNg99a6tWaDh5WpuZpd8Lm6gNAPo+UJsYMCeaGeqmq+GMikBCqdBKYTOgx7cZQrJBOdOjitEEvANX2W7GGgXiOZVQkbMHatqgyGOKLNtRNBI1ddrVTgbUPBsIub4j1noiQTSWZzCotpU93Tk10j/lZc1m1yKn1FSaXUqzZbXEhqjwA6Fjulnh3W6WoAbE/PTURrIAEqN2FZYWgzGREvVK3hylQdhjbOIFzIcy9KwTm8f2JDMjKnFFcdRBgOOF4DudgtY92lxZ5FlIGkMK0hyOo3JWiQpAi0w/ZodT0JAkSXpkQDipYkTe/B6CZiyMQKbC2MRPST0KwSwpVDtR6jra6oFV0dbqJRoT4PaYM67YmzVh8eUFOHuYUTWQJGHZIQxZOMGNo2SsCQylzGKqFMH+jmQDmDyTClt9rGa2oDWzeY2qMmHIY6UmgRY15t9GZHuzUaThs4kk1U/gRASQE6ms251UQMaXcbtDf6eBV/IpIZl9aMpCISgyfBZkzABn3XYahT7ESMY08ZXdwIwyDhSlAzNgTUD/LUI6rHN4JswzTuYpK272qefnyzecD11DceWPzyG5MPT39jwto/+IXLYeXGwO9bTRD5U9cbv6wQIRIYOcSg1fBq0O4oX12/DzSh+L/w9XcXb8Kb64/X4fWPny9/Cm/u3l7dhv/cxeNnaeGX+7C92Jy90j/7vdXN5siDO98f/uxCsG1UZFS1aWzvKyS+H4O3gJYWQiFvcUOnL602odhQcICa9V9bdbk8gCawujW5gnQ4hiGEYbMLZWYD9rrLeqANdLk/+ga05LXhBAXRLW6oHG90gPYHulwewM3RbzfgnuJml40vnvDlpX2t93r5ZvkNsvX8DQplbmRzdHJlYW0KZW5kb2JqCjExIDAgb2JqCjIzMjkKZW5kb2JqCjE2IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMjU5ID4+CnN0cmVhbQp4nD1SSXLDMAy7+xV8AndJ70mnJ+f/1wL01JcQY4oACKZ3i8rZ+GkrWX7kxy47SyK3fAdtk/uKqgekboLMJcclV4hZyudKkFi0lJlYn6kOys81qFX4wk/MTEyHJBFLyEk9irB+Xgf3oH69pPNbZPyjY4PSW5JMBZ0a7pWjlNiwVJ8a5B403ZRWn5l2ZQcsjTdkpSZ1+vXT4wdLMKUvEtFBiMSf5HKHLAspB/F2rt5AeFe7INVTKw87RJqPkd5SBas5MwFLRtMOI2Bt1pggVQK8RWsItBbi8oVOKyI+OTd0ME1VnoXIsFLhWHvPaA0Xuoa80nB6knH/+R+8K97X7x/a82ZkCmVuZHN0cmVhbQplbmRvYmoKMTcgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCA3NCA+PgpzdHJlYW0KeJwzNTdVMFCwtAASpobmCuZGlgophlxAPoiVywUTywGzzEzMgCxDS2SWibEhkGViYYbEMjaxgMoiWAZAGmxNDsz0HK40AANxGJMKZW5kc3RyZWFtCmVuZG9iagoxOCAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDQ5ID4+CnN0cmVhbQp4nDOyNFUwULC0ABKGluYK5kaWCimGXEA+iJXLBRPLAbMMgDRYaQ5MRQ5XGgClRAzkCmVuZHN0cmVhbQplbmRvYmoKMTkgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCA1OSA+PgpzdHJlYW0KeJwzNTVXMFCwtAASpqZGCuZGlgophlxAPoiVy2VoaQ5m5YBZFsZABkgZnGEApMGac2B6crjSAKnhEFoKZW5kc3RyZWFtCmVuZG9iagoyMCAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDg3ID4+CnN0cmVhbQp4nD2OuxHAMAhDe6ZgBPMJhn1yqZz924A/adBDOk64CTYMz9Htws6BN0HuRS+wLm+ACiGLJBn59s7FolaZxa+mReqENEmcN1HoTotK5wPj9A54Prp1HXoKZW5kc3RyZWFtCmVuZG9iagoyMSAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDc1ID4+CnN0cmVhbQp4nDWNsRHAMAgDe6ZgBMsBE/bJpcL7twn4aKQXHMjk5sGesnSwTecH9OekTfNCUZCqMNB0Zn1xaOQOVg4XXmhKr4roz0HvB5nqF5oKZW5kc3RyZWFtCmVuZG9iagoyMiAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDMzOCA+PgpzdHJlYW0KeJw1UkuSW0EI279T6AKuav7NeZya1eT+2wjsrKCBFhKQFjjIxEsMUY1yxR95gvE6gb/r5Wn8Pt6F1IKnIv3AtWkb78eaNVGwNGIpzD72/Sghx1Pj3xDouUgTZmQyciAZiPu1Pn/Wm0w5/AakaXP6KEl6EC3Y3Rp2fFmQQdKTGpbs5Id1LbC6CE2YG2siGTm1MjXPx57hMp4YI0HVLCBJn7hPFYxIMx47Zy15kOF4qhcvfr2N1zKPqZdVBTK2CeZgO5kJpygiEL+gJLmJu2jqKI5mxprbhYaSIvfdPZyc9Lq/nEQFXgnhLNYSjhl6yjInOw1KoGrlBJhhvfaFcZo2SrhT0+1dsa/fZyZh3Oaws1IyDc5xcC+bzBEke90xYRMeh5j37hGMxLz5XWwRXLnMuSbTj/0o2kgfFNfnXE2ZrSjhH6rkiRXX+P/83s/PP5A3fbEKZW5kc3RyZWFtCmVuZG9iagoyMyAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDY0ID4+CnN0cmVhbQp4nDMzNFQwUNA1AhJmhiYK5kaWCimGXEA+iJXLBRPLAbPMTMyALGNTUySWAZA2MjWD0xAZoAFwBkR/GgApTxROCmVuZHN0cmVhbQplbmRvYmoKMjQgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAzMDQgPj4Kc3RyZWFtCnicPZI7ksMwDEN7nYIXyIz4k+TzZCeV9/7tPjLJVoBJiQAoL3WZsqY8IGkmCf/R4eFiO+V32J7NzMC1RC8TyynPoSvE3EX5spmNurI6xarDMJ1b9Kici4ZNk5rnKksZtwuew7WJ55Z9xA83NKgHdY1Lwg3d1WhZCs1wdf87vUfZdzU8F5tU6tQXjxdRFeb5IU+ih+lK4nw8KCFcezBGFhLkU9FAjrNcrfJeQvYOtxqywkFqSeezJzzYdXpPLm4XzRAPZLlU+E5R7O3QM77sSgk9ErbhWO59O5qx6RqbOOx+70bWyoyuaCF+yFcn6yVg3FMmRRJkTrZYbovVnu6hKKZzhnMZIOrZioZS5mJXq38MO28sL9ksyJTMCzJGp02eOHjIfo2a9HmV53j9AWzzczsKZW5kc3RyZWFtCmVuZG9iagoyNSAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDIzNyA+PgpzdHJlYW0KeJxFUUlyBCEMu/cr9IGpwivwnk7NqfP/aywzSU4WYGsxaYGBLXiJIdbAzIEvuXxN6DR8NzLb8DrZHnBPuC7cl8uCZ8KWwFdUl3e9L13ZSH13h6p+ZmR7s0jNkJWVOvVCNCbYIRE9IzLJVixzg6QprVLlvihbgC7qlbZOO42SoCMU4W+UI+HpFUp2TWwaq9Q6oKEIy7YuiDqZJKJ2YXFq8ZYhIp91YzXH+ItOInbH4/6sMOtRJJLSZwfdcSajTZZdAzm5eaqwVio5iD5e0caE6nSqgWO817b0E2ngufZf4Qc+ff+PGPq53j/G7lwiCmVuZHN0cmVhbQplbmRvYmoKMjYgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAyMzAgPj4Kc3RyZWFtCnicNVFJbsMwDLzrFfOBAOIuv8dBT+3/rx3SCWBgaEuczREbGxF4icHPQeTGW9aMmvibyV3xuzwVHgm3gidRBF6Ge9kJLm8Yl/04zHzwXlo5kxpPMiAX2fTwRMhgl0DowOwa1GGbaSf6hoTPjkg1G1lOX0vQS6sQKE/ZfqcLSrSt6s/tsy607WtPONntqSeVTyCeW7ICl41XTBZjGfRE5S7F9EGqs4WehPKifA6y+aghEl2inIEnBgejQDuw57afiVeFoHV1n7aNoRopHU//NjQ1SSLkEyWc2dK4W/j+nnv9/AOmVFOfCmVuZHN0cmVhbQplbmRvYmoKMjcgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAyMjcgPj4Kc3RyZWFtCnicNU87sgMhDOs5hS6QGYxtYM+zmVQv92+fZLINEv5I8vRERyZe5sgIrNnxthYZiBn4FlPxrz3tw4TqPbiHCOXiQphhJJw167ibp+PFv13lM9bBuw2+YpYXBLYwk/WVxZnLdsFYGidxTrIbY9dEbGNd6+kU1hFMKAMhne0wJcgcFSl9sqOMOTpO5InnYqrFLr/vYX3BpjGiwhxXBU/QZFCWPe8moB0X9N/Vjd9JNIteAjKRYGGdJObOWU741WtHx1GLIjEnpBnkMhHSnK5iCqEJxTo7CioVBZfqc8rdPv9oXVtNCmVuZHN0cmVhbQplbmRvYmoKMjggMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAyNDUgPj4Kc3RyZWFtCnicRVC7jUMxDOs9BRcIYP0se553SJXbvz1KRnCFIVo/kloSmIjASwyxlG/iR0ZBPQu/F4XiM8TPF4VBzoSkQJz1GRCZeIbaRm7odnDOvMMzjDkCF8VacKbTmfZc2OScBycQzm2U8YxCuklUFXFUn3FM8aqyz43XgaW1bLPTkewhjYRLSSUml35TKv+0KVsq6NpFE7BI5IGTTTThLD9DkmLMoJRR9zC1jvRxspFHddDJ2Zw5LZnZ7qftTHwPWCaZUeUpnecyPiep81xOfe6zHdHkoqVV+5z93pGW8iK126HV6VclUZmN1aeQuDz/jJ/x/gOOoFk+CmVuZHN0cmVhbQplbmRvYmoKMjkgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAzOTIgPj4Kc3RyZWFtCnicPVJLbgUxCNvPKbhApfBNcp6p3u7df1ubzFSqCi8DtjGUlwypJT/qkogzTH71cl3iUfK9bGpn5iHuLjam+FhyX7qG2HLRmmKxTxzJL8i0VFihVt2jQ/GFKBMPAC3ggQXhvhz/8ReowdewhXLDe2QCYErUbkDGQ9EZSFlBEWH7kRXopFCvbOHvKCBX1KyFoXRiiA2WACm+qw2JmKjZoIeElZKqHdLxjKTwW8FdiWFQW1vbBHhm0BDZ3pGNETPt0RlxWRFrPz3po1EytVEZD01nfPHdMlLz0RXopNLI3cpDZ89CJ2Ak5kmY53Aj4Z7bQQsx9HGvlk9s95gpVpHwBTvKAQO9/d6Sjc974CyMXNvsTCfw0WmnHBOtvh5i/YM/bEubXMcrh0UUqLwoCH7XQRNxfFjF92SjRHe0AdYjE9VoJRAMEsLO7TDyeMZ52d4VtOb0RGijRB7UjhE9KLLF5ZwVsKf8rM2xHJ4PJntvtI+UzMyohBXUdnqots9jHdR3nvv6/AEuAKEZCmVuZHN0cmVhbQplbmRvYmoKMzAgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAxMzMgPj4Kc3RyZWFtCnicTY9BEsMwCAPvfoWegLEB8550ekr+fy2QNu4F7YyAkYYwCDxiDOswJbx6++FVpEtwNo75JRlFPAhqC9wXVAVHY4qd+Njdoeyl4ukUTYvrEXPTtKR0N1Eqbb2dyPjAfZ/eH1W2JJ2CHlvqhC7RJPJFAnPYVDDP6sZLS4+n7dneH2Y+M9cKZW5kc3RyZWFtCmVuZG9iagozMSAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDI0NyA+PgpzdHJlYW0KeJxNUbttRDEM698UXOAA62t5ngtSXfZvQ8kIkMIgoS8ppyUW9sZLDOEHWw++5JFVQ38ePzHsMyw9yeTUP+a5yVQUvhWqm5hQF2Lh/WgEvBZ0LyIrygffj2UMc8734KMQl2AmNGCsb0kmF9W8M2TCiaGOw0GbVBh3TRQsrhXNM8jtVjeyOrMgbHglE+LGAEQE2ReQzWCjjLGVkMVyHqgKkgVaYNfpG1GLgiuU1gl0otbEuszgq+f2djdDL/LgqLp4fQzrS7DC6KV7LHyuQh/M9Ew7d0kjvfCmExFmDwVSmZ2RlTo9Yn23QP+fZSv4+8nP8/0LFShcKgplbmRzdHJlYW0KZW5kb2JqCjMyIDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggOTAgPj4Kc3RyZWFtCnicTY1BEsAgCAPvvCJPUETQ/3R60v9fq9QOvcBOAokWRYL0NWpLMO64MhVrUCmYlJfAVTBcC9ruosr+MklMnYbTe7cDg7LxcYPSSfv2cXoAq/16Bt0P0hwiWAplbmRzdHJlYW0KZW5kb2JqCjMzIDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMzM4ID4+CnN0cmVhbQp4nEVSS3LFMAjb5xRcIDPmZ+PzvE5X6f23lXA63Tz0DAgJMj1lSKbcNpZkhOQc8qVXZIjVkJ9GjkTEEN8pocCu8rm8lsRcyG6JSvGhHT+XpTcyza7QqrdHpzaLRjUrI+cgQ4R6VujM7lHbZMPrdiHpOlMWh3As/0MFspR1yimUBG1B39gj6G8WPBHcBrPmcrO5TG71v+5bC57XOluxbQdACZZz3mAGAMTDCdoAxNza3hYpKB9VuopJwq3yXCc7ULbQqnS8N4AZBxg5YMOSrQ7XaG8Awz4P9KJGxfYVoKgsIP7O2WbB3jHJSLAn5gZOPXE6xZFwSTjGAkCKreIUuvEd2OIvF66ImvAJdTplTbzCntrix0KTCO9ScQLwIhtuXR1FtWxP5wm0PyqSM2KkHsTRCZHUks4RFJcG9dAa+7iJGa+NxOaevt0/wjmf6/sXFriD4AplbmRzdHJlYW0KZW5kb2JqCjM0IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMTYzID4+CnN0cmVhbQp4nEWQuXUEMQxDc1WBEniAOuoZP0ez/acLabzeQPp4hHiIPQnDcl3FhdENP962zDS8jjLcjfVlxviosUBO0AcYIhNXo0n17YozVOnh1WKuo6JcLzoiEsyS46tAI3w6ssdDW9uZfjqvf+wh7xP/KirnbmEBLqruQPlSH/HUj9lR6pqhjyorax5q2r8IuyKUtn1cTmWcunsHtMJnK1f7fQOo5zqACmVuZHN0cmVhbQplbmRvYmoKMzUgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCA2OCA+PgpzdHJlYW0KeJwzMrdQMFCwNAEShhYmCuZmBgophlxAvqmJuUIuF0gMxMoBswyAtCWcgohbQjRBlIJYEKVmJmYQSTgDIpcGAMm0FeUKZW5kc3RyZWFtCmVuZG9iagozNiAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDQ1ID4+CnN0cmVhbQp4nDMyt1AwULA0ARKGFiYK5mYGCimGXJYQVi4XTCwHzALRlnAKIp4GAJ99DLUKZW5kc3RyZWFtCmVuZG9iagozNyAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDE2MSA+PgpzdHJlYW0KeJxFkEsSwyAMQ/ecQkfwRwZ8nnS6Su+/rSFNs4CnsUAGdycEqbUFE9EFL21Lugs+WwnOxnjoNm41EuQEdYBWpONolFJ9ucVplXTxaDZzKwutEx1mDnqUoxmgEDoV3u2i5HKm7s75R3D1X/VHse6czcTAZOUOhGb1Ke58mx1RXd1kf9JjbtZrfxX2qrC0rKXlhNvOXTOgBO6pHO39BalzOoQKZW5kc3RyZWFtCmVuZG9iagozOCAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDE0OCA+PgpzdHJlYW0KeJxNkDkWBCEIRHNPwRFYRPA+8yZy7p8OLtid6NeCKtTFAcFsLo3AyOFDRYmgVoRfkdYXjSLswFE4SjU6lHVxFz6zOTrcQEmnKpjk23qUpj0J+1Fn8EPbTzsmKZ40Q30T8sqgQ9UR1iRxWrvyUSSSNnGkT41OLd/usH1gZXDtSXcWEnnRnlnuO+S+LX9jlO8fxYBD5QplbmRzdHJlYW0KZW5kb2JqCjM5IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMjE0ID4+CnN0cmVhbQp4nD1QuxFDMQjrPQUL5M587TfPy6XL/m0knKRCNkISlJpMyZSHOsqSrClPHT5LYoe8h+VuZDYlKkUvk7Al99AK8X2J5hT33dWWs0M0l2g5fgszKqobHdNLNppwKhO6oNzDM/oNbXQDVocesVsg0KRg17YgcscPGAzBmROLIgxKTQb/rXL3UtzvPRxvooiUdPCu+eX0y88tvE49jkS6vfmKa3GmOgpEcEZq8op0YcWyyEOk1QQ1PQNrtQCu3nr5N2hHdBmA7BOJ4zSlHEP/1rjH6wOHilL0CmVuZHN0cmVhbQplbmRvYmoKNDAgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCA4MCA+PgpzdHJlYW0KeJxFjLsNwDAIRHumYAR+JmafKJWzfxsgStxwT7p7uDoSMlPeYYaHBJ4MLIZT8QaZo2A1uEZSjZ3so7BuX3WB5npTq/X3BypPdnZxPc3LGfQKZW5kc3RyZWFtCmVuZG9iago0MSAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDQ5ID4+CnN0cmVhbQp4nDM2tFAwUDA0MAeSRoZAlpGJQoohF0gAxMzlggnmgFkGQBqiOAeuJocrDQDG6A0mCmVuZHN0cmVhbQplbmRvYmoKNDIgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAxNTcgPj4Kc3RyZWFtCnicRZC5EUMxCERzVUEJErAI6rHH0Xf/qRf5SrRvAC2HryVTqh8nIqbc12j0MHkOn00lVizYJraTGnIbFkFKMZh4TjGro7ehmYfU67ioqrh1ZpXTacvKxX/zaFczkz3CNeon8E3o+J88tKnoW6CvC5R9QLU4nUlQMX2vYoGjnHZ/IpwY4D4ZR5kpI3Fibgrs9xkAZr5XuMbjBd0BN3kKZW5kc3RyZWFtCmVuZG9iago0MyAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDMzMiA+PgpzdHJlYW0KeJwtUjmOJDEMy/0KfmAA6/Lxnh5M1Pv/dElVBQWqbMs85HLDRCV+LJDbUWvi10ZmoMLwr6vMhe9I28g6iGvIRVzJlsJnRCzkMcQ8xILv2/gZHvmszMmzB8Yv2fcZVuypCctCxosztMMqjsMqyLFg6yKqe3hTpMOpJNjji/8+xXMXgha+I2jAL/nnqyN4vqRF2j1m27RbD5ZpR5UUloPtac7L5EvrLFfH4/kg2d4VO0JqV4CiMHfGeS6OMm1lRGthZ4OkxsX25tiPpQRd6MZlpDgC+ZkqwgNKmsxsoiD+yOkhpzIQpq7pSie3URV36slcs7m8nUkyW/dFis0UzuvCmfV3mDKrzTt5lhOlTkX4GXu2BA2d4+rZa5mFRrc5wSslfDZ2enLyvZpZD8mpSEgV07oKTqPIFEvYlviaiprS1Mvw35f3GX//ATPifAEKZW5kc3RyZWFtCmVuZG9iago0NCAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDMxNyA+PgpzdHJlYW0KeJw1UktyQzEI279TcIHOmL99nnSyau6/rYQnK7AtQEIuL1nSS37UJdulw+RXH/clsUI+j+2azFLF9xazFM8tr0fPEbctCgRREz34MicVItTP1Og6eGGXPgOvEE4pFngHkwAGr+FfeJROg8A7GzLeEZORGhAkwZpLi01IlD1J/Cvl9aSVNHR+Jitz+XtyqRRqo8kIFSBYudgHpCspHiQTPYlIsnK9N1aI3pBXksdnJSYZEN0msU20wOPclbSEmZhCBeZYgNV0s7r6HExY47CE8SphFtWDTZ41qYRmtI5jZMN498JMiYWGwxJQm32VCaqXj9PcCSOmR0127cKyWzbvIUSj+TMslMHHKCQBh05jJArSsIARgTm9sIq95gs5FsCIZZ2aLAxtaCW7eo6FwNCcs6Vhxtee1/P+B0Vbe6MKZW5kc3RyZWFtCmVuZG9iago0NSAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDE3ID4+CnN0cmVhbQp4nDM2tFAwgMMUQy4AGpQC7AplbmRzdHJlYW0KZW5kb2JqCjQ2IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMTMxID4+CnN0cmVhbQp4nEWPyw0EIQxD71ThEvIZPqmH1Z7Y/q/rMJpBQvhBIjvxMAis8/I20MXw0aLDN/421atjlSwfunpSVg/pkIe88hVQaTBRxIVZTB1DYc6YysiWMrcb4bZNg6xslVStg3Y8Bg+2p2WrCH6pbWHqLPEMwlVeuMcNP5BLrXe9Vb5/QlMwlwplbmRzdHJlYW0KZW5kb2JqCjQ3IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMzM4ID4+CnN0cmVhbQp4nDVSOa7dQAzrfQpdIIB2zZznBal+7t+GlF8KQ7RWipqOFpVp+WUhVS2TLr/tSW2JG/L3yQqJE5JXJdqlDJFQ+TyFVL9ny7y+1pwRIEuVCpOTksclC/4Ml94uHOdjaz+PI3c9emBVjIQSAcsUE6NrWTq7w5qN/DymAT/iEXKuWLccYxVIDbpx2hXvQ/N5yBogZpiWigpdVokWfkHxoEetffdYVFgg0e0cSXCMjVCRgHaB2kgMObMWu6gv+lmUmAl07Ysi7qLAEknMnGJdOvoPPnQsqL8248uvjkr6SCtrTNp3o0lpzCKTrpdFbzdvfT24QPMuyn9ezSBBU9YoaXzQqp1jKJoZZYV3HJoMNMcch8wTPIczEpT0fSh+X0smuiiRPw4NoX9fHqOMnAZvAXPRn7aKAxfx2WGvHGCF0sWa5H1AKhN6YPr/1/h5/vwDHLaAVAplbmRzdHJlYW0KZW5kb2JqCjQ4IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMjQ4ID4+CnN0cmVhbQp4nC1ROZIDQQjL5xV6QnPT77HLkff/6QrKAYOGQyA6LXFQxk8Qlive8shVtOHvmRjBd8Gh38p1GxY5EBVI0hhUTahdvB69B3YcZgLzpDUsgxnrAz9jCjd6cXhMxtntdRk1BHvXa09mUDIrF3HJxAVTddjImcNPpowL7VzPDci5EdZlGKSblcaMhCNNIVJIoeomqTNBkASjq1GjjRzFfunLI51hVSNqDPtcS9vXcxPOGjQ7Fqs8OaVHV5zLycULKwf9vM3ARVQaqzwQEnC/20P9nOzkN97SubPF9Phec7K8MBVY8ea1G5BNtfg3L+L4PePr+fwDqKVbFgplbmRzdHJlYW0KZW5kb2JqCjQ5IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggNzIgPj4Kc3RyZWFtCnicNYyxEcAwCAN7ptAINlhg75NLRfZvQ3xOAy8dD5eiwVoNuoIjcHWp/NEjXbkpRZdjzoLhcapfSDFGPagj497HT7lfcBYSfQplbmRzdHJlYW0KZW5kb2JqCjUwIDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMjEwID4+CnN0cmVhbQp4nDVQyw1DMQi7ZwoWqBQCgWSeVr11/2tt0DthEf9CWMiUCHmpyc4p6Us+OkwPti6/sSILrXUl7MqaIJ4r76GZsrHR2OJgcBomXoAWN2DoaY0aNXThgqYulUKBxSXwmXx1e+i+Txl4ahlydgQRQ8lgCWq6Fk1YtDyfkE4B4v9+w+4t5KGS88qeG/kbnO3wO7Nu4SdqdiLRchUy1LM0xxgIE0UePHlFpnDis9Z31TQS1GYLTpYBrk4/jA4AYCJeWYDsrkQ5S9KOpZ9vvMf3D0AAU7QKZW5kc3RyZWFtCmVuZG9iagoxNCAwIG9iago8PCAvQmFzZUZvbnQgL0RlamFWdVNhbnMgL0NoYXJQcm9jcyAxNSAwIFIKL0VuY29kaW5nIDw8Ci9EaWZmZXJlbmNlcyBbIDMyIC9zcGFjZSAzNSAvbnVtYmVyc2lnbiA0NiAvcGVyaW9kIDQ4IC96ZXJvIC9vbmUgL3R3byAvdGhyZWUgL2ZvdXIKL2ZpdmUgL3NpeCA1NiAvZWlnaHQgNjYgL0IgNzAgL0YgNzMgL0kgNzYgL0wgL00gL04gODMgL1MgL1QgOTcgL2EgL2IgL2MgL2QKL2UgL2YgL2cgL2ggL2kgMTA4IC9sIDExMCAvbiAvbyAxMTQgL3IgL3MgL3QgMTE4IC92IF0KL1R5cGUgL0VuY29kaW5nID4+Ci9GaXJzdENoYXIgMCAvRm9udEJCb3ggWyAtMTAyMSAtNDYzIDE3OTQgMTIzMyBdIC9Gb250RGVzY3JpcHRvciAxMyAwIFIKL0ZvbnRNYXRyaXggWyAwLjAwMSAwIDAgMC4wMDEgMCAwIF0gL0xhc3RDaGFyIDI1NSAvTmFtZSAvRGVqYVZ1U2FucwovU3VidHlwZSAvVHlwZTMgL1R5cGUgL0ZvbnQgL1dpZHRocyAxMiAwIFIgPj4KZW5kb2JqCjEzIDAgb2JqCjw8IC9Bc2NlbnQgOTI5IC9DYXBIZWlnaHQgMCAvRGVzY2VudCAtMjM2IC9GbGFncyAzMgovRm9udEJCb3ggWyAtMTAyMSAtNDYzIDE3OTQgMTIzMyBdIC9Gb250TmFtZSAvRGVqYVZ1U2FucyAvSXRhbGljQW5nbGUgMAovTWF4V2lkdGggMTM0MiAvU3RlbVYgMCAvVHlwZSAvRm9udERlc2NyaXB0b3IgL1hIZWlnaHQgMCA+PgplbmRvYmoKMTIgMCBvYmoKWyA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMAo2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDMxOCA0MDEgNDYwIDgzOCA2MzYKOTUwIDc4MCAyNzUgMzkwIDM5MCA1MDAgODM4IDMxOCAzNjEgMzE4IDMzNyA2MzYgNjM2IDYzNiA2MzYgNjM2IDYzNiA2MzYgNjM2CjYzNiA2MzYgMzM3IDMzNyA4MzggODM4IDgzOCA1MzEgMTAwMCA2ODQgNjg2IDY5OCA3NzAgNjMyIDU3NSA3NzUgNzUyIDI5NQoyOTUgNjU2IDU1NyA4NjMgNzQ4IDc4NyA2MDMgNzg3IDY5NSA2MzUgNjExIDczMiA2ODQgOTg5IDY4NSA2MTEgNjg1IDM5MCAzMzcKMzkwIDgzOCA1MDAgNTAwIDYxMyA2MzUgNTUwIDYzNSA2MTUgMzUyIDYzNSA2MzQgMjc4IDI3OCA1NzkgMjc4IDk3NCA2MzQgNjEyCjYzNSA2MzUgNDExIDUyMSAzOTIgNjM0IDU5MiA4MTggNTkyIDU5MiA1MjUgNjM2IDMzNyA2MzYgODM4IDYwMCA2MzYgNjAwIDMxOAozNTIgNTE4IDEwMDAgNTAwIDUwMCA1MDAgMTM0MiA2MzUgNDAwIDEwNzAgNjAwIDY4NSA2MDAgNjAwIDMxOCAzMTggNTE4IDUxOAo1OTAgNTAwIDEwMDAgNTAwIDEwMDAgNTIxIDQwMCAxMDIzIDYwMCA1MjUgNjExIDMxOCA0MDEgNjM2IDYzNiA2MzYgNjM2IDMzNwo1MDAgNTAwIDEwMDAgNDcxIDYxMiA4MzggMzYxIDEwMDAgNTAwIDUwMCA4MzggNDAxIDQwMSA1MDAgNjM2IDYzNiAzMTggNTAwCjQwMSA0NzEgNjEyIDk2OSA5NjkgOTY5IDUzMSA2ODQgNjg0IDY4NCA2ODQgNjg0IDY4NCA5NzQgNjk4IDYzMiA2MzIgNjMyIDYzMgoyOTUgMjk1IDI5NSAyOTUgNzc1IDc0OCA3ODcgNzg3IDc4NyA3ODcgNzg3IDgzOCA3ODcgNzMyIDczMiA3MzIgNzMyIDYxMSA2MDUKNjMwIDYxMyA2MTMgNjEzIDYxMyA2MTMgNjEzIDk4MiA1NTAgNjE1IDYxNSA2MTUgNjE1IDI3OCAyNzggMjc4IDI3OCA2MTIgNjM0CjYxMiA2MTIgNjEyIDYxMiA2MTIgODM4IDYxMiA2MzQgNjM0IDYzNCA2MzQgNTkyIDYzNSA1OTIgXQplbmRvYmoKMTUgMCBvYmoKPDwgL0IgMTYgMCBSIC9GIDE3IDAgUiAvSSAxOCAwIFIgL0wgMTkgMCBSIC9NIDIwIDAgUiAvTiAyMSAwIFIgL1MgMjIgMCBSCi9UIDIzIDAgUiAvYSAyNCAwIFIgL2IgMjUgMCBSIC9jIDI2IDAgUiAvZCAyNyAwIFIgL2UgMjggMCBSIC9laWdodCAyOSAwIFIKL2YgMzAgMCBSIC9maXZlIDMxIDAgUiAvZm91ciAzMiAwIFIgL2cgMzMgMCBSIC9oIDM0IDAgUiAvaSAzNSAwIFIgL2wgMzYgMCBSCi9uIDM3IDAgUiAvbnVtYmVyc2lnbiAzOCAwIFIgL28gMzkgMCBSIC9vbmUgNDAgMCBSIC9wZXJpb2QgNDEgMCBSIC9yIDQyIDAgUgovcyA0MyAwIFIgL3NpeCA0NCAwIFIgL3NwYWNlIDQ1IDAgUiAvdCA0NiAwIFIgL3RocmVlIDQ3IDAgUiAvdHdvIDQ4IDAgUgovdiA0OSAwIFIgL3plcm8gNTAgMCBSID4+CmVuZG9iagozIDAgb2JqCjw8IC9GMSAxNCAwIFIgPj4KZW5kb2JqCjQgMCBvYmoKPDwgL0ExIDw8IC9DQSAwIC9UeXBlIC9FeHRHU3RhdGUgL2NhIDEgPj4KL0EyIDw8IC9DQSAxIC9UeXBlIC9FeHRHU3RhdGUgL2NhIDEgPj4KL0EzIDw8IC9DQSAwLjggL1R5cGUgL0V4dEdTdGF0ZSAvY2EgMC44ID4+ID4+CmVuZG9iago1IDAgb2JqCjw8ID4+CmVuZG9iago2IDAgb2JqCjw8ID4+CmVuZG9iago3IDAgb2JqCjw8ID4+CmVuZG9iagoyIDAgb2JqCjw8IC9Db3VudCAxIC9LaWRzIFsgMTAgMCBSIF0gL1R5cGUgL1BhZ2VzID4+CmVuZG9iago1MSAwIG9iago8PCAvQ3JlYXRpb25EYXRlIChEOjIwMjIxMjAzMTQyODM1WikKL0NyZWF0b3IgKG1hdHBsb3RsaWIgMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZykKL1Byb2R1Y2VyIChtYXRwbG90bGliIHBkZiBiYWNrZW5kIDMuMi4yKSA+PgplbmRvYmoKeHJlZgowIDUyCjAwMDAwMDAwMDAgNjU1MzUgZiAKMDAwMDAwMDAxNiAwMDAwMCBuIAowMDAwMDEzOTUyIDAwMDAwIG4gCjAwMDAwMTM3MTUgMDAwMDAgbiAKMDAwMDAxMzc0NyAwMDAwMCBuIAowMDAwMDEzODg5IDAwMDAwIG4gCjAwMDAwMTM5MTAgMDAwMDAgbiAKMDAwMDAxMzkzMSAwMDAwMCBuIAowMDAwMDAwMDY1IDAwMDAwIG4gCjAwMDAwMDAzOTYgMDAwMDAgbiAKMDAwMDAwMDIwOCAwMDAwMCBuIAowMDAwMDAyODAwIDAwMDAwIG4gCjAwMDAwMTIyNDkgMDAwMDAgbiAKMDAwMDAxMjA0OSAwMDAwMCBuIAowMDAwMDExNTYxIDAwMDAwIG4gCjAwMDAwMTMzMDIgMDAwMDAgbiAKMDAwMDAwMjgyMSAwMDAwMCBuIAowMDAwMDAzMTUzIDAwMDAwIG4gCjAwMDAwMDMyOTkgMDAwMDAgbiAKMDAwMDAwMzQyMCAwMDAwMCBuIAowMDAwMDAzNTUxIDAwMDAwIG4gCjAwMDAwMDM3MTAgMDAwMDAgbiAKMDAwMDAwMzg1NyAwMDAwMCBuIAowMDAwMDA0MjY4IDAwMDAwIG4gCjAwMDAwMDQ0MDQgMDAwMDAgbiAKMDAwMDAwNDc4MSAwMDAwMCBuIAowMDAwMDA1MDkxIDAwMDAwIG4gCjAwMDAwMDUzOTQgMDAwMDAgbiAKMDAwMDAwNTY5NCAwMDAwMCBuIAowMDAwMDA2MDEyIDAwMDAwIG4gCjAwMDAwMDY0NzcgMDAwMDAgbiAKMDAwMDAwNjY4MyAwMDAwMCBuIAowMDAwMDA3MDAzIDAwMDAwIG4gCjAwMDAwMDcxNjUgMDAwMDAgbiAKMDAwMDAwNzU3NiAwMDAwMCBuIAowMDAwMDA3ODEyIDAwMDAwIG4gCjAwMDAwMDc5NTIgMDAwMDAgbiAKMDAwMDAwODA2OSAwMDAwMCBuIAowMDAwMDA4MzAzIDAwMDAwIG4gCjAwMDAwMDg1MjQgMDAwMDAgbiAKMDAwMDAwODgxMSAwMDAwMCBuIAowMDAwMDA4OTYzIDAwMDAwIG4gCjAwMDAwMDkwODQgMDAwMDAgbiAKMDAwMDAwOTMxNCAwMDAwMCBuIAowMDAwMDA5NzE5IDAwMDAwIG4gCjAwMDAwMTAxMDkgMDAwMDAgbiAKMDAwMDAxMDE5OCAwMDAwMCBuIAowMDAwMDEwNDAyIDAwMDAwIG4gCjAwMDAwMTA4MTMgMDAwMDAgbiAKMDAwMDAxMTEzNCAwMDAwMCBuIAowMDAwMDExMjc4IDAwMDAwIG4gCjAwMDAwMTQwMTIgMDAwMDAgbiAKdHJhaWxlcgo8PCAvSW5mbyA1MSAwIFIgL1Jvb3QgMSAwIFIgL1NpemUgNTIgPj4Kc3RhcnR4cmVmCjE0MTYwCiUlRU9GCg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7WjcxXntMi5"
      },
      "source": [
        "# Tree-LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyj_UD6GtO5M"
      },
      "source": [
        "In the final part of this lab we will exploit the tree structure of the SST data. \n",
        "Until now we only used the surface tokens, but remember that our data examples include binary trees with a sentiment score at every node.\n",
        "\n",
        "In particular, we will implement **N-ary Tree-LSTMs** which are described in:\n",
        "\n",
        "> Kai Sheng Tai, Richard Socher, and Christopher D. Manning. [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://aclweb.org/anthology/P/P15/P15-1150.pdf) ACL 2015.\n",
        "\n",
        "Since our trees are binary (i.e., N=2), we can refer to these as *Binary Tree-LSTMs*. If you study equations (9) to (14) in the paper, you will find that they are not all too different from the original LSTM that you already have.\n",
        "\n",
        "You should read this paper carefully and make sure that you understand the approach. You will also find our LSTM baseline there.\n",
        "Note however that Tree-LSTMs were proposed around the same time by two other groups:\n",
        "\n",
        "> Phong Le and Willem Zuidema. [Compositional distributional semantics with long short term memory](http://anthology.aclweb.org/S/S15/S15-1002.pdf). *SEM 2015.\n",
        "\n",
        "> Xiaodan Zhu, Parinaz Sobihani,  and Hongyu Guo. [Long short-term memory over recursive structures](http://proceedings.mlr.press/v37/zhub15.pdf). ICML 2015.\n",
        "\n",
        "It is good scientific practice to cite all three papers in your report.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rDzvSos3JFp"
      },
      "source": [
        "## Computation\n",
        "\n",
        "Do you remember the `transitions_from_treestring` function all the way in the beginning of this lab? Every example contains a **transition sequence** produced by this function. Let's look at it again:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pg0Xumc3ZUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef09b389-d609-4623-a904-1c3c8e8e7fc4"
      },
      "source": [
        "ex = next(examplereader(\"trees/dev.txt\"))\n",
        "print(TreePrettyPrinter(ex.tree))\n",
        "print(\"Transitions:\")\n",
        "print(ex.transitions)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              3                                                                     \n",
            "  ____________|____________________                                                  \n",
            " |                                 4                                                \n",
            " |        _________________________|______________________________________________   \n",
            " |       4                                                                        | \n",
            " |    ___|______________                                                          |  \n",
            " |   |                  4                                                         | \n",
            " |   |         _________|__________                                               |  \n",
            " |   |        |                    3                                              | \n",
            " |   |        |               _____|______________________                        |  \n",
            " |   |        |              |                            4                       | \n",
            " |   |        |              |            ________________|_______                |  \n",
            " |   |        |              |           |                        2               | \n",
            " |   |        |              |           |                 _______|___            |  \n",
            " |   |        3              |           |                |           2           | \n",
            " |   |    ____|_____         |           |                |        ___|_____      |  \n",
            " |   |   |          4        |           3                |       2         |     | \n",
            " |   |   |     _____|___     |      _____|_______         |    ___|___      |     |  \n",
            " 2   2   2    3         2    2     3             2        2   2       2     2     2 \n",
            " |   |   |    |         |    |     |             |        |   |       |     |     |  \n",
            " It  's  a  lovely     film with lovely     performances  by Buy     and Accorsi  . \n",
            "\n",
            "Transitions:\n",
            "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-0d63b35e49bd>:2: DeprecationWarning: \n",
            "    Class TreePrettyPrinter has been deprecated.  Import\n",
            "    `TreePrettyPrinter` using `from nltk.tree import\n",
            "    TreePrettyPrinter` instead.\n",
            "  print(TreePrettyPrinter(ex.tree))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceBFe9fU4BI_"
      },
      "source": [
        "Note that the tree is **binary**. Every node has two children, except for pre-terminal nodes.\n",
        "\n",
        "A tree like this can be described by a sequence of **SHIFT (0)** and **REDUCE (1)** actions.\n",
        "\n",
        "To construct a tree, we can use the transitions as follows:\n",
        "- **reverse** the sentence (a list of tokens) and call this the **buffer**\n",
        "   - the first word is now on top (last in the list), and we would get it when calling pop() on the buffer\n",
        "- create an empty list and call it the **stack**\n",
        "- iterate through the transition sequence:\n",
        "  - if it says SHIFT(0), we pop a word from the buffer, and push it to the stack\n",
        "  - if it says REDUCE(1), we pop the **top two items** from the stack, and combine them (e.g. with a Tree-LSTM!), creating a new node that we push back on the stack\n",
        "  \n",
        "Convince yourself that going through the transition sequence above will result in the tree that you see.\n",
        "For example, we would start by putting the following words on the stack (by shifting 5 times, starting with `It`):\n",
        "\n",
        "```\n",
        "Top of the stack:\n",
        "-----------------\n",
        "film\n",
        "lovely\n",
        "a \n",
        "'s  \n",
        "It\n",
        "```\n",
        "Now we find a REDUCE in the transition sequence, so we get the top two words (film and lovely), and combine them, so our new stack becomes:\n",
        "```\n",
        "Top of the stack:\n",
        "-----------------\n",
        "lovely film\n",
        "a \n",
        "'s  \n",
        "It\n",
        "```\n",
        "\n",
        "We will use this approach when encoding sentences with our Tree-LSTM.\n",
        "Now, our sentence is a reversed list of word embeddings.\n",
        "When we shift, we move a word embedding to the stack.\n",
        "When we reduce, we apply the Tree-LSTM to the top two vectors, and the result is a single vector that we put back on the stack.\n",
        "After going through the whole transition sequence, we will have the root node on our stack! We can use that to classify the sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDWKShm1AfmR"
      },
      "source": [
        "## Obtaining the transition sequence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO7VKWVpAbWj"
      },
      "source": [
        "\n",
        "So what goes on in the `transitions_from_treestring` function?\n",
        "\n",
        "The idea ([explained in this blog post](https://devblogs.nvidia.com/recursive-neural-networks-pytorch/)) is that, if we had a tree, we could traverse through the tree, and every time that we find a node containing only a word, we output a SHIFT.\n",
        "Every time **after** we have finished visiting the children of a node, we output a REDUCE.\n",
        "(What is this tree traversal called?)\n",
        "\n",
        "However, our `transitions_from_treestring` function operates directly on the string representation. It works as follows.\n",
        "\n",
        "We start with the representation:\n",
        "\n",
        "```\n",
        "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\n",
        "```\n",
        "\n",
        "First we remove pre-terminal nodes (and add spaces before closing brackets):\n",
        "\n",
        "```\n",
        "(3 It (4 (4 's (4 (3 a (4 lovely film ) ) (3 with (4 (3 lovely performances ) (2 by (2 (2 Buy and )  Accorsi ) ) ) ) ) ) . ) )\n",
        "```\n",
        "\n",
        "Then we remove node labels:\n",
        "\n",
        "```\n",
        "( It ( ( 's ( ( a ( lovely film ) ) ( with ( ( lovely performances) ( by ( ( Buy and )  Accorsi ) ) ) ) ) ) . ) )\n",
        "```\n",
        "\n",
        "Then we remove opening brackets:\n",
        "\n",
        "```\n",
        "It 's a lovely film ) ) with lovely performances ) by Buy and ) Accorsi ) ) ) ) ) ) . ) )\n",
        "```\n",
        "\n",
        "Now we replace words by S (for SHIFT), and closing brackets by R (for REDUCE):\n",
        "\n",
        "```\n",
        "S S S S S R R S S S R S S S R S R R R R R R S R R\n",
        "0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 \n",
        "```\n",
        "\n",
        "Et voilà. We just obtained the transition sequence!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y069gM4_v64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab90252-238e-4d88-b615-9d4112395627"
      },
      "source": [
        "# for comparison\n",
        "seq = ex.transitions\n",
        "s = \" \".join([\"S\" if t == 0 else \"R\" for t in seq])\n",
        "print(s)\n",
        "print(\" \".join(map(str, seq)))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S S S S S R R S S S R S S S R S R R R R R R S R R\n",
            "0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-qOuKbDAiBn"
      },
      "source": [
        "## Coding the Tree-LSTM\n",
        "\n",
        "The code below contains a Binary Tree-LSTM cell.\n",
        "It is used in the TreeLSTM class below it, which in turn is used in the TreeLSTMClassifier.\n",
        "The job of the TreeLSTM class is to encode a complete sentence and return the root node.\n",
        "The job of the TreeLSTMCell is to return a new state when provided with two children (a reduce action). By repeatedly calling the TreeLSTMCell, the TreeLSTM will encode a sentence. This can be done for multiple sentences at the same time.\n",
        "\n",
        "\n",
        "#### Exercise \n",
        "Check the `forward` function and complete the Tree-LSTM formulas.\n",
        "You can see that we defined a large linear layer for you, that projects the *concatenation* of the left and right child into the input gate, left forget gate, right forget gate, candidate, and output gate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9b9mjMlN7Pb"
      },
      "source": [
        "class TreeLSTMCell(nn.Module):\n",
        "  \"\"\"A Binary Tree LSTM cell\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, bias=True):\n",
        "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
        "    super(TreeLSTMCell, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bias = bias\n",
        "\n",
        "    self.reduce_layer = nn.Linear(2 * hidden_size, 5 * hidden_size)\n",
        "    self.dropout_layer = nn.Dropout(p=0.25)\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    \"\"\"This is PyTorch's default initialization method\"\"\"\n",
        "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "    for weight in self.parameters():\n",
        "      weight.data.uniform_(-stdv, stdv)  \n",
        "\n",
        "  def forward(self, hx_l, hx_r, mask=None):\n",
        "    \"\"\"\n",
        "    hx_l is ((batch, hidden_size), (batch, hidden_size))\n",
        "    hx_r is ((batch, hidden_size), (batch, hidden_size))    \n",
        "    \"\"\"\n",
        "   \n",
        "    prev_h_l, prev_c_l = hx_l  # left child\n",
        "    prev_h_r, prev_c_r = hx_r  # right child\n",
        "\n",
        "    B = prev_h_l.size(0)\n",
        "\n",
        "    # we concatenate the left and right children\n",
        "    # you can also project from them separately and then sum\n",
        "    children = torch.cat([prev_h_l, prev_h_r], dim=1)\n",
        "    \n",
        "    # project the combined children into a 5D tensor for i,fl,fr,g,o\n",
        "    # this is done for speed, and you could also do it separately\n",
        "    proj = self.reduce_layer(children)  # shape: B x 5D\n",
        "\n",
        "    # each shape: B x D\n",
        "    i, f_l, f_r, g, o = torch.chunk(proj, 5, dim=-1)\n",
        "    \n",
        "    # main Tree LSTM computation\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    # You only need to complete the commented lines below.\n",
        "    # The shape of each of these is [batch_size, hidden_size]\n",
        "\n",
        "    i = torch.sigmoid(i)\n",
        "    f_l = torch.sigmoid(f_l)   \n",
        "    f_r = torch.sigmoid(f_r)\n",
        "    g = torch.tanh(g)  \n",
        "    o = torch.sigmoid(o)\n",
        "    c = f_l*prev_c_l + f_r*prev_c_r + i*g\n",
        "    h = o*torch.tanh(c)\n",
        "    \n",
        "    return h, c\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return \"{}({:d}, {:d})\".format(\n",
        "        self.__class__.__name__, self.input_size, self.hidden_size)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj5dYSGh_643"
      },
      "source": [
        "## Explanation of the TreeLSTM class\n",
        "\n",
        "\n",
        "The code below contains the TreeLSTM class, which implements everything we need in order to encode a sentence from word embeddings. The calculations are the same as in the paper, implemented such that the class `TreeLSTMCell` above is as general as possible and only takes two children to reduce them into a parent. \n",
        "\n",
        "\n",
        "**Initialize $\\mathbf{h}$ and $\\mathbf{c}$ outside of the cell for the leaves**\n",
        "\n",
        "At the leaves of each tree the children nodes are **empty**, whereas in higher levels the nodes are binary tree nodes that *do* have a left and right child (but no input $x$). By initializing the leaf nodes outside of the cell class (`TreeLSTMCell`), we avoid if-else statements in the forward pass.\n",
        "\n",
        "The `TreeLSTM` class (among other things) pre-calculates an initial $h$ and $c$ for every word in the sentence. Since the initial left and right child are 0, the only calculations we need to do are based on $x$, and we can drop the forget gate calculation (`prev_c_l` and `prev_c_r` are zero). The calculations we do in order to initalize $h$ and $c$ are then:\n",
        "\n",
        "$$\n",
        "c_1 =  W^{(u)}x_1 \\\\\n",
        "o_1 = \\sigma (W^{(i)}x_1) \\\\\n",
        "h_1 = o_1 \\odot \\text{tanh}(c_1)$$\n",
        "*NB: note that these equations are chosen as initializations of $c$ and $h$, other initializations are possible and might work equally well.*\n",
        "\n",
        "**Sentence Representations**\n",
        "\n",
        "All our leaf nodes are now initialized, so we can start processing the sentence in its tree form. Each sentence is represented by a buffer (initially a list with a concatenation of $[h_1, c_1]$ for every word in the reversed sentence), a stack (initially an empty list) and a transition sequence. To encode our sentence, we construct the tree from its transition sequence as explained earlier. \n",
        "\n",
        "*A short example that constructs a tree:*\n",
        "\n",
        "We loop over the time dimension of the batched transition sequences (i.e. row by row), which contain values of 0's, 1's and 2's (representing SHIFT, REDUCE and padding respectively). If we have a batch of size 2 where the first example has a transition sequence given by [0, 0, 1, 0, 0, 0, 1] and the second by [0, 0, 1, 0, 0, 1], our transition batch will be given by the following two-dimensional numpy array:\n",
        "\n",
        "$$\n",
        "\\text{transitions} = \n",
        "\\begin{pmatrix}\n",
        "0 & 0\\\\ \n",
        "0 & 0\\\\ \n",
        "1 & 1\\\\ \n",
        "0 & 0\\\\ \n",
        "0 & 0\\\\ \n",
        "0 & 1\\\\ \n",
        "1 & 2\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "The inner loop (`for transition, buffer, stack in zip(t_batch, buffers, stacks)`) goes over each example in the batch and updates its buffer and stack. The nested loop for this example will then do roughy the following:\n",
        "\n",
        "```\n",
        "Time = 0:  t_batch = [0, 0], the inner loop performs 2 SHIFTs. \n",
        "\n",
        "Time = 1:  t_batch = [0, 0], \"..\"\n",
        "\n",
        "Time = 2:  t_batch = [1, 1], causing the inner loop to fill the list child_l and child_r for both examples in the batch. Now the statement if child_l will return True, triggering a REDUCE action to be performed by our Tree LSTM cell with a batch size of 2. \n",
        "\n",
        "Time = 3:  t_batch = [0, 0], \"..\".\n",
        "\n",
        "Time = 4:  t_batch = [0, 0], \"..\"\n",
        "\n",
        "Time = 5:  t_batch = [0, 1], one SHIFT will be done and another REDUCE action will be performed by our Tree LSTM, this time of batch size 1.  \n",
        "\n",
        "Time = 6:  t_batch = [1, 2], triggering another REDUCE action with batch size 1.\n",
        "```\n",
        "*NB: note that this was an artificial example for the purpose of demonstrating parts of the code, the transition sequences do not necessarily represent actual trees.*\n",
        "\n",
        "**Batching and Unbatching**\n",
        "\n",
        "Within the body of the outer loop over time, we use the functions for batching and unbatching. \n",
        "\n",
        "*Batching*\n",
        "\n",
        "Before passing two lists of children to the reduce layer (an instance of `TreeLSTMCell`), we batch the children as they are at this point a list of tensors of variable length based on how many REDUCE actions there are to perform at a certain time step across the batch (let's call the length `L`). To do an efficient forward pass we want to transform the list to a pair of tensors of shape `([L, D], [L, D])`, which the function `batch` achieves. \n",
        "\n",
        "*Unbatching*\n",
        "\n",
        "In the same line where we batched the children, we unbatch the output of the forward pass to become a list of states of length `L` again. We do this because we need to loop over each example's transition at the current time step and push the children that are reduced into a parent to the stack.\n",
        "\n",
        "*The batch and unbatch functions let us switch between the \"PyTorch world\" (Tensors) and the Python world (easy to manipulate lists).*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PixvTd4AqsQ"
      },
      "source": [
        "# Helper functions for batching and unbatching states\n",
        "# For speed we want to combine computations by batching, but \n",
        "# for processing logic we want to turn the output into lists again\n",
        "# to easily manipulate.\n",
        "\n",
        "def batch(states):\n",
        "  \"\"\"\n",
        "  Turns a list of states into a single tensor for fast processing. \n",
        "  This function also chunks (splits) each state into a (h, c) pair\"\"\"\n",
        "  return torch.cat(states, 0).chunk(2, 1)\n",
        "\n",
        "def unbatch(state):\n",
        "  \"\"\"\n",
        "  Turns a tensor back into a list of states.\n",
        "  First, (h, c) are merged into a single state.\n",
        "  Then the result is split into a list of sentences.\n",
        "  \"\"\"\n",
        "  return torch.split(torch.cat(state, 1), 1, 0)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CynltDasaLPt"
      },
      "source": [
        "Take some time to understand the class below, having read the explanation above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQOqMXG4gX5G"
      },
      "source": [
        "class TreeLSTM(nn.Module):\n",
        "  \"\"\"Encodes a sentence using a TreeLSTMCell\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, bias=True):\n",
        "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
        "    super(TreeLSTM, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bias = bias\n",
        "    self.reduce = TreeLSTMCell(input_size, hidden_size)\n",
        "\n",
        "    # project word to initial c\n",
        "    self.proj_x = nn.Linear(input_size, hidden_size)\n",
        "    self.proj_x_gate = nn.Linear(input_size, hidden_size)\n",
        "    \n",
        "    self.buffers_dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "  def forward(self, x, transitions):\n",
        "    \"\"\"\n",
        "    WARNING: assuming x is reversed!\n",
        "    :param x: word embeddings [B, T, E]\n",
        "    :param transitions: [2T-1, B]\n",
        "    :return: root states\n",
        "    \"\"\"\n",
        "\n",
        "    B = x.size(0)  # batch size\n",
        "    T = x.size(1)  # time\n",
        "\n",
        "    # compute an initial c and h for each word\n",
        "    # Note: this corresponds to input x in the Tai et al. Tree LSTM paper.\n",
        "    # We do not handle input x in the TreeLSTMCell itself.\n",
        "    buffers_c = self.proj_x(x)\n",
        "    buffers_h = buffers_c.tanh()\n",
        "    buffers_h_gate = self.proj_x_gate(x).sigmoid()\n",
        "    buffers_h = buffers_h_gate * buffers_h\n",
        "    \n",
        "    # concatenate h and c for each word\n",
        "    buffers = torch.cat([buffers_h, buffers_c], dim=-1)\n",
        "\n",
        "    D = buffers.size(-1) // 2\n",
        "\n",
        "    # we turn buffers into a list of stacks (1 stack for each sentence)\n",
        "    # first we split buffers so that it is a list of sentences (length B)\n",
        "    # then we split each sentence to be a list of word vectors\n",
        "    buffers = buffers.split(1, dim=0)  # Bx[T, 2D]\n",
        "    buffers = [list(b.squeeze(0).split(1, dim=0)) for b in buffers]  # BxTx[2D]\n",
        "\n",
        "    # create B empty stacks\n",
        "    stacks = [[] for _ in buffers]\n",
        "\n",
        "    # t_batch holds 1 transition for each sentence\n",
        "    for t_batch in transitions:\n",
        "\n",
        "      child_l = []  # contains the left child for each sentence with reduce action\n",
        "      child_r = []  # contains the corresponding right child\n",
        "\n",
        "      # iterate over sentences in the batch\n",
        "      # each has a transition t, a buffer and a stack\n",
        "      for transition, buffer, stack in zip(t_batch, buffers, stacks):\n",
        "        if transition == SHIFT:\n",
        "          stack.append(buffer.pop())\n",
        "        elif transition == REDUCE:\n",
        "          assert len(stack) >= 2, \\\n",
        "            \"Stack too small! Should not happen with valid transition sequences\"\n",
        "          child_r.append(stack.pop())  # right child is on top\n",
        "          child_l.append(stack.pop())\n",
        "\n",
        "      # if there are sentences with reduce transition, perform them batched\n",
        "      if child_l:\n",
        "        reduced = iter(unbatch(self.reduce(batch(child_l), batch(child_r))))\n",
        "        for transition, stack in zip(t_batch, stacks):\n",
        "          if transition == REDUCE:\n",
        "            stack.append(next(reduced))\n",
        "\n",
        "    final = [stack.pop().chunk(2, -1)[0] for stack in stacks]\n",
        "    final = torch.cat(final, dim=0)  # tensor [B, D]\n",
        "\n",
        "    return final"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4EzbVzqaXkw"
      },
      "source": [
        "Just like the LSTM before, we will need an extra class that does the classifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLxpYRvtQKge"
      },
      "source": [
        "class TreeLSTMClassifier(nn.Module):\n",
        "  \"\"\"Encodes sentence with a TreeLSTM and projects final hidden state\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
        "    super(TreeLSTMClassifier, self).__init__()\n",
        "    self.vocab = vocab\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
        "    self.treelstm = TreeLSTM(embedding_dim, hidden_dim)\n",
        "    self.output_layer = nn.Sequential(     \n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(hidden_dim, output_dim, bias=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    # x is a pair here of words and transitions; we unpack it here.\n",
        "    # x is batch-major: [B, T], transitions is time major [2T-1, B]\n",
        "    x, transitions = x\n",
        "    emb = self.embed(x)\n",
        "    \n",
        "    # we use the root/top state of the Tree LSTM to classify the sentence\n",
        "    root_states = self.treelstm(emb, transitions)\n",
        "\n",
        "    # we use the last hidden state to classify the sentence\n",
        "    logits = self.output_layer(root_states)\n",
        "    return logits"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6aJi3cvJyt2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh9RbhGwaiLg"
      },
      "source": [
        "## Special `prepare` function for Tree-LSTM\n",
        "\n",
        "We need yet another `prepare` function. For our implementation, sentences need to be *reversed*. We will do that here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiqH-_2xdm9H"
      },
      "source": [
        "def prepare_treelstm_minibatch(mb, vocab):\n",
        "  \"\"\"\n",
        "  Returns sentences reversed (last word first)\n",
        "  Returns transitions together with the sentences.  \n",
        "  \"\"\"\n",
        "  batch_size = len(mb)\n",
        "  maxlen = max([len(ex.tokens) for ex in mb])\n",
        "    \n",
        "  # vocab returns 0 if the word is not there\n",
        "  # NOTE: reversed sequence!\n",
        "  x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen)[::-1] for ex in mb]\n",
        "  \n",
        "  x = torch.LongTensor(x)\n",
        "  x = x.to(device)\n",
        "  \n",
        "  y = [ex.label for ex in mb]\n",
        "  y = torch.LongTensor(y)\n",
        "  y = y.to(device)\n",
        "  \n",
        "  maxlen_t = max([len(ex.transitions) for ex in mb])\n",
        "  transitions = [pad(ex.transitions, maxlen_t, pad_value=2) for ex in mb]\n",
        "  transitions = np.array(transitions)\n",
        "  transitions = transitions.T  # time-major\n",
        "  \n",
        "  return (x, transitions), y"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMUsrlL9ayVe"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpOYUdg2D3v0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b02673-7de9-4ae2-d18e-3a7add1c5904"
      },
      "source": [
        "# Now let's train the Tree LSTM!\n",
        "\n",
        "tree_model = TreeLSTMClassifier(\n",
        "    len(v.w2i), 300, 150, len(t2i), v)\n",
        "\n",
        "with torch.no_grad():\n",
        "  tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "  tree_model.embed.weight.requires_grad = False\n",
        "  \n",
        "def do_train(model):\n",
        "  \n",
        "  print(model)\n",
        "  print_parameters(model)\n",
        "\n",
        "  model = model.to(device)\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "  \n",
        "  return train_model(\n",
        "      model, optimizer, num_iterations=30000, \n",
        "      print_every=250, eval_every=250,\n",
        "      prep_fn=prepare_treelstm_minibatch,\n",
        "      eval_fn=evaluate,\n",
        "      batch_fn=get_minibatch,\n",
        "      batch_size=25, eval_batch_size=25)\n",
        "  \n",
        "results = do_train(tree_model)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TreeLSTMClassifier(\n",
            "  (embed): Embedding(18922, 300, padding_idx=1)\n",
            "  (treelstm): TreeLSTM(\n",
            "    (reduce): TreeLSTMCell(300, 150)\n",
            "    (proj_x): Linear(in_features=300, out_features=150, bias=True)\n",
            "    (proj_x_gate): Linear(in_features=300, out_features=150, bias=True)\n",
            "    (buffers_dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (output_layer): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=150, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "embed.weight             [18922, 300] requires_grad=False\n",
            "treelstm.reduce.reduce_layer.weight [750, 300]   requires_grad=True\n",
            "treelstm.reduce.reduce_layer.bias [750]        requires_grad=True\n",
            "treelstm.proj_x.weight   [150, 300]   requires_grad=True\n",
            "treelstm.proj_x.bias     [150]        requires_grad=True\n",
            "treelstm.proj_x_gate.weight [150, 300]   requires_grad=True\n",
            "treelstm.proj_x_gate.bias [150]        requires_grad=True\n",
            "output_layer.1.weight    [5, 150]     requires_grad=True\n",
            "output_layer.1.bias      [5]          requires_grad=True\n",
            "\n",
            "Total number of parameters: 5993405\n",
            "\n",
            "Shuffling training data\n",
            "Iter 250: loss=392.1950, time=17.91s\n",
            "iter 250: dev acc=0.3361\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 500: loss=352.5363, time=36.71s\n",
            "iter 500: dev acc=0.4114\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 750: loss=329.3683, time=55.96s\n",
            "iter 750: dev acc=0.4078\n",
            "Iter 1000: loss=320.3070, time=76.18s\n",
            "iter 1000: dev acc=0.4269\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1250: loss=313.4102, time=95.01s\n",
            "iter 1250: dev acc=0.4151\n",
            "Shuffling training data\n",
            "Iter 1500: loss=311.7620, time=114.00s\n",
            "iter 1500: dev acc=0.4460\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1750: loss=305.7321, time=133.26s\n",
            "iter 1750: dev acc=0.4323\n",
            "Iter 2000: loss=304.5605, time=153.70s\n",
            "iter 2000: dev acc=0.4432\n",
            "Shuffling training data\n",
            "Iter 2250: loss=296.2219, time=173.01s\n",
            "iter 2250: dev acc=0.4496\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 2500: loss=297.7402, time=192.16s\n",
            "iter 2500: dev acc=0.4269\n",
            "Shuffling training data\n",
            "Iter 2750: loss=297.3221, time=211.22s\n",
            "iter 2750: dev acc=0.4414\n",
            "Iter 3000: loss=291.0208, time=230.24s\n",
            "iter 3000: dev acc=0.4460\n",
            "Shuffling training data\n",
            "Iter 3250: loss=286.7872, time=249.33s\n",
            "iter 3250: dev acc=0.4559\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 3500: loss=289.2303, time=268.38s\n",
            "iter 3500: dev acc=0.4469\n",
            "Iter 3750: loss=288.7299, time=287.20s\n",
            "iter 3750: dev acc=0.4487\n",
            "Shuffling training data\n",
            "Iter 4000: loss=281.0571, time=306.10s\n",
            "iter 4000: dev acc=0.4496\n",
            "Shuffling training data\n",
            "Iter 4250: loss=279.0286, time=325.06s\n",
            "iter 4250: dev acc=0.4314\n",
            "Shuffling training data\n",
            "Iter 4500: loss=276.9035, time=344.37s\n",
            "iter 4500: dev acc=0.4342\n",
            "Iter 4750: loss=277.8047, time=363.17s\n",
            "iter 4750: dev acc=0.4487\n",
            "Shuffling training data\n",
            "Iter 5000: loss=269.7572, time=382.11s\n",
            "iter 5000: dev acc=0.4323\n",
            "Shuffling training data\n",
            "Iter 5250: loss=265.1655, time=400.89s\n",
            "iter 5250: dev acc=0.4396\n",
            "Shuffling training data\n",
            "Iter 5500: loss=269.4285, time=419.54s\n",
            "iter 5500: dev acc=0.4342\n",
            "Iter 5750: loss=265.3651, time=439.67s\n",
            "iter 5750: dev acc=0.4024\n",
            "Shuffling training data\n",
            "Iter 6000: loss=261.8604, time=458.61s\n",
            "iter 6000: dev acc=0.4332\n",
            "Shuffling training data\n",
            "Iter 6250: loss=256.5937, time=477.33s\n",
            "iter 6250: dev acc=0.4296\n",
            "Shuffling training data\n",
            "Iter 6500: loss=256.1741, time=496.19s\n",
            "iter 6500: dev acc=0.4278\n",
            "Iter 6750: loss=250.1762, time=514.96s\n",
            "iter 6750: dev acc=0.4105\n",
            "Shuffling training data\n",
            "Iter 7000: loss=246.5315, time=534.04s\n",
            "iter 7000: dev acc=0.4087\n",
            "Shuffling training data\n",
            "Iter 7250: loss=246.9492, time=552.75s\n",
            "iter 7250: dev acc=0.4015\n",
            "Iter 7500: loss=241.9325, time=571.71s\n",
            "iter 7500: dev acc=0.4069\n",
            "Shuffling training data\n",
            "Iter 7750: loss=238.1642, time=590.26s\n",
            "iter 7750: dev acc=0.4151\n",
            "Shuffling training data\n",
            "Iter 8000: loss=234.4121, time=609.13s\n",
            "iter 8000: dev acc=0.4069\n",
            "Shuffling training data\n",
            "Iter 8250: loss=228.2472, time=627.86s\n",
            "iter 8250: dev acc=0.3851\n",
            "Iter 8500: loss=228.9363, time=646.66s\n",
            "iter 8500: dev acc=0.4078\n",
            "Shuffling training data\n",
            "Iter 8750: loss=223.5422, time=665.48s\n",
            "iter 8750: dev acc=0.3869\n",
            "Shuffling training data\n",
            "Iter 9000: loss=219.5032, time=684.47s\n",
            "iter 9000: dev acc=0.3987\n",
            "Shuffling training data\n",
            "Iter 9250: loss=219.0317, time=704.73s\n",
            "iter 9250: dev acc=0.3860\n",
            "Iter 9500: loss=210.0183, time=723.59s\n",
            "iter 9500: dev acc=0.4114\n",
            "Shuffling training data\n",
            "Iter 9750: loss=212.3718, time=742.63s\n",
            "iter 9750: dev acc=0.3996\n",
            "Shuffling training data\n",
            "Iter 10000: loss=202.6052, time=761.33s\n",
            "iter 10000: dev acc=0.3915\n",
            "Iter 10250: loss=204.7693, time=779.83s\n",
            "iter 10250: dev acc=0.4060\n",
            "Shuffling training data\n",
            "Iter 10500: loss=192.3995, time=798.62s\n",
            "iter 10500: dev acc=0.3978\n",
            "Shuffling training data\n",
            "Iter 10750: loss=196.9926, time=817.52s\n",
            "iter 10750: dev acc=0.3942\n",
            "Shuffling training data\n",
            "Iter 11000: loss=193.2857, time=835.95s\n",
            "iter 11000: dev acc=0.3878\n",
            "Iter 11250: loss=185.2759, time=854.67s\n",
            "iter 11250: dev acc=0.4069\n",
            "Shuffling training data\n",
            "Iter 11500: loss=182.1519, time=873.20s\n",
            "iter 11500: dev acc=0.3887\n",
            "Shuffling training data\n",
            "Iter 11750: loss=177.8266, time=892.26s\n",
            "iter 11750: dev acc=0.3969\n",
            "Shuffling training data\n",
            "Iter 12000: loss=174.8085, time=911.00s\n",
            "iter 12000: dev acc=0.3924\n",
            "Iter 12250: loss=166.6429, time=929.58s\n",
            "iter 12250: dev acc=0.3878\n",
            "Shuffling training data\n",
            "Iter 12500: loss=168.0346, time=948.32s\n",
            "iter 12500: dev acc=0.3969\n",
            "Shuffling training data\n",
            "Iter 12750: loss=158.9512, time=966.92s\n",
            "iter 12750: dev acc=0.3769\n",
            "Shuffling training data\n",
            "Iter 13000: loss=162.0982, time=987.09s\n",
            "iter 13000: dev acc=0.3987\n",
            "Iter 13250: loss=152.1224, time=1005.94s\n",
            "iter 13250: dev acc=0.3887\n",
            "Shuffling training data\n",
            "Iter 13500: loss=148.4553, time=1024.52s\n",
            "iter 13500: dev acc=0.3724\n",
            "Shuffling training data\n",
            "Iter 13750: loss=144.3613, time=1043.55s\n",
            "iter 13750: dev acc=0.3833\n",
            "Iter 14000: loss=142.9158, time=1062.67s\n",
            "iter 14000: dev acc=0.3806\n",
            "Shuffling training data\n",
            "Iter 14250: loss=135.2830, time=1081.12s\n",
            "iter 14250: dev acc=0.3851\n",
            "Shuffling training data\n",
            "Iter 14500: loss=141.0558, time=1099.86s\n",
            "iter 14500: dev acc=0.3851\n",
            "Shuffling training data\n",
            "Iter 14750: loss=131.2661, time=1118.48s\n",
            "iter 14750: dev acc=0.3769\n",
            "Iter 15000: loss=127.3820, time=1137.29s\n",
            "iter 15000: dev acc=0.3733\n",
            "Shuffling training data\n",
            "Iter 15250: loss=122.1071, time=1156.08s\n",
            "iter 15250: dev acc=0.3951\n",
            "Shuffling training data\n",
            "Iter 15500: loss=119.7394, time=1174.78s\n",
            "iter 15500: dev acc=0.3706\n",
            "Shuffling training data\n",
            "Iter 15750: loss=124.5100, time=1193.43s\n",
            "iter 15750: dev acc=0.3733\n",
            "Iter 16000: loss=110.7523, time=1212.39s\n",
            "iter 16000: dev acc=0.3742\n",
            "Shuffling training data\n",
            "Iter 16250: loss=110.2313, time=1230.87s\n",
            "iter 16250: dev acc=0.3851\n",
            "Shuffling training data\n",
            "Iter 16500: loss=105.1017, time=1250.96s\n",
            "iter 16500: dev acc=0.3669\n",
            "Iter 16750: loss=102.8301, time=1269.55s\n",
            "iter 16750: dev acc=0.3724\n",
            "Shuffling training data\n",
            "Iter 17000: loss=99.3248, time=1288.23s\n",
            "iter 17000: dev acc=0.3806\n",
            "Shuffling training data\n",
            "Iter 17250: loss=97.7789, time=1307.30s\n",
            "iter 17250: dev acc=0.3706\n",
            "Shuffling training data\n",
            "Iter 17500: loss=97.1844, time=1325.65s\n",
            "iter 17500: dev acc=0.3887\n",
            "Iter 17750: loss=105.8242, time=1344.34s\n",
            "iter 17750: dev acc=0.3760\n",
            "Shuffling training data\n",
            "Iter 18000: loss=87.8447, time=1363.00s\n",
            "iter 18000: dev acc=0.3742\n",
            "Shuffling training data\n",
            "Iter 18250: loss=88.1463, time=1381.69s\n",
            "iter 18250: dev acc=0.3806\n",
            "Shuffling training data\n",
            "Iter 18500: loss=86.2402, time=1400.18s\n",
            "iter 18500: dev acc=0.3706\n",
            "Iter 18750: loss=87.7480, time=1419.10s\n",
            "iter 18750: dev acc=0.3860\n",
            "Shuffling training data\n",
            "Iter 19000: loss=80.9110, time=1437.90s\n",
            "iter 19000: dev acc=0.3560\n",
            "Shuffling training data\n",
            "Iter 19250: loss=75.8551, time=1456.79s\n",
            "iter 19250: dev acc=0.3806\n",
            "Shuffling training data\n",
            "Iter 19500: loss=74.8666, time=1475.48s\n",
            "iter 19500: dev acc=0.3806\n",
            "Iter 19750: loss=74.6344, time=1494.17s\n",
            "iter 19750: dev acc=0.3797\n",
            "Shuffling training data\n",
            "Iter 20000: loss=75.0777, time=1512.83s\n",
            "iter 20000: dev acc=0.3760\n",
            "Shuffling training data\n",
            "Iter 20250: loss=71.6281, time=1532.84s\n",
            "iter 20250: dev acc=0.3660\n",
            "Iter 20500: loss=84.8568, time=1551.83s\n",
            "iter 20500: dev acc=0.3797\n",
            "Shuffling training data\n",
            "Iter 20750: loss=80.6870, time=1570.48s\n",
            "iter 20750: dev acc=0.3824\n",
            "Shuffling training data\n",
            "Iter 21000: loss=59.7123, time=1589.33s\n",
            "iter 21000: dev acc=0.3751\n",
            "Shuffling training data\n",
            "Iter 21250: loss=66.0164, time=1608.21s\n",
            "iter 21250: dev acc=0.3751\n",
            "Iter 21500: loss=65.4173, time=1626.95s\n",
            "iter 21500: dev acc=0.3778\n",
            "Shuffling training data\n",
            "Iter 21750: loss=60.4641, time=1645.86s\n",
            "iter 21750: dev acc=0.3742\n",
            "Shuffling training data\n",
            "Iter 22000: loss=56.5295, time=1664.09s\n",
            "iter 22000: dev acc=0.3760\n",
            "Shuffling training data\n",
            "Iter 22250: loss=57.8361, time=1682.86s\n",
            "iter 22250: dev acc=0.3615\n",
            "Iter 22500: loss=54.0172, time=1701.60s\n",
            "iter 22500: dev acc=0.3706\n",
            "Shuffling training data\n",
            "Iter 22750: loss=56.9250, time=1720.20s\n",
            "iter 22750: dev acc=0.3778\n",
            "Shuffling training data\n",
            "Iter 23000: loss=55.8626, time=1738.77s\n",
            "iter 23000: dev acc=0.3860\n",
            "Iter 23250: loss=53.2659, time=1757.46s\n",
            "iter 23250: dev acc=0.3760\n",
            "Shuffling training data\n",
            "Iter 23500: loss=54.2469, time=1776.65s\n",
            "iter 23500: dev acc=0.3769\n",
            "Shuffling training data\n",
            "Iter 23750: loss=49.3431, time=1796.70s\n",
            "iter 23750: dev acc=0.3942\n",
            "Shuffling training data\n",
            "Iter 24000: loss=56.5778, time=1815.46s\n",
            "iter 24000: dev acc=0.3806\n",
            "Iter 24250: loss=47.9292, time=1834.09s\n",
            "iter 24250: dev acc=0.3688\n",
            "Shuffling training data\n",
            "Iter 24500: loss=44.5986, time=1853.08s\n",
            "iter 24500: dev acc=0.3797\n",
            "Shuffling training data\n",
            "Iter 24750: loss=42.1130, time=1871.83s\n",
            "iter 24750: dev acc=0.3769\n",
            "Shuffling training data\n",
            "Iter 25000: loss=37.8068, time=1890.41s\n",
            "iter 25000: dev acc=0.3751\n",
            "Iter 25250: loss=41.5719, time=1909.15s\n",
            "iter 25250: dev acc=0.3797\n",
            "Shuffling training data\n",
            "Iter 25500: loss=45.0583, time=1927.83s\n",
            "iter 25500: dev acc=0.3869\n",
            "Shuffling training data\n",
            "Iter 25750: loss=44.6913, time=1947.06s\n",
            "iter 25750: dev acc=0.3660\n",
            "Shuffling training data\n",
            "Iter 26000: loss=43.0789, time=1965.55s\n",
            "iter 26000: dev acc=0.3760\n",
            "Iter 26250: loss=48.3817, time=1984.06s\n",
            "iter 26250: dev acc=0.3815\n",
            "Shuffling training data\n",
            "Iter 26500: loss=35.6703, time=2002.78s\n",
            "iter 26500: dev acc=0.3842\n",
            "Shuffling training data\n",
            "Iter 26750: loss=53.1021, time=2021.44s\n",
            "iter 26750: dev acc=0.3933\n",
            "Iter 27000: loss=39.8823, time=2040.16s\n",
            "iter 27000: dev acc=0.3906\n",
            "Shuffling training data\n",
            "Iter 27250: loss=31.4906, time=2058.76s\n",
            "iter 27250: dev acc=0.3851\n",
            "Shuffling training data\n",
            "Iter 27500: loss=31.5127, time=2078.88s\n",
            "iter 27500: dev acc=0.3806\n",
            "Shuffling training data\n",
            "Iter 27750: loss=36.0582, time=2097.57s\n",
            "iter 27750: dev acc=0.3833\n",
            "Iter 28000: loss=37.6145, time=2116.03s\n",
            "iter 28000: dev acc=0.3896\n",
            "Shuffling training data\n",
            "Iter 28250: loss=30.1625, time=2135.06s\n",
            "iter 28250: dev acc=0.3715\n",
            "Shuffling training data\n",
            "Iter 28500: loss=32.7014, time=2153.86s\n",
            "iter 28500: dev acc=0.3860\n",
            "Shuffling training data\n",
            "Iter 28750: loss=35.6545, time=2172.35s\n",
            "iter 28750: dev acc=0.3797\n",
            "Iter 29000: loss=34.0592, time=2190.85s\n",
            "iter 29000: dev acc=0.3824\n",
            "Shuffling training data\n",
            "Iter 29250: loss=35.9651, time=2209.49s\n",
            "iter 29250: dev acc=0.3787\n",
            "Shuffling training data\n",
            "Iter 29500: loss=27.0348, time=2228.39s\n",
            "iter 29500: dev acc=0.3733\n",
            "Iter 29750: loss=29.5133, time=2247.26s\n",
            "iter 29750: dev acc=0.3733\n",
            "Shuffling training data\n",
            "Iter 30000: loss=28.6436, time=2265.96s\n",
            "iter 30000: dev acc=0.3851\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 3250: train acc=0.5061, dev acc=0.4559, test acc=0.4738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHcHHaLtguUg"
      },
      "source": [
        "# plot\n",
        "lstm_tree_loss, lstm_tree_acc = results"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(lstm_tree_acc, \"Fig 4.4.a Tree LSTM No. of Iterations vs Val Accuracies\", \"Accuracy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "Lal4reQEyjR_",
        "outputId": "3a8bb776-ce7e-4851-f1ef-ba835a1445d7"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"331.389812pt\" version=\"1.1\" viewBox=\"0 0 421.125937 331.389812\" width=\"421.125937pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 331.389812 \nL 421.125937 331.389812 \nL 421.125937 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 56.805937 288.430125 \nL 413.925938 288.430125 \nL 413.925938 22.318125 \nL 56.805937 22.318125 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p1315833fd4)\" d=\"M 70.310475 288.430125 \nL 70.310475 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(66.8111 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#p1315833fd4)\" d=\"M 124.874264 288.430125 \nL 124.874264 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(117.875514 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p1315833fd4)\" d=\"M 179.438054 288.430125 \nL 179.438054 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(172.439304 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#p1315833fd4)\" d=\"M 234.001843 288.430125 \nL 234.001843 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(227.003093 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p1315833fd4)\" d=\"M 288.565632 288.430125 \nL 288.565632 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(281.566882 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#p1315833fd4)\" d=\"M 343.129421 288.430125 \nL 343.129421 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(332.631296 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p1315833fd4)\" d=\"M 397.69321 288.430125 \nL 397.69321 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120 -->\n      <g style=\"fill:#262626;\" transform=\"translate(387.195085 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- # of Iterations -->\n     <defs>\n      <path d=\"M 51.125 44 \nL 36.921875 44 \nL 32.8125 27.6875 \nL 47.125 27.6875 \nz\nM 43.796875 71.78125 \nL 38.71875 51.515625 \nL 52.984375 51.515625 \nL 58.109375 71.78125 \nL 65.921875 71.78125 \nL 60.890625 51.515625 \nL 76.125 51.515625 \nL 76.125 44 \nL 58.984375 44 \nL 54.984375 27.6875 \nL 70.515625 27.6875 \nL 70.515625 20.21875 \nL 53.078125 20.21875 \nL 48 0 \nL 40.1875 0 \nL 45.21875 20.21875 \nL 30.90625 20.21875 \nL 25.875 0 \nL 18.015625 0 \nL 23.09375 20.21875 \nL 7.71875 20.21875 \nL 7.71875 27.6875 \nL 24.90625 27.6875 \nL 29 44 \nL 13.28125 44 \nL 13.28125 51.515625 \nL 30.90625 51.515625 \nL 35.890625 71.78125 \nz\n\" id=\"DejaVuSans-35\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-73\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(192.165 321.694187)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"83.789062\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"115.576172\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"176.757812\" xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"211.962891\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"243.75\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"273.242188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"312.451172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"373.974609\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"415.087891\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"476.367188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"515.576172\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"543.359375\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"604.541016\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"667.919922\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#p1315833fd4)\" d=\"M 56.805937 280.001238 \nL 413.925938 280.001238 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.34 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 284.180379)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p1315833fd4)\" d=\"M 56.805937 237.235319 \nL 413.925938 237.235319 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.36 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 241.41446)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#p1315833fd4)\" d=\"M 56.805937 194.4694 \nL 413.925938 194.4694 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.38 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 198.64854)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p1315833fd4)\" d=\"M 56.805937 151.703481 \nL 413.925938 151.703481 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.40 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 155.882621)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <path clip-path=\"url(#p1315833fd4)\" d=\"M 56.805937 108.937562 \nL 413.925938 108.937562 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.42 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 113.116702)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p1315833fd4)\" d=\"M 56.805937 66.171642 \nL 413.925938 66.171642 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.44 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 70.350783)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <path clip-path=\"url(#p1315833fd4)\" d=\"M 56.805937 23.405723 \nL 413.925938 23.405723 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.46 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 27.584864)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- Accuracy -->\n     <defs>\n      <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(16.318125 182.767875)rotate(-90)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"66.658203\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"121.638672\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"176.619141\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"239.998047\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"281.111328\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"342.390625\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"397.371094\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p1315833fd4)\" d=\"M 73.038665 288.430125 \nL 75.766854 127.232519 \nL 78.495044 135.001078 \nL 81.223233 94.216142 \nL 83.951423 119.463959 \nL 86.679612 53.431205 \nL 89.407802 82.563303 \nL 92.135991 59.257625 \nL 94.86418 45.662646 \nL 97.59237 94.216142 \nL 100.320559 63.141904 \nL 103.048749 53.431205 \nL 105.776938 32.067667 \nL 108.505128 51.489065 \nL 111.233317 47.604786 \nL 113.961507 45.662646 \nL 116.689696 84.505442 \nL 119.417886 78.679023 \nL 122.146075 47.604786 \nL 124.874264 82.563303 \nL 127.602454 67.026184 \nL 130.330643 78.679023 \nL 133.058833 146.653917 \nL 135.787022 80.621163 \nL 138.515212 88.389722 \nL 141.243401 92.274002 \nL 143.971591 129.174659 \nL 146.69978 133.058938 \nL 149.42797 148.596057 \nL 152.156159 136.943218 \nL 154.884349 119.463959 \nL 157.612538 136.943218 \nL 160.340727 183.554574 \nL 163.068917 135.001078 \nL 165.797106 179.670294 \nL 168.525296 154.422476 \nL 171.253485 181.612434 \nL 173.981675 127.232519 \nL 176.709864 152.480337 \nL 179.438054 169.959595 \nL 182.166243 138.885358 \nL 184.894433 156.364616 \nL 187.622622 164.133176 \nL 190.350811 177.728154 \nL 193.079001 136.943218 \nL 195.80719 175.786015 \nL 198.53538 158.306756 \nL 201.263569 168.017455 \nL 203.991759 177.728154 \nL 206.719948 158.306756 \nL 209.448138 201.033832 \nL 212.176327 154.422476 \nL 214.904517 175.786015 \nL 217.632706 210.744532 \nL 220.360895 187.438854 \nL 223.089085 193.265273 \nL 225.817274 183.554574 \nL 228.545464 183.554574 \nL 231.273653 201.033832 \nL 234.001843 208.802392 \nL 236.730032 162.191036 \nL 239.458222 214.628811 \nL 242.186411 208.802392 \nL 244.914601 206.860252 \nL 247.64279 183.554574 \nL 250.37098 222.397371 \nL 253.099169 210.744532 \nL 255.827358 193.265273 \nL 258.555548 214.628811 \nL 261.283737 175.786015 \nL 264.011927 202.975972 \nL 266.740116 206.860252 \nL 269.468306 193.265273 \nL 272.196495 214.628811 \nL 274.924685 181.612434 \nL 277.652874 245.703049 \nL 280.381064 193.265273 \nL 283.109253 193.265273 \nL 285.837442 195.207413 \nL 288.565632 202.975972 \nL 291.293821 224.33951 \nL 294.022011 195.207413 \nL 296.7502 189.380993 \nL 299.47839 204.918112 \nL 302.206579 204.918112 \nL 304.934769 199.091693 \nL 307.662958 206.860252 \nL 310.391148 202.975972 \nL 313.119337 234.05021 \nL 315.847526 214.628811 \nL 318.575716 199.091693 \nL 321.303905 181.612434 \nL 324.032095 202.975972 \nL 326.760284 201.033832 \nL 329.488474 164.133176 \nL 332.216663 193.265273 \nL 334.944853 218.513091 \nL 337.673042 195.207413 \nL 340.401232 201.033832 \nL 343.129421 204.918112 \nL 345.857611 195.207413 \nL 348.5858 179.670294 \nL 351.313989 224.33951 \nL 354.042179 202.975972 \nL 356.770368 191.323133 \nL 359.498558 185.496714 \nL 362.226747 166.075315 \nL 364.954937 171.901735 \nL 367.683126 183.554574 \nL 370.411316 193.265273 \nL 373.139505 187.438854 \nL 375.867695 173.843875 \nL 378.595884 212.686671 \nL 381.324073 181.612434 \nL 384.052263 195.207413 \nL 386.780452 189.380993 \nL 389.508642 197.149553 \nL 392.236831 208.802392 \nL 394.965021 208.802392 \nL 397.69321 183.554574 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 56.805937 288.430125 \nL 56.805937 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 413.925938 288.430125 \nL 413.925938 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 56.805937 288.430125 \nL 413.925937 288.430125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 56.805937 22.318125 \nL 413.925937 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- Fig 4.4.a Tree LSTM No. of Iterations vs Val Accuracies -->\n    <defs>\n     <path d=\"M 9.8125 72.90625 \nL 51.703125 72.90625 \nL 51.703125 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.109375 \nL 48.578125 43.109375 \nL 48.578125 34.8125 \nL 19.671875 34.8125 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-70\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n     <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n     <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n     <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     <path d=\"M 28.609375 0 \nL 0.78125 72.90625 \nL 11.078125 72.90625 \nL 34.1875 11.53125 \nL 57.328125 72.90625 \nL 67.578125 72.90625 \nL 39.796875 0 \nz\n\" id=\"DejaVuSans-86\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(73.789688 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"50.269531\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"78.052734\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"141.529297\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"173.316406\" xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"236.939453\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"268.726562\" xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"332.349609\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"364.136719\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"425.416016\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"457.203125\" xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"503.537109\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"542.400391\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"603.923828\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"665.447266\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"697.234375\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"752.947266\" xlink:href=\"#DejaVuSans-83\"/>\n     <use x=\"816.423828\" xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"877.507812\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"963.787109\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"995.574219\" xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"1070.378906\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1129.810547\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"1161.597656\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1193.384766\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1254.566406\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"1289.771484\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1321.558594\" xlink:href=\"#DejaVuSans-73\"/>\n     <use x=\"1351.050781\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1390.259766\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1451.783203\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1492.896484\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1554.175781\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1593.384766\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1621.167969\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1682.349609\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1745.728516\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1797.828125\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1829.615234\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"1888.794922\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1940.894531\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1972.681641\" xlink:href=\"#DejaVuSans-86\"/>\n     <use x=\"2033.339844\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2094.619141\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"2122.402344\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2154.189453\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"2220.847656\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"2275.828125\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"2330.808594\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"2394.1875\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"2435.300781\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2496.580078\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"2551.560547\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"2579.34375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"2640.867188\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 401.825937 36.618125 \nL 406.225937 36.618125 \nQ 408.425937 36.618125 408.425937 34.418125 \nL 408.425937 30.018125 \nQ 408.425937 27.818125 406.225937 27.818125 \nL 401.825937 27.818125 \nQ 399.625937 27.818125 399.625937 30.018125 \nL 399.625937 34.418125 \nQ 399.625937 36.618125 401.825937 36.618125 \nz\n\" style=\"fill:#eaeaf2;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p1315833fd4\">\n   <rect height=\"266.112\" width=\"357.12\" x=\"56.805937\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "application/pdf": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(lstm_tree_loss, \"Fig 4.4.b Tree LSTM No. of Iterations vs Loss\", \"Loss\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "mOhJW3YCytPx",
        "outputId": "6c161766-b9e5-4c25-b6ce-fcec90dfdd5f"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"331.389812pt\" version=\"1.1\" viewBox=\"0 0 417.63 331.389812\" width=\"417.63pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 331.389812 \nL 417.63 331.389812 \nL 417.63 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 53.31 288.430125 \nL 410.43 288.430125 \nL 410.43 22.318125 \nL 53.31 22.318125 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 66.814538 288.430125 \nL 66.814538 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(63.315163 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 121.378327 288.430125 \nL 121.378327 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(114.379577 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 175.942116 288.430125 \nL 175.942116 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(168.943366 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 230.505905 288.430125 \nL 230.505905 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(223.507155 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 285.069694 288.430125 \nL 285.069694 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(278.070944 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 339.633484 288.430125 \nL 339.633484 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(329.135359 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 394.197273 288.430125 \nL 394.197273 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120 -->\n      <g style=\"fill:#262626;\" transform=\"translate(383.699148 306.288406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- # of Iterations -->\n     <defs>\n      <path d=\"M 51.125 44 \nL 36.921875 44 \nL 32.8125 27.6875 \nL 47.125 27.6875 \nz\nM 43.796875 71.78125 \nL 38.71875 51.515625 \nL 52.984375 51.515625 \nL 58.109375 71.78125 \nL 65.921875 71.78125 \nL 60.890625 51.515625 \nL 76.125 51.515625 \nL 76.125 44 \nL 58.984375 44 \nL 54.984375 27.6875 \nL 70.515625 27.6875 \nL 70.515625 20.21875 \nL 53.078125 20.21875 \nL 48 0 \nL 40.1875 0 \nL 45.21875 20.21875 \nL 30.90625 20.21875 \nL 25.875 0 \nL 18.015625 0 \nL 23.09375 20.21875 \nL 7.71875 20.21875 \nL 7.71875 27.6875 \nL 24.90625 27.6875 \nL 29 44 \nL 13.28125 44 \nL 13.28125 51.515625 \nL 30.90625 51.515625 \nL 35.890625 71.78125 \nz\n\" id=\"DejaVuSans-35\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-73\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(188.669062 321.694187)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"83.789062\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"115.576172\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"176.757812\" xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"211.962891\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"243.75\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"273.242188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"312.451172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"373.974609\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"415.087891\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"476.367188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"515.576172\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"543.359375\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"604.541016\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"667.919922\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 53.31 271.87197 \nL 410.43 271.87197 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(29.8125 276.05111)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 53.31 235.821461 \nL 410.43 235.821461 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 100 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 240.000601)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 53.31 199.770952 \nL 410.43 199.770952 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 150 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 203.950093)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 53.31 163.720443 \nL 410.43 163.720443 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 200 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 167.899584)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 53.31 127.669934 \nL 410.43 127.669934 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 250 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 131.849075)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 53.31 91.619426 \nL 410.43 91.619426 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_14\">\n      <!-- 300 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 95.798566)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 53.31 55.568917 \nL 410.43 55.568917 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_15\">\n      <!-- 350 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 59.748057)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- Loss -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(16.318125 168.53475)rotate(-90)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pefca6ef0e0)\" d=\"M 69.542727 25.145891 \nL 72.270917 53.740196 \nL 74.999106 70.444567 \nL 77.727296 76.977889 \nL 80.455485 81.950565 \nL 83.183675 83.13891 \nL 85.911864 87.486549 \nL 88.640053 88.331244 \nL 91.368243 94.343496 \nL 94.096432 93.248774 \nL 96.824622 93.550207 \nL 99.552811 98.093537 \nL 102.281001 101.146006 \nL 105.00919 99.384517 \nL 107.73738 99.745298 \nL 110.465569 105.277469 \nL 113.193759 106.740004 \nL 115.921948 108.272216 \nL 118.650138 107.622432 \nL 121.378327 113.424783 \nL 124.106516 116.735474 \nL 126.834706 113.661756 \nL 129.562895 116.591541 \nL 132.291085 119.118471 \nL 135.019274 122.915836 \nL 137.747464 123.218359 \nL 140.475653 127.542876 \nL 143.203843 130.170748 \nL 145.932032 129.86962 \nL 148.660222 133.486654 \nL 151.388411 136.203667 \nL 154.1166 138.908985 \nL 156.84479 143.353943 \nL 159.572979 142.857057 \nL 162.301169 146.746265 \nL 165.029358 149.658408 \nL 167.757548 149.998386 \nL 170.485737 156.497145 \nL 173.213927 154.800239 \nL 175.942116 161.842057 \nL 178.670306 160.281705 \nL 181.398495 169.200489 \nL 184.126684 165.888799 \nL 186.854874 168.56155 \nL 189.583063 174.336636 \nL 192.311253 176.589073 \nL 195.039442 179.707721 \nL 197.767632 181.883751 \nL 200.495821 187.771256 \nL 203.224011 186.767825 \nL 205.9522 193.317059 \nL 208.68039 191.048032 \nL 211.408579 198.240694 \nL 214.136769 200.884711 \nL 216.864958 203.836518 \nL 219.593147 204.878729 \nL 222.321337 210.382056 \nL 225.049526 206.21978 \nL 227.777716 213.278269 \nL 230.505905 216.078733 \nL 233.234095 219.882 \nL 235.962284 221.589178 \nL 238.690474 218.149466 \nL 241.418663 228.068961 \nL 244.146853 228.444611 \nL 246.875042 232.14311 \nL 249.603231 233.780925 \nL 252.331421 236.308273 \nL 255.05961 237.422873 \nL 257.7878 237.851566 \nL 260.515989 231.622162 \nL 263.244179 244.585566 \nL 265.972368 244.368096 \nL 268.700558 245.742414 \nL 271.428747 244.655256 \nL 274.156937 249.584831 \nL 276.885126 253.230213 \nL 279.613316 253.942915 \nL 282.341505 254.110344 \nL 285.069694 253.790693 \nL 287.797884 256.277886 \nL 290.526073 246.73985 \nL 293.254263 249.746335 \nL 295.982452 264.869296 \nL 298.710642 260.324007 \nL 301.438831 260.755927 \nL 304.167021 264.327215 \nL 306.89521 267.164141 \nL 309.6234 266.222028 \nL 312.351589 268.975498 \nL 315.079778 266.87897 \nL 317.807968 267.644999 \nL 320.536157 269.517232 \nL 323.264347 268.809925 \nL 325.992536 272.345624 \nL 328.720726 267.129285 \nL 331.448915 273.365035 \nL 334.177105 275.766462 \nL 336.905294 277.558582 \nL 339.633484 280.663401 \nL 342.361673 277.948696 \nL 345.089862 275.434991 \nL 347.818052 275.699562 \nL 350.546241 276.862183 \nL 353.274431 273.038775 \nL 356.00262 282.203827 \nL 358.73081 269.635318 \nL 361.458999 279.166963 \nL 364.187189 285.217409 \nL 366.915378 285.201522 \nL 369.643568 281.924156 \nL 372.371757 280.802044 \nL 375.099947 286.174982 \nL 377.828136 284.344422 \nL 380.556325 282.215221 \nL 383.284515 283.365482 \nL 386.012704 281.99126 \nL 388.740894 288.430125 \nL 391.469083 286.643075 \nL 394.197273 287.270124 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 53.31 288.430125 \nL 53.31 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 410.43 288.430125 \nL 410.43 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 53.31 288.430125 \nL 410.43 288.430125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 53.31 22.318125 \nL 410.43 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- Fig 4.4.b Tree LSTM No. of Iterations vs Loss -->\n    <defs>\n     <path d=\"M 9.8125 72.90625 \nL 51.703125 72.90625 \nL 51.703125 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.109375 \nL 48.578125 43.109375 \nL 48.578125 34.8125 \nL 19.671875 34.8125 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-70\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n     <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n     <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n     <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n     <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n     <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(100.217813 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"50.269531\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"78.052734\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"141.529297\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"173.316406\" xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"236.939453\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"268.726562\" xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"332.349609\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"364.136719\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"427.613281\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"459.400391\" xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"505.734375\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"544.597656\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"606.121094\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"667.644531\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"699.431641\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"755.144531\" xlink:href=\"#DejaVuSans-83\"/>\n     <use x=\"818.621094\" xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"879.705078\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"965.984375\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"997.771484\" xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"1072.576172\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1132.007812\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"1163.794922\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1195.582031\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1256.763672\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"1291.96875\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1323.755859\" xlink:href=\"#DejaVuSans-73\"/>\n     <use x=\"1353.248047\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1392.457031\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1453.980469\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1495.09375\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1556.373047\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1595.582031\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1623.365234\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1684.546875\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1747.925781\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1800.025391\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1831.8125\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"1890.992188\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1943.091797\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1974.878906\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"2028.841797\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"2090.023438\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2142.123047\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 398.33 36.618125 \nL 402.73 36.618125 \nQ 404.93 36.618125 404.93 34.418125 \nL 404.93 30.018125 \nQ 404.93 27.818125 402.73 27.818125 \nL 398.33 27.818125 \nQ 396.13 27.818125 396.13 30.018125 \nL 396.13 34.418125 \nQ 396.13 36.618125 398.33 36.618125 \nz\n\" style=\"fill:#eaeaf2;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pefca6ef0e0\">\n   <rect height=\"266.112\" width=\"357.12\" x=\"53.31\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "application/pdf": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7QZZH86eHqu"
      },
      "source": [
        "# Further experiments and report\n",
        "\n",
        "For your report, you are expected to answer research questions by doing further experiments.\n",
        "\n",
        "## Research Questions\n",
        "\n",
        "Make sure you cover at least the following:\n",
        "\n",
        "- How important is word order for this task?\n",
        "- Does the tree structure help to get a better accuracy?\n",
        "- How does performance depend on the sentence length? Compare the various models. Is there a model that does better on longer sentences? If so, why?\n",
        "- Do you get better performance if you supervise the sentiment **at each node in the tree**? You can extract more training examples by treating every node in each tree as a separate tree. You will need to write a function that extracts all subtrees given a treestring. \n",
        "    - Warning: NLTK's Tree function seems to result in invalid trees in some cases, so be careful if you want to parse the string to a tree structure before extraction the phrases.\n",
        "\n",
        "**To be able to obtain a full grade (10), you should conduct further investigations.** For example, you can also investigate one the following:\n",
        "\n",
        "- When making a wrong prediction, can you figure out at what point in the tree (sentence) the model fails? You can make a prediction at each node to investigate.\n",
        "- How does N-ary Tree LSTM compare to the Child-Sum Tree LSTM? \n",
        "- How do the Tai et al. Tree LSTMs compare to Le & Zuidema's formulation?\n",
        "- Or... your own research question!\n",
        "\n",
        "These examples should give you an idea of the expected scope of your further investigations.\n",
        "\n",
        "In general:\n",
        "\n",
        "- ***When you report numbers, please report the mean accuracy across 3 (or more) runs with different random seed, together with the standard deviation.*** This is because the final performance may vary per random seed. \n",
        "More precisely, you should run each model with 3 different seeds, and for each of these 3 runs, evaluate the best model (according to the validation) on the test dataset. The validation dataset is used for finding the best model over iterations, but the accuracy you report should be on the test dataset.\n",
        "\n",
        "## Report instructions\n",
        "\n",
        "Your report needs to be written in LaTeX. You are required to use the ACL 2020 template which you can download from or edit directly on [Overleaf](https://www.overleaf.com/latex/templates/instructions-for-acl-2018-proceedings/xzmhqgnmkppc). Make sure your names and student numbers are visible at the top. (Tip: you need to uncomment `\\aclfinalcopy`).\n",
        "You can find some general tips about writing a research paper [here](https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/), but note that you need to make your own judgment about what is appropriate for this project. \n",
        "\n",
        "We expect you to use the following structure:\n",
        "1. Introduction (~1 page) - describe the problem, your research questions and goals, a summary of your findings and contributions. Please cite related work (models, data set) as part of your introduction here, since this is a short paper.\n",
        "    - Introduce the task and the main goal\n",
        "    - Present your research questions\n",
        "    - Motivate the importance of the questions\n",
        "    - Present and explain your expectations\n",
        "    - Make clear whether the research questions are addressed or not addressed in the literature\n",
        "    - Describe the approach you use to answer the research questions\n",
        "    - Summarise your findings\n",
        "2. Background (~1/2-1 page) -\n",
        "cover the main techniques (\"building blocks\") used in your project (e.g. word embeddings, LSTM, Tree-LSTM) and intuitions behind them. Be accurate and concise.\n",
        "    - How does each technique work? (Don't just copy the formulas)\n",
        "    - What is the relation between the techniques?\n",
        "3. Models (~1/2 page) - Describe the architecture of the final models.\n",
        "    - How do you use LSTM or Tree-LSTM for the sentiment classification task? \n",
        "    - What layers do you have, how do you do classification? \n",
        "    - What is your loss function?\n",
        "    - Etc.\n",
        "4. Experiments (~1/2 page) - Describe your experimental setup. This section should allow someone else to reproduce your experiments. Describe how you evaluate the models.\n",
        "    - Task and the data\n",
        "    - Training (model, data, parameters and hyper parameters, training algorithms, supervision signals, etc.)\n",
        "    - Evaluation (e.g. metrics)\n",
        "5. Results and Analysis (~1 page). Present the results and analyse your findings.\n",
        "    - Answer each of the research questions you raised in the introduction.\n",
        "    - Use figures and tables to highlight interesting patterns\n",
        "    - What are the factors that make model A better than model B in task C? Investigate to prove their effect!\n",
        "6. Conclusion (~1/4 page). Present the conclusions that can be drawn from your experiments.\n",
        "    - What have you learned from you experiments?\n",
        "    - How do your findings relate to what is already known in the literature?\n",
        "    - Were the results as expected? Any surprising results? Why?\n",
        "    - Based on what you learned, what would you suggest doing next?\n",
        "\n",
        "\n",
        "General Tips:\n",
        "\n",
        "- Math notation – define each variable (either in running text, or in a pseudo-legenda after or before the equation).\n",
        "- Define technical terminology you need.\n",
        "- Avoid colloquial language – everything can be said in a scientific-sounding way.\n",
        "- Avoid lengthy sentences, stay to the point.\n",
        "- Do not spend space on \"obvious\" things.\n",
        "- Do not go over the page limit. (We will deduct points for that.)\n",
        "- The page limit is 4 pages excluding references and appendix. This is a strict limit; points will be deducted for longer reports. \n",
        "- There is no strict limit to references and appendix. However, the report needs to remain fully self-contained: the appendix should only include content that is not necessary to understand your work. For example, preprocessing decisions, model parameters, pseudocode, sample system inputs/outputs, and other details that are necessary for the exact replication of your work can be put into the appendix. \n",
        "\n",
        "\n",
        "An ideal report:\n",
        "- Precise, scientific-sounding, technical, to the point \n",
        "  - Little general “waffle”/chit-chat\n",
        "- Not boring – because you don’t explain obvious things too much\n",
        "- Efficient delivery of (only) the facts that we need to know to understand/reimplement\n",
        "- Results visually well-presented and described with the correct priority of importance of sub-results\n",
        "- Insightful analysis – speculation should connect to something interesting and not be too much; the reader “learns something new”\n",
        "- No typos, no colloquialisms – well-considered language\n",
        "- This normally means several re-draftings (re-orderings of information)\n"
      ]
    }
  ]
}